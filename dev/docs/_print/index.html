<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.105.0">
<link rel="canonical" type="text/html" href="https://gkcalat.github.io/kubeflow-docs/docs/">
<link rel="alternate" type="application/rss&#43;xml" href="https://gkcalat.github.io/kubeflow-docs/docs/index.xml">
<meta name="robots" content="noindex, nofollow">

<script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "WebSite",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gkcalat.github.io\/kubeflow-docs"
      },
      "articleSection" : "docs",
      "name" : "Kubeflow on Google Cloud",
      "headline" : "Kubeflow on Google Cloud",
      "description" : "Running Kubeflow on Kubernetes Engine and Google Cloud Platform",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "0001",
      "datePublished": "0001-01-01 00:00:00 \u002b0000 UTC",
      "dateModified" : "0001-01-01 00:00:00 \u002b0000 UTC",
      "url" : "https:\/\/gkcalat.github.io\/kubeflow-docs\/docs\/",
      "wordCount" : "0",
      "keywords" : [ "Kubeflow on GCP" ]
  }
  </script>

<link rel="shortcut icon" href="/kubeflow-docs/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/kubeflow-docs/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/kubeflow-docs/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/kubeflow-docs/favicons/favicon-32x32.png" sizes="32x32">
<link rel="manifest" href="/kubeflow-docs/favicons/manifest.json">
<meta name="msapplication-config" content="/kubeflow-docs/favicons/browserconfig.xml" />
<meta name="msapplication-TileColor" content="#4279f4">
<meta name="theme-color" content="#4279f4">

<title>Kubeflow on Google Cloud | Kubeflow on Google Cloud Platform</title>
<meta name="description" content="Running Kubeflow on Kubernetes Engine and Google Cloud Platform">
<meta property="og:title" content="Kubeflow on Google Cloud" />
<meta property="og:description" content="Running Kubeflow on Kubernetes Engine and Google Cloud Platform" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://gkcalat.github.io/kubeflow-docs/docs/" /><meta property="og:site_name" content="Kubeflow on Google Cloud Platform" />

<meta itemprop="name" content="Kubeflow on Google Cloud">
<meta itemprop="description" content="Running Kubeflow on Kubernetes Engine and Google Cloud Platform"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kubeflow on Google Cloud"/>
<meta name="twitter:description" content="Running Kubeflow on Kubernetes Engine and Google Cloud Platform"/>




<link rel="preload" href="/kubeflow-docs/scss/main.min.5006b434b14f73c3d5537a7ba4c8ecef723ddc93f3a3c8530e19d24f75486ea9.css" as="style">
<link href="/kubeflow-docs/scss/main.min.5006b434b14f73c3d5537a7ba4c8ecef723ddc93f3a3c8530e19d24f75486ea9.css" rel="stylesheet" integrity="">

<script
  src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
  crossorigin="anonymous"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-GK9XL47N6S"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-GK9XL47N6S', { 'anonymize_ip': false });
}
</script>

  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand-md navbar-dark  td-navbar">
        <a class="navbar-brand" href="/kubeflow-docs/">
		<span class="navbar-logo"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 276.93 274.55"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M95.9 62.15l4.1 102.1 73.75-94.12a6.79 6.79.0 019.6-1.11l46 36.92-15-65.61z" fill="#4279f4"/><path fill="#0028aa" d="M102.55 182.98h65.42l-40.17-32.23-25.25 32.23z"/><path fill="#014bd1" d="M180.18 83.92l-44 56.14 46.88 37.61 44.47-55.76-47.35-37.99z"/><path fill="#bedcff" d="M83.56 52.3l.01-.01 38.69-48.52-62.39 30.05-15.41 67.51 39.1-49.03z"/><path fill="#6ca1ff" d="M45.32 122.05l41.44 51.96-3.95-98.98-37.49 47.02z"/><path fill="#a1c3ff" d="M202.31 28.73 142.65.0l-37.13 46.56 96.79-17.83z"/><path d="M1.6 272v-44.78h5.74v23.41l20.48-23.41h6.4l-17.39 19.7 19 25.07H29.1l-15.92-20.8-5.84 6.65V272zm40.02-9.79V240h5.43v22.39a4.67 4.67.0 002.35 4.19 11 11 0 0011 0 4.69 4.69.0 002.33-4.19V240h5.43v22.19a9.08 9.08.0 01-4.1 7.87 16.2 16.2.0 01-18.37.0 9.07 9.07.0 01-4.07-7.85zM77.46 272v-48h5.43v16.81a29.29 29.29.0 019.32-1.73 13.1 13.1.0 016.2 1.41 10.71 10.71.0 014.18 3.74 18.07 18.07.0 012.23 5.06 21.26 21.26.0 01.73 5.58q0 8.43-4.38 12.79T87.35 272zm5.43-4.87h4.55q6.77.0 9.72-2.95t3-9.51a14.21 14.21.0 00-2-7.52 6.55 6.55.0 00-6-3.22 24.73 24.73.0 00-9.25 1.54zm29.47-11.19q0-7.71 4.09-12.3a13.75 13.75.0 0110.8-4.59q13.35.0 13.36 18.86h-22.82a12.3 12.3.0 002.9 7.07q2.59 3.11 7.9 3.1a24.92 24.92.0 0010.55-2v5a27.74 27.74.0 01-9.86 1.87 19.83 19.83.0 01-7.7-1.37 13.31 13.31.0 01-5.28-3.76 16.21 16.21.0 01-3-5.38 20.84 20.84.0 01-.94-6.5zm5.62-2.12h17.26a14.91 14.91.0 00-2.37-7.12 6.44 6.44.0 00-5.62-2.78 8.2 8.2.0 00-6.21 2.72 12.07 12.07.0 00-3.04 7.18z" fill="#4279f4" stroke="#4279f4" stroke-miterlimit="10" stroke-width="3.2"/><path d="M147.32 244.89V240h5v-7.59a8.14 8.14.0 012.31-6.05 7.79 7.79.0 015.69-2.28h7.86V229h-5c-2.21.0-3.67.45-4.37 1.34s-1.06 2.55-1.06 5V240h8.46v4.87h-8.46V272h-5.44v-27.1zM175.26 272v-48h5.43v48zm19.15-3.95a17.86 17.86.0 1112.33 4.9 16.57 16.57.0 01-12.33-4.9zm3.84-20.65a13.16 13.16.0 000 17.2 12.07 12.07.0 0017 0 13.09 13.09.0 000-17.2 12.07 12.07.0 00-17 0zm30.2-7.4h5.75l7.3 25.32 7.43-25.32h5.36l7.34 25.34L269 240h5.74l-10.04 32h-6.12l-6.83-24.58L245 272h-6.47z" fill="#0028aa" stroke="#0028aa" stroke-miterlimit="10" stroke-width="3.2"/></g></g></svg></span><span class="text-uppercase font-weight-bold">Kubeflow on Google Cloud Platform</span>
	</a>
	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main_navbar" aria-controls="main_navbar" aria-expanded="false" aria-label="Toggle navigation">
		<span class="navbar-toggler-icon"></span>
	</button>
	<div class="collapse navbar-collapse ml-md-auto" id="main_navbar">
		<ul class="navbar-nav ml-auto pt-4 pt-md-0 my-2 my-md-1">
			
			
			<li class="nav-item mr-2 mr-lg-4 mt-1 mt-lg-0">
				
				
				
				
				
				
				<a class="nav-link" href="/kubeflow-docs/docs/" ><span>Docs</span></a>
			</li>
			
			<li class="nav-item mr-2 mr-lg-4 mt-1 mt-lg-0">
				
				
				
				
				
				
				<a class="nav-link" href="/kubeflow-docs/news/" ><span>News</span></a>
			</li>
			
			<li class="nav-item mr-2 mr-lg-4 mt-1 mt-lg-0">
				
				
				
				
				
				
				<a class="nav-link" href="https://github.com/GoogleCloudPlatform/kubeflow-distribution" target="_blank" ><i class='fa-brands fa-github pr-2'></i><span>Manifests</span></a>
			</li>
			
			
			<li class="nav-item dropdown mt-1 mt-lg-0 mr-2">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	dev
</a>
<div class="dropdown-menu dropdown-menu-md-right dropdown-menu-lg-left" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/docs">v1.6 (latest)</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/v1.5/docs">v1.5</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/v1.4/docs">v1.4</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/dev/docs">dev</a>
	
</div>

			</li>
			
			
		</ul>
	</div>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This is the multi-page printable view of this section.
<a href="#" onclick="print();return false;">Click here to print</a>.
</p><p>
<a href="/kubeflow-docs/docs/">Return to the regular view of this page</a>.
</p>
</div>



<h1 class="title">Kubeflow on Google Cloud</h1>
<div class="lead">Running Kubeflow on Kubernetes Engine and Google Cloud Platform</div>




    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-1ac23283ce7f3012a623fab0370bff64">Deployment</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>1.1: <a href="#pg-801b64aec43e775f0c9b6e3dc100bdbd">Overview</a></li>


    
  
    
    
	
<li>1.2: <a href="#pg-4007501063b92f590822dfd88381b1a3">Setting up GCP Project</a></li>


    
  
    
    
	
<li>1.3: <a href="#pg-a2544b3f4602c9ce9d6a5f442a5bd281">Setting up OAuth client</a></li>


    
  
    
    
	
<li>1.4: <a href="#pg-b0b82cb1e2fd9d83177abd017cb5231c">Deploying Management cluster</a></li>


    
  
    
    
	
<li>1.5: <a href="#pg-ee52ce1a23681fe0132afd393399fa02">Deploying Kubeflow cluster</a></li>


    
  
    
    
	
<li>1.6: <a href="#pg-1a4842b2ce53fddfdc881ee75b5de98b">Upgrade Kubeflow</a></li>


    
  
    
    
	
<li>1.7: <a href="#pg-b01cda5c30a4cd24f94100af83fdaaa2">Monitoring Cloud IAP Setup</a></li>


    
  
    
    
	
<li>1.8: <a href="#pg-f10ecd64c7b37a30c6eb2c6c15ae2ca2">Deleting Kubeflow</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2: <a href="#pg-31cedb300807aeaac671a176f8cd692b">Pipelines on Google Cloud</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1: <a href="#pg-0b0498d20bf77acec45ad87018b21f47">Connecting to Kubeflow Pipelines on Google Cloud using the SDK</a></li>


    
  
    
    
	
<li>2.2: <a href="#pg-25e5f676d4864c88d4a268fec2aaca06">Authenticating Pipelines to Google Cloud</a></li>


    
  
    
    
	
<li>2.3: <a href="#pg-e8f58dfedb7f24d2f30aab8ff214e5d9">Upgrading</a></li>


    
  
    
    
	
<li>2.4: <a href="#pg-d79816c8e6fac2f07383ee4b56fd17de">Enabling GPU and TPU</a></li>


    
  
    
    
	
<li>2.5: <a href="#pg-ee70eee465844b15b58f4b4844b00eab">Using Preemptible VMs and GPUs on Google Cloud</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3: <a href="#pg-0d94edcf690e8a4204c514d9d35c3f61">Customize Kubeflow on GKE</a></li>


    
  
    
    
	
<li>4: <a href="#pg-969055f2547f60f12c1c4256f95b4eeb">Using Your Own Domain</a></li>


    
  
    
    
	
<li>5: <a href="#pg-361f92eed2eaa3203409b5ccbdfa0802">Authenticating Kubeflow to Google Cloud</a></li>


    
  
    
    
	
<li>6: <a href="#pg-c89dbb2f91c73dbff2bd78b4bc0de1d4">Securing Your Clusters</a></li>


    
  
    
    
	
<li>7: <a href="#pg-2ff67fee0173608876ccc012bf3ff6ce">Troubleshooting Deployments on GKE</a></li>


    
  
    
    
	
<li>8: <a href="#pg-49d3fab273dc187c2097b2cf2ff485fb">Kubeflow On-premises on Anthos</a></li>


    
  
    
    
	
<li>9: <a href="#pg-d9ec0777c4f7a8a41476a65f2ecca6f7">Changelog</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-1ac23283ce7f3012a623fab0370bff64">1 - Deployment</h1>
    <div class="lead">Instructions for deploying Kubeflow on Google Cloud</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-801b64aec43e775f0c9b6e3dc100bdbd">1.1 - Overview</h1>
    <div class="lead">Full fledged Kubeflow deployment on Google Cloud</div>
	<p>This guide describes how to deploy Kubeflow and a series of Kubeflow components on Google Kubernetes Engine (GKE).</p>
<!-- If you want to use Kubeflow Pipelines only, refer to [Installation Options for Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/installation/overview/) for choosing an installation option.
-->
<h2 id="features">Features</h2>
<p>Kubeflow deployed on Google Cloud includes the following:</p>
<ol>
<li>Full-fledged multi-user Kubeflow running on GKE.</li>
<li><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler">Cluster Autoscaler</a>
with automatic resizing of the node pool.</li>
<li><a href="https://cloud.google.com/endpoints/docs">Cloud Endpoint</a> integrated with <a href="https://cloud.google.com/iap">Identity-aware Proxy (IAP)</a>.</li>
<li>GPU and <a href="https://cloud.google.com/tpu/">Cloud TPU</a> accelerated nodes available for your Machine Learning (ML) workloads.</li>
<li><a href="https://cloud.google.com/logging/docs/">Cloud Logging</a> for easy debugging and troubleshooting.</li>
<li>Other managed services offered by Google Cloud, such as <a href="https://cloud.google.com/storage">Cloud Storage</a>, <a href="https://cloud.google.com/sql">Cloud SQL</a>, <a href="https://cloud.google.com/anthos/service-mesh">Anthos Service Mesh</a>, <a href="https://cloud.google.com/iam">Identity and Access Management (IAM)</a>, <a href="https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview">Config Controller</a>, and so on.</li>
</ol>
<p><img src=".{{ .Site.Params.version_url_prefix }}/docs/images/gke/full-kf-home.png" 
alt="Full Kubeflow Central Dashboard"
class="mt-3 mb-3 border border-info rounded"></p>
<h2 id="management-cluster">Management cluster</h2>
<p>Kubeflow on GCP employs management cluster, which allows you to manage Google Cloud resources via <a href="https://cloud.google.com/config-connector/docs/overview">Config Connector</a>. The management cluster is independent from Kubeflow cluster. Its purpose is to manage Kubeflow clusters (see figure below). The management cluster can live in a different Google Cloud project by assigning owner permission to the associated service account.</p>
<p><img src="./{{ .Site.Params.version_url_prefix }}docs/images/gke/full-deployment-structure.png" 
alt="Full Kubeflow deployment structure"
class="mt-3 mb-3 border border-info rounded"></p>
<h2 id="deployment-process">Deployment process</h2>
<p>Follow the steps below to set up Kubeflow environment on Google Cloud. Some of these steps are one-time only, for example: OAuth Client can be shared by multiple Kubeflow clusters in the same Google Cloud project.</p>
<ol>
<li>[Set up Google Cloud project](/{{ .Site.Params.version_url_prefix }}docs/deploy/project-setup/).</li>
<li>[Set up OAuth Client](/{{ .Site.Params.version_url_prefix }}docs/deploy/oauth-setup/).</li>
<li>[Deploy Management Cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/management-setup/).</li>
<li>[Deploy Kubeflow Cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli/).</li>
</ol>
<p>If you encounter any issues during the deployment steps, refer to [Troubleshooting deployments](/{{ .Site.Params.version_url_prefix }}docs/troubleshooting/) to find common issues
and debugging approaches. If this issue is new, file a bug to <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution">GoogleCloudPlatform/kubeflow-distribution</a>.</p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>Repeat [Deploy Kubeflow Cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli/) if you want to deploy another Kubeflow cluster.</li>
<li>Run a full ML workflow on Kubeflow, using the <a href="https://github.com/kubeflow/pipelines/blob/e42d9d2609369b96973c821dca11fe5b2565e705/samples/contrib/kubeflow-e2e-mnist/kubeflow-e2e-mnist.ipynb">end-to-end MNIST notebook</a>.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4007501063b92f590822dfd88381b1a3">1.2 - Setting up GCP Project</h1>
    <div class="lead">Creating a Google Cloud project for your Kubeflow deployment</div>
	<p>In order to deploy Kubeflow on GCP, you need to set up a Google Cloud project and enable necessary APIs for the deployment.</p>
<h2 id="setting-up-a-project">Setting up a project</h2>
<p>Follow these steps to set up your Google Cloud project:</p>
<ul>
<li>
<p>Select or create a project on the
<a href="https://console.cloud.google.com/cloud-resource-manager">Google Cloud Console</a>. If you plan to use different Google Cloud projects for <strong>Management Cluster</strong> and <strong>Kubeflow Clusters</strong>: create <strong>one Management project</strong> for Management Cluster, and create <strong>one or more Kubeflow projects</strong> for Kubeflow Clusters.</p>
</li>
<li>
<p>Make sure that you have the
<a href="https://cloud.google.com/iam/docs/understanding-roles#primitive_role_definitions">Owner role</a>
for the project in Cloud IAM (Identity and Access Management).
The deployment process creates various service accounts with
appropriate roles in order to enable seamless integration with
Google Cloud services. This process requires that you have the
owner role for the project in order to deploy Kubeflow.</p>
</li>
<li>
<p>Make sure that billing is enabled for your project. Refer to
<a href="https://cloud.google.com/billing/docs/how-to/modify-project">Enable billing for a project</a>.</p>
</li>
<li>
<p>Enable the following APIs by running the following command in a Cloud Shell or local terminal (needs to be authenticated via <code>gcloud auth login</code>):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud services <span class="nb">enable</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  serviceusage.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  compute.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  container.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  iam.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  servicemanagement.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  cloudresourcemanager.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  ml.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  iap.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  sqladmin.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  meshconfig.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  krmapihosting.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  servicecontrol.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  endpoints.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  cloudbuild.googleapis.com
</span></span></code></pre></div></li>
</ul>
<p>Alternatively, you can these APIs can be enabled via Google Cloud Console:</p>
<pre><code>* [Service Usage API](https://cloud.google.com/service-usage/docs/reference/rest)
* [Compute Engine API](https://console.cloud.google.com/apis/library/compute.googleapis.com)
* [Kubernetes Engine API](https://console.cloud.google.com/apis/library/container.googleapis.com)
* [Identity and Access Management (IAM) API](https://console.cloud.google.com/apis/library/iam.googleapis.com)
* [Service Management API](https://console.cloud.google.com/apis/api/servicemanagement.googleapis.com)
* [Cloud Resource Manager API](https://console.developers.google.com/apis/library/cloudresourcemanager.googleapis.com)
* [AI Platform Training &amp; Prediction API](https://console.developers.google.com/apis/library/ml.googleapis.com)
* [Cloud Identity-Aware Proxy API](https://console.cloud.google.com/apis/library/iap.googleapis.com)
* [Cloud Build API](https://console.cloud.google.com/apis/library/cloudbuild.googleapis.com)
* [Cloud SQL Admin API](https://console.cloud.google.com/apis/library/sqladmin.googleapis.com)
* [Config Controller (KRM API Hosting API)](https://console.cloud.google.com/apis/library/krmapihosting.googleapis.com)
* [Service Control API](https://console.cloud.google.com/apis/library/servicecontrol.googleapis.com)
* [Google Cloud Endpoints](https://console.cloud.google.com/apis/library/endpoints.googleapis.com)
</code></pre>
<ul>
<li>
<p>If you are using the
<a href="https://cloud.google.com/free/docs/gcp-free-tier">Google Cloud Free Program</a> or the
12-month trial period with $300 credit, note that the free tier does not offer enough
resources for default full Kubeflow installation. You need to
<a href="https://cloud.google.com/free/docs/gcp-free-tier#how-to-upgrade">upgrade to a paid account</a>.</p>
<p>For more information, see the following issues:</p>
<ul>
<li><a href="https://github.com/kubeflow/website/issues/1065">kubeflow/website #1065</a>
reports the problem.</li>
<li><a href="https://github.com/kubeflow/kubeflow/issues/3936">kubeflow/kubeflow #3936</a>
requests a Kubeflow configuration to work with a free trial project.</li>
</ul>
<p>Read the Google Cloud <a href="https://cloud.google.com/compute/quotas">Resource quotas</a>
to understand quotas on resource usage that Compute Engine enforces, and
to learn how to check and increase your quotas.</p>
</li>
<li>
<p>Initialize your project to prepare it for Anthos Service Mesh installation:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">PROJECT_ID</span><span class="o">=</span>&lt;YOUR_PROJECT_ID&gt;
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl --request POST <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --header <span class="s2">&#34;Authorization: Bearer </span><span class="k">$(</span>gcloud auth print-access-token<span class="k">)</span><span class="s2">&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --data <span class="s1">&#39;&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  https://meshconfig.googleapis.com/v1alpha1/projects/<span class="si">${</span><span class="nv">PROJECT_ID</span><span class="si">}</span>:initialize
</span></span></code></pre></div><p>Refer to <a href="https://cloud.google.com/service-mesh/docs/archive/1.4/docs/gke-install-new-cluster#setting_credentials_and_permissions">Anthos Service Mesh documentation</a> for details.</p>
<p>If you encounter a <code>Workload Identity Pool does not exist</code> error, refer to the following issue:</p>
<ul>
<li><a href="https://github.com/kubeflow/website/issues/2121">kubeflow/website #2121</a>
describes that creating and then removing a temporary Kubernetes cluster may
be needed for projects that haven&rsquo;t had a cluster set up beforehand.</li>
</ul>
</li>
</ul>
<p>You do not need a running GKE cluster. The deployment process creates a
cluster for you.</p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>[Set up an OAuth credential](/{{ .Site.Params.version_url_prefix }}docs/deploy/oauth-setup) to use
<a href="https://cloud.google.com/iap/docs/">Cloud Identity-Aware Proxy (Cloud IAP)</a>.
Cloud IAP is recommended for production deployments or deployments with access
to sensitive data.</li>
<li>[Set up Management Cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/management-setup) to deploy and manage Kubeflow clusters.</li>
<li>[Deploy Kubeflow](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli) using kubectl, kustomize and kpt.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a2544b3f4602c9ce9d6a5f442a5bd281">1.3 - Setting up OAuth client</h1>
    <div class="lead">Creating an OAuth client for Cloud IAP on Google Cloud</div>
	<h2 id="set-up-oauth-consent-screen-and-client-credential">Set up OAuth Consent Screen and Client Credential</h2>
<p>If you want to use
<a href="https://cloud.google.com/iap/docs/">Cloud Identity-Aware Proxy (Cloud IAP)</a>
when deploying Kubeflow on Google Cloud,
then you must follow these instructions to create an OAuth client for use
with Kubeflow.</p>
<p>Cloud IAP is recommended for production deployments or deployments with access
to sensitive data.</p>
<p>Follow the steps below to create an OAuth client ID that identifies Cloud IAP
when requesting access to a user&rsquo;s email account. Kubeflow uses the email
address to verify the user&rsquo;s identity.</p>
<ol>
<li>
<p>Set up your OAuth <a href="https://console.cloud.google.com/apis/credentials/consent">consent screen</a>:</p>
<ul>
<li>
<p>In the <strong>Application name</strong> box, enter the name of your application.
The example below uses the name &ldquo;Kubeflow&rdquo;.</p>
</li>
<li>
<p>Under <strong>Support email</strong>, select the email address that you want to display
as a public contact. You must use either your email address or a Google
Group that you own.</p>
</li>
<li>
<p>If you see <strong>Authorized domains</strong>, enter</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;project&gt;.cloud.goog
</span></span></code></pre></div><ul>
<li>where &lt;project&gt; is your Google Cloud project ID.</li>
<li>If you are using your own domain, such as <strong>acme.com</strong>, you should add
that as well</li>
<li>The <strong>Authorized domains</strong> option appears only for certain project
configurations. If you don&rsquo;t see the option, then there&rsquo;s nothing you
need to set.</li>
</ul>
</li>
<li>
<p>Click <strong>Save</strong>.</p>
</li>
<li>
<p>Here&rsquo;s an example of the completed form:<br>
<img src="/{{ .Site.Params.version_url_prefix }}docs/images/consent-screen.png" 
alt="OAuth consent screen"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
</ul>
</li>
<li>
<p>On the <a href="https://console.cloud.google.com/apis/credentials">credentials screen</a>:</p>
<ul>
<li>Click <strong>Create credentials</strong>, and then click <strong>OAuth client ID</strong>.</li>
<li>Under <strong>Application type</strong>, select <strong>Web application</strong>.</li>
<li>In the <strong>Name</strong> box enter any name for your OAuth client ID. This is <em>not</em>
the name of your application nor the name of your Kubeflow deployment. It&rsquo;s
just a way to help you identify the OAuth client ID.</li>
</ul>
</li>
<li>
<p>Click <strong>Create</strong>. A dialog box appears, like the one below:</p>
<p><img src="/{{ .Site.Params.version_url_prefix }}docs/images/new-oauth.png" 
alt="OAuth consent screen"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
<li>
<p>Copy the <strong>client ID</strong> shown in the dialog box, because you need the client
ID in the next step.</p>
</li>
<li>
<p>On the <strong>Create credentials</strong> screen, find your newly created OAuth
credential and click the pencil icon to edit it:</p>
<p><img src="/{{ .Site.Params.version_url_prefix }}docs/images/oauth-edit.png" 
alt="OAuth consent screen"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
<li>
<p>In the <strong>Authorized redirect URIs</strong> box, enter the following (if it&rsquo;s not
already present in the list of authorized redirect URIs):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://iap.googleapis.com/v1/oauth/clientIds/&lt;CLIENT_ID&gt;:handleRedirect
</span></span></code></pre></div><ul>
<li><code>&lt;CLIENT_ID&gt;</code> is the OAuth client ID that you copied from the dialog box in
step four. It looks like <code>XXX.apps.googleusercontent.com</code>.</li>
<li>Note that the URI is not dependent on the Kubeflow deployment or endpoint.
Multiple Kubeflow deployments can share the same OAuth client without the
need to modify the redirect URIs.</li>
</ul>
</li>
<li>
<p>Press <strong>Enter/Return</strong> to add the URI. Check that the URI now appears as
a confirmed item under <strong>Authorized redirect URIs</strong>. (The URI should no longer be
editable.)</p>
<p>Here&rsquo;s an example of the completed form:
<img src="/{{ .Site.Params.version_url_prefix }}docs/images/oauth-credential.png" 
alt="OAuth credentials"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
<li>
<p>Click <strong>Save</strong>.</p>
</li>
<li>
<p>Make note that you can find your OAuth client credentials in the credentials
section of the Google Cloud Console. You need to retrieve the <strong>client ID</strong> and
<strong>client secret</strong> later when you&rsquo;re ready to enable Cloud IAP.</p>
</li>
</ol>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>[Set up your management cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/management-setup/).</li>
<li><a href="https://cloud.google.com/iam/docs/granting-changing-revoking-access#granting-console">Grant your users the IAP-secured Web App User IAM role</a> so they can access the Kubeflow console through IAP.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b0b82cb1e2fd9d83177abd017cb5231c">1.4 - Deploying Management cluster</h1>
    <div class="lead">Setting up a management cluster on Google Cloud</div>
	<p>This guide describes how to setup a management cluster which you will use to deploy one or more instances of Kubeflow.</p>
<p>The management cluster is used to run <a href="https://cloud.google.com/config-connector/docs/overview">Cloud Config Connector</a>. Cloud Config Connector is a Kubernetes addon that allows you to manage Google Cloud resources through Kubernetes.</p>
<p>While the management cluster can be deployed in the same project as your Kubeflow cluster, typically you will want to deploy
it in a separate project used for administering one or more Kubeflow instances, because it will run with escalated permissions to create Google Cloud resources in the managed projects.</p>
<p>Optionally, the cluster can be configured with <a href="https://cloud.google.com/anthos-config-management/docs">Anthos Config Management</a>
to manage Google Cloud infrastructure using GitOps.</p>
<h2 id="deployment-steps">Deployment steps</h2>
<h3 id="install-the-required-tools">Install the required tools</h3>
<ol>
<li>
<p><a href="https://cloud.google.com/sdk/docs/components">gcloud components</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud components install kubectl kustomize kpt anthoscli beta
</span></span><span class="line"><span class="cl">gcloud components update
</span></span><span class="line"><span class="cl"><span class="c1"># If the output said the Cloud SDK component manager is disabled for installation, copy the command from output and run it.</span>
</span></span></code></pre></div><p>You can install specific version of kubectl by following instruction (Example: <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/">Install kubectl on Linux</a>). Latest patch version of kubectl from <code>v1.17</code> to <code>v1.19</code> works well too.</p>
<p>Note: Starting from Kubeflow 1.4, it requires <code>kpt v1.0.0-beta.6</code> or above to operate in <code>GoogleCloudPlatform/kubeflow-distribution</code> repository. gcloud hasn&rsquo;t caught up with this kpt version yet, <a href="https://kpt.dev/installation/">install kpt</a> separately from <a href="https://github.com/GoogleContainerTools/kpt/tags">https://github.com/GoogleContainerTools/kpt/tags</a> for now. Note that kpt requires docker to be installed.</p>
</li>
</ol>
<h3 id="fetch-googlecloudplatformkubeflow-distribution-package">Fetch GoogleCloudPlatform/kubeflow-distribution package</h3>
<p>The management cluster manifests live in GitHub repository <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/">GoogleCloudPlatform/kubeflow-distribution</a>, use the following commands to pull Kubeflow manifests:</p>
<ol>
<li>
<p>Clone the GitHub repository and check out the v1.6.1 tag:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git clone https://github.com/GoogleCloudPlatform/kubeflow-distribution.git 
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kubeflow-distribution
</span></span><span class="line"><span class="cl">git checkout tags/v1.6.1 -b v1.6.1
</span></span></code></pre></div><p>Alternatively, you can get the package by using <code>kpt</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Check out Kubeflow v1.6.1 blueprints</span>
</span></span><span class="line"><span class="cl">kpt pkg get https://github.com/GoogleCloudPlatform/kubeflow-distribution.git@v1.6.1 kubeflow-distribution
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kubeflow-distribution
</span></span></code></pre></div></li>
<li>
<p>Go to <code>kubeflow-distribution/management</code> directory for Management cluster configurations.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> management
</span></span></code></pre></div></li>
</ol>


<div class="alert alert-info" role="alert">
<h4 class="alert-heading">Tip</h4>

    To continuously manage the management cluster, you are recommended to check
the management configuration directory into source control. For example, <code>MGMT_DIR=~/kubeflow-distribution/management/</code>.

</div>

<h3 id="configure-environment-variables">Configure Environment Variables</h3>
<p>Fill in environment variables in <code>kubeflow-distribution/management/env.sh</code> as followed:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">MGMT_PROJECT</span><span class="o">=</span>&lt;the project where you deploy your management cluster&gt;
</span></span><span class="line"><span class="cl"><span class="nv">MGMT_NAME</span><span class="o">=</span>&lt;name of your management cluster&gt;
</span></span><span class="line"><span class="cl"><span class="nv">LOCATION</span><span class="o">=</span>&lt;location of your management cluster, use either us-central1 or us-east1&gt;
</span></span></code></pre></div><p>And run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">source</span> env.sh
</span></span></code></pre></div><p>This guide assumes the following convention:</p>
<ul>
<li>
<p>The <code>${MGMT_PROJECT}</code> environment variable contains the Google Cloud project
ID where management cluster is deployed to.</p>
</li>
<li>
<p><code>${MGMT_NAME}</code> is the cluster name of your management cluster and the prefix for other Google Cloud resources created in the deployment process. Management cluster
should be a different cluster from your Kubeflow cluster.</p>
<p>Note, <code>${MGMT_NAME}</code> should</p>
<ul>
<li>start with a lowercase letter</li>
<li>only contain lowercase letters, numbers and <code>-</code></li>
<li>end with a number or a letter</li>
<li>contain no more than 18 characters</li>
</ul>
</li>
<li>
<p>The <code>${LOCATION}</code> environment variable contains the location of your management cluster.
you can choose between regional or zonal, see <a href="https://cloud.google.com/compute/docs/regions-zones#available">Available regions and zones</a>.</p>
</li>
</ul>
<h3 id="configure-kpt-setter-values">Configure kpt setter values</h3>
<p>Use kpt to <a href="https://catalog.kpt.dev/apply-setters/v0.2/">set values</a> for the name, project, and location of your management cluster. Run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">bash kpt-set.sh
</span></span></code></pre></div><p>Note, you can find out which setters exist in a package and what their
current values are by running the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kpt fn <span class="nb">eval</span> -i list-setters:v0.1 ./manifests
</span></span></code></pre></div><h3 id="prerequisite-for-config-controller">Prerequisite for Config Controller</h3>
<p>In order to deploy Google Cloud Services like Kubernetes resources, we need to create a management cluster with Config Controller installed. Follow <a href="https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-setup#before_you_begin">Before you begin</a> to create default network if not existed. Make sure to use <code>${MGMT_PROJECT}</code> for PROJECT_ID.</p>
<h3 id="deploy-management-cluster">Deploy Management Cluster</h3>
<ol>
<li>
<p>Deploy the management cluster by applying cluster resources:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make create-cluster
</span></span></code></pre></div></li>
<li>
<p>Create a kubectl <strong>context</strong> for the management cluster, it will be named <code>${MGMT_NAME}</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make create-context
</span></span></code></pre></div></li>
<li>
<p>Grant permission to Config Controller service account:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make grant-owner-permission
</span></span></code></pre></div><p>Config Controller has created a default service account, this step grants owner permission to this service account in order to
allow Config Controller to manage Google Cloud resources. Refer to <a href="https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-setup#set_up">Config Controller setup</a>.</p>
</li>
</ol>
<h2 id="understanding-the-deployment-process">Understanding the deployment process</h2>
<p>This section gives you more details about the configuration and
deployment process, so that you can customize your management cluster if necessary.</p>
<h3 id="config-controller">Config Controller</h3>
<p>Management cluster is a tool for managing Google Cloud services like KRM, for example: GKE container cluster, MySQL database, etc.
And you can use one Management cluster for multiple Kubeflow clusters, across multiple Google Cloud projects.
This capability is offered by <a href="https://cloud.google.com/config-connector/docs/how-to/getting-started">Config Connector</a>.</p>
<p>Starting with Kubeflow 1.5, we leveraged the managed version of Config Connector, which is called <a href="https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview">Config Controller</a>.
Therefore, The Management cluster is the Config Controller cluster deployed using <a href="https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-setup">Config Controller setup</a> process.
Note that you can create only one Management cluster within a Google Cloud project, and you usually need just one Management cluster.</p>
<h3 id="management-cluster-layout">Management cluster layout</h3>
<p>Inside the Config Controller, we manage Google Cloud resources in namespace mode. That means one namespace is responsible to manage Google Cloud resources deployed to the Google Cloud project with the same name. Your management cluster contains following namespaces:</p>
<ol>
<li>config-control</li>
<li>namespace with the same name as your Kubeflow clusters&rsquo; Google Cloud project name</li>
</ol>
<p><code>config-control</code> is the default namespace which is installed while creating Management cluster, you have granted the default service account (like <code>service-&lt;management-project-id&gt;@gcp-sa-yakima.iam.gserviceaccount.com</code>)
within this project to manage Config Connector. It is the prerequisite for managing resources in other Google Cloud projects.</p>
<p><code>namespace with the same name as your Kubeflow clusters' Google Cloud project name</code> is the resource pool for Kubeflow cluster&rsquo;s Google Cloud project.
For each Kubeflow Google Cloud project, you will have service account with pattern <code>kcc-&lt;kf-project-name&gt;@&lt;management-project-name&gt;.iam.gserviceaccount.com</code> in <code>config-control</code> namespace, and it needs to have owner permission to <code>${KF_PROJECT}</code>, you will perform this step during [Deploy Kubeflow cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli/). After setup, your Google Cloud resources in Kubeflow cluster project will be deployed to the namespace with name <code>${KF_PROJECT}</code> in the management cluster.</p>
<p>Your management cluster directory contains the following file:</p>
<ul>
<li><strong>Makefile</strong> is a file that defines rules to automate deployment process. You can refer to <a href="https://www.gnu.org/software/make/manual/make.html#Introduction">GNU make documentation</a> for more introduction. The Makefile we provide is designed to be user maintainable. You are encouraged to read, edit and maintain it to suit your own deployment customization needs.</li>
</ul>
<h3 id="debug">Debug</h3>
<p>If you encounter issue creating Google Cloud resources using Config Controller. You can list resources in the <code>${KF_PROJECT}</code> namespace of management cluster to learn about the detail.
Learn more with <a href="https://cloud.google.com/config-connector/docs/how-to/monitoring-your-resources">Monitoring your resources</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span> get all -n <span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># If you want to check the service account creation status</span>
</span></span><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span> get IAMServiceAccount -n <span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span> get IAMServiceAccount &lt;service-account-name&gt; -n <span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span> -oyaml
</span></span></code></pre></div><h3 id="faqs">FAQs</h3>
<ul>
<li>
<p>Where is <code>kfctl</code>?</p>
<p><code>kfctl</code> is no longer being used to apply resources for Google Cloud, because required functionalities are now supported by generic tools including <a href="https://www.gnu.org/software/make/">Make</a>, <a href="https://kustomize.io">Kustomize</a>, <a href="https://googlecontainertools.github.io/kpt/">kpt</a>, and <a href="https://cloud.google.com/config-connector/docs/overview">Cloud Config Connector</a>.</p>
</li>
<li>
<p>Why do we use an extra management cluster to manage Google Cloud resources?</p>
<p>The management cluster is very lightweight cluster that runs <a href="https://cloud.google.com/config-connector/docs/overview">Cloud Config Connector</a>. Cloud Config Connector makes it easier to configure Google Cloud resources using YAML and Kustomize.</p>
</li>
</ul>
<p>For a more detailed explanation of the drastic changes happened in Kubeflow v1.1 on Google Cloud, read <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/issues/123">GoogleCloudPlatform/kubeflow-distribution #123</a>.</p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>[Deploy Kubeflow](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli) using kubectl, kustomize and kpt.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ee52ce1a23681fe0132afd393399fa02">1.5 - Deploying Kubeflow cluster</h1>
    <div class="lead">Instructions for using kubectl and kpt to deploy Kubeflow on Google Cloud</div>
	<p>This guide describes how to use <code>kubectl</code> and <a href="https://googlecontainertools.github.io/kpt/">kpt</a> to
deploy Kubeflow on Google Cloud.</p>
<h2 id="deployment-steps">Deployment steps</h2>
<h3 id="prerequisites">Prerequisites</h3>
<p>Before installing Kubeflow on the command line:</p>
<ol>
<li>
<p>You must have created a management cluster and installed Config Connector.</p>
<ul>
<li>
<p>If you don&rsquo;t have a management cluster follow the [instructions](/{{ .Site.Params.version_url_prefix }}docs/deploy/management-setup/)</p>
</li>
<li>
<p>Your management cluster will need a namespace setup to administer the Google Cloud project where Kubeflow will be deployed. This step will be included in later step of current page.</p>
</li>
</ul>
</li>
<li>
<p>You need to use Linux or <a href="https://cloud.google.com/shell/">Cloud Shell</a> for ASM installation. Currently ASM installation doesn&rsquo;t work on macOS because it <a href="https://cloud.google.com/service-mesh/docs/scripted-install/asm-onboarding#installing_required_tools">comes with an old version of bash</a>.</p>
</li>
<li>
<p>Make sure that your Google Cloud project meets the minimum requirements
described in the [project setup guide](/{{ .Site.Params.version_url_prefix }}docs/deploy/project-setup/).</p>
</li>
<li>
<p>Follow the guide
[setting up OAuth credentials](/{{ .Site.Params.version_url_prefix }}docs/deploy/oauth-setup/)
to create OAuth credentials for <a href="https://cloud.google.com/iap/docs/">Cloud Identity-Aware Proxy (Cloud
IAP)</a>.</p>
<ul>
<li>Unfortunately <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/backendconfig">GKE&rsquo;s BackendConfig</a>
currently doesn&rsquo;t support creating <a href="https://cloud.google.com/iap/docs/programmatic-oauth-clients">IAP OAuth clients programmatically</a>.</li>
</ul>
</li>
</ol>
<h3 id="install-the-required-tools">Install the required tools</h3>
<ol>
<li>
<p>Install <a href="https://cloud.google.com/sdk/">gcloud</a>.</p>
</li>
<li>
<p>Install gcloud components</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud components install kubectl kustomize kpt anthoscli beta
</span></span><span class="line"><span class="cl">gcloud components update
</span></span></code></pre></div><p>You can install specific version of kubectl by following instruction (Example: <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/">Install kubectl on Linux</a>). Latest patch version of kubectl from <code>v1.17</code> to <code>v1.19</code> works well too.</p>
<p>Note: Starting from Kubeflow 1.4, it requires <code>kpt v1.0.0-beta.6</code> or above to operate in <code>GoogleCloudPlatform/kubeflow-distribution</code> repository. gcloud hasn&rsquo;t caught up with this kpt version yet, <a href="https://kpt.dev/installation/">install kpt</a> separately from <a href="https://github.com/GoogleContainerTools/kpt/tags">https://github.com/GoogleContainerTools/kpt/tags</a> for now. Note that kpt requires docker to be installed.</p>
<p>Note: You also need to <a href="https://cloud.google.com/service-mesh/v1.10/docs/scripted-install/asm-onboarding#installing_required_tools">install required tools</a> for ASM installation tool <code>install_asm</code>.</p>
</li>
</ol>
<h3 id="fetch-googlecloudplatformkubeflow-distribution-and-upstream-packages">Fetch GoogleCloudPlatform/kubeflow-distribution and upstream packages</h3>
<ol>
<li>
<p>If you have already installed Management cluster, you have <code>GoogleCloudPlatform/kubeflow-distribution</code> locally. You just need to run <code>cd kubeflow</code> to access Kubeflow cluster manifests. Otherwise, you can run the following commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Check out Kubeflow v1.6.1 blueprints</span>
</span></span><span class="line"><span class="cl">git clone https://github.com/GoogleCloudPlatform/kubeflow-distribution.git 
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kubeflow-distribution
</span></span><span class="line"><span class="cl">git checkout tags/v1.6.1 -b v1.6.1
</span></span></code></pre></div><p>Alternatively, you can get the package by using <code>kpt</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Check out Kubeflow v1.6.1 blueprints</span>
</span></span><span class="line"><span class="cl">kpt pkg get https://github.com/GoogleCloudPlatform/kubeflow-distribution.git@v1.6.1 kubeflow-distribution
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kubeflow-distribution
</span></span></code></pre></div></li>
<li>
<p>Run the following command to pull upstream manifests from <code>kubeflow/manifests</code> repository.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Visit Kubeflow cluster related manifests</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kubeflow
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">bash ./pull-upstream.sh
</span></span></code></pre></div></li>
</ol>
<h3 id="environment-variables">Environment Variables</h3>
<p>Log in to gcloud. You only need to run this command once:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud auth login
</span></span></code></pre></div><ol>
<li>
<p>Review and fill all the environment variables in <code>kubeflow-distribution/kubeflow/env.sh</code>, they will be used by <code>kpt</code> later on, and some of them will be used in this deployment guide. Review the comment in <code>env.sh</code> for the explanation for each environment variable. After defining these environment variables, run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">source</span> env.sh
</span></span></code></pre></div></li>
<li>
<p>Set environment variables with OAuth Client ID and Secret for IAP:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLIENT_ID</span><span class="o">=</span>&lt;Your CLIENT_ID&gt;
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLIENT_SECRET</span><span class="o">=</span>&lt;Your CLIENT_SECRET&gt;
</span></span></code></pre></div>

<div class="alert alert-primary" role="alert">
<h4 class="alert-heading">Note</h4>

    Do not omit the <b>export</b> because scripts triggered by <b>make</b> need these environment variables. Do not check in these two environment variables configuration to source control, they are secrets.

</div>

</li>
</ol>
<h4 id="kpt-setter-config">kpt setter config</h4>
<p>Run the following commands to configure kpt setter for your Kubeflow cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">bash ./kpt-set.sh
</span></span></code></pre></div><p>Everytime you change environment variables, make sure you run the command above to apply
kpt setter change to all packages. Otherwise, kustomize build will not be able to pick up
new changes.</p>
<p>Note, you can find out which setters exist in a package and their
current values by running the following commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kpt fn <span class="nb">eval</span> -i list-setters:v0.1 ./apps
</span></span><span class="line"><span class="cl">kpt fn <span class="nb">eval</span> -i list-setters:v0.1 ./common
</span></span></code></pre></div><p>You can learn more about <code>list-setters</code> in <a href="https://catalog.kpt.dev/list-setters/v0.1/">kpt documentation</a>.</p>
<h4 id="authorize-cloud-config-connector-for-each-kubeflow-project">Authorize Cloud Config Connector for each Kubeflow project</h4>
<p>In the [Management cluster deployment](/{{ .Site.Params.version_url_prefix }}docs/deploy/management-setup/) we created the Google Cloud service account <strong>serviceAccount:kcc-${KF_PROJECT}@${MGMT_PROJECT}.iam.gserviceaccount.com</strong>
this is the service account that Config Connector will use to create any Google Cloud resources in <code>${KF_PROJECT}</code>. You need to grant this Google Cloud service account sufficient privileges to create the desired resources in Kubeflow project.
You only need to perform steps below once for each Kubeflow project, but make sure to do it even when KF_PROJECT and MGMT_PROJECT are the same project.</p>
<p>The easiest way to do this is to grant the Google Cloud service account owner permissions on one or more projects.</p>
<ol>
<li>
<p>Set the Management environment variable if you haven&rsquo;t:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">MGMT_PROJECT</span><span class="o">=</span>&lt;the project where you deploy your management cluster&gt;
</span></span><span class="line"><span class="cl"><span class="nv">MGMT_NAME</span><span class="o">=</span>&lt;the kubectl context name <span class="k">for</span> management cluster&gt;
</span></span></code></pre></div></li>
<li>
<p>Apply ConfigConnectorContext for <code>${KF_PROJECT}</code> in management cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply-kcc
</span></span></code></pre></div></li>
</ol>
<h3 id="configure-kubeflow">Configure Kubeflow</h3>
<p>Make sure you are using KF_PROJECT in the gcloud CLI tool:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud config <span class="nb">set</span> project <span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span>
</span></span></code></pre></div><h3 id="deploy-kubeflow">Deploy Kubeflow</h3>
<p>To deploy Kubeflow, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply
</span></span></code></pre></div><ul>
<li>
<p>If deployment returns an error due to missing resources in <code>serving.kserve.io</code> API group, rerun <code>make apply</code>. This is due to a race condition between CRD and runtime resources in KServe.</p>
<ul>
<li>This issue is being tracked in <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/issues/384">GoogleCloudPlatform/kubeflow-distribution#384</a></li>
</ul>
</li>
<li>
<p>If resources can&rsquo;t be created because <code>webhook.cert-manager.io</code> is unavailable wait and
then rerun <code>make apply</code></p>
<ul>
<li>This issue is being tracked in <a href="https://github.com/kubeflow/manifests/issues/1234">kubeflow/manifests#1234</a></li>
</ul>
</li>
<li>
<p>If resources can&rsquo;t be created with an error message like:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">error: unable to recognize <span class="s2">&#34;.build/application/app.k8s.io_v1beta1_application_application-controller-kubeflow.yaml&#34;</span>: no matches <span class="k">for</span> kind <span class="s2">&#34;Application&#34;</span> in version <span class="s2">&#34;app.k8s.io/v1beta1
</span></span></span></code></pre></div><p>This issue occurs when the CRD endpoint isn&rsquo;t established in the Kubernetes API server when the CRD&rsquo;s custom object is applied.
This issue is expected and can happen multiple times for different kinds of resource. To resolve this issue, try running <code>make apply</code> again.</p>
</li>
</ul>
<h3 id="check-your-deployment">Check your deployment</h3>
<p>Follow these steps to verify the deployment:</p>
<ol>
<li>
<p>When the deployment finishes, check the resources installed in the namespace
<code>kubeflow</code> in your new cluster.  To do this from the command line, first set
your <code>kubectl</code> credentials to point to the new cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud container clusters get-credentials <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span><span class="s2">&#34;</span> --zone <span class="s2">&#34;</span><span class="si">${</span><span class="nv">ZONE</span><span class="si">}</span><span class="s2">&#34;</span> --project <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div><p>Then, check what&rsquo;s installed in the <code>kubeflow</code> namespace of your GKE cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl -n kubeflow get all
</span></span></code></pre></div></li>
</ol>
<h3 id="access-the-kubeflow-user-interface-ui">Access the Kubeflow user interface (UI)</h3>
<p>To access the Kubeflow central dashboard, follow these steps:</p>
<ol>
<li>
<p>Use the following command to grant yourself the <a href="https://cloud.google.com/iap/docs/managing-access">IAP-secured Web App User</a> role:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span><span class="s2">&#34;</span> --member<span class="o">=</span>user:&lt;EMAIL&gt; --role<span class="o">=</span>roles/iap.httpsResourceAccessor
</span></span></code></pre></div><p>Note, you need the <code>IAP-secured Web App User</code> role even if you are already an owner or editor of the project. <code>IAP-secured Web App User</code> role is not implied by the <code>Project Owner</code> or <code>Project Editor</code> roles.</p>
</li>
<li>
<p>Enter the following URI into your browser address bar. It can take 20
minutes for the URI to become available: <code>https://${KF_NAME}.endpoints.${KF_PROJECT}.cloud.goog/</code></p>
<p>You can run the following command to get the URI for your deployment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl -n istio-system get ingress
</span></span><span class="line"><span class="cl">NAME            HOSTS                                                      ADDRESS         PORTS   AGE
</span></span><span class="line"><span class="cl">envoy-ingress   your-kubeflow-name.endpoints.your-gcp-project.cloud.goog   34.102.232.34   <span class="m">80</span>      5d13h
</span></span></code></pre></div><p>The following command sets an environment variable named <code>HOST</code> to the URI:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">HOST</span><span class="o">=</span><span class="k">$(</span>kubectl -n istio-system get ingress envoy-ingress -o<span class="o">=</span><span class="nv">jsonpath</span><span class="o">={</span>.spec.rules<span class="o">[</span>0<span class="o">]</span>.host<span class="o">}</span><span class="k">)</span>
</span></span></code></pre></div></li>
</ol>
<p>Notes:</p>
<ul>
<li>It can take 20 minutes for the URI to become available.
Kubeflow needs to provision a signed SSL certificate and register a DNS
name.</li>
<li>If you own or manage the domain or a subdomain with
<a href="https://cloud.google.com/dns/docs/">Cloud DNS</a>
then you can configure this process to be much faster.
Check <a href="https://github.com/kubeflow/kubeflow/issues/731">kubeflow/kubeflow#731</a>.</li>
</ul>
<h2 id="understanding-the-deployment-process">Understanding the deployment process</h2>
<p>This section gives you more details about the kubectl, kustomize, config connector configuration and
deployment process, so that you can customize your Kubeflow deployment if necessary.</p>
<h3 id="application-layout">Application layout</h3>
<p>Your Kubeflow application directory <code>kubeflow-distribution/kubeflow</code> contains the following files and
directories:</p>
<ul>
<li>
<p><strong>Makefile</strong> is a file that defines rules to automate deployment process. You can refer to <a href="https://www.gnu.org/software/make/manual/make.html#Introduction">GNU make documentation</a> for more introduction. The Makefile we provide is designed to be user maintainable. You are encouraged to read, edit and maintain it to suit your own deployment customization needs.</p>
</li>
<li>
<p><strong>apps</strong>, <strong>common</strong>, <strong>contrib</strong> are a series of independent components  directory containing kustomize packages for deploying Kubeflow components. The structure is to align with upstream <a href="https://github.com/kubeflow/manifests">kubeflow/manifests</a>.</p>
<ul>
<li>
<p><a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution">GoogleCloudPlatform/kubeflow-distribution</a> repository only stores <code>kustomization.yaml</code> and <code>patches</code> for Google Cloud specific resources.</p>
</li>
<li>
<p><code>./pull_upstream.sh</code> will pull <code>kubeflow/manifests</code> and store manifests in <code>upstream</code> folder of each component in this guide. <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution">GoogleCloudPlatform/kubeflow-distribution</a> repository doesn&rsquo;t store the copy of upstream manifests.</p>
</li>
</ul>
</li>
<li>
<p><strong>build</strong> is a directory that will contain the hydrated manifests outputted by
the <code>make</code> rules, each component will have its own <strong>build</strong> directory. You can customize the <strong>build</strong> path when calling <code>make</code> command.</p>
</li>
</ul>
<h3 id="source-control">Source Control</h3>
<p>It is recommended that you check in your entire local repository into source control.</p>
<p>Checking in <strong>build</strong> is recommended so you can easily see differences by <code>git diff</code> in manifests before applying them.</p>
<h2 id="google-cloud-service-accounts">Google Cloud service accounts</h2>
<p>The kfctl deployment process creates three service accounts in your
Google Cloud project. These service accounts follow the <a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">principle of least
privilege</a>.
The service accounts are:</p>
<ul>
<li><code>${KF_NAME}-admin</code> is used for some admin tasks like configuring the load
balancers. The principle is that this account is needed to deploy Kubeflow but
not needed to actually run jobs.</li>
<li><code>${KF_NAME}-user</code> is intended to be used by training jobs and models to access
Google Cloud resources (Cloud Storage, BigQuery, etc.). This account has a much smaller
set of privileges compared to <code>admin</code>.</li>
<li><code>${KF_NAME}-vm</code> is used only for the virtual machine (VM) service account. This
account has the minimal permissions needed to send metrics and logs to
<a href="https://cloud.google.com/stackdriver/">Stackdriver</a>.</li>
</ul>
<h2 id="upgrade-kubeflow">Upgrade Kubeflow</h2>
<p>Refer to [Upgrading Kubeflow cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/upgrade#upgrading-kubeflow-cluster).</p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>Run a full ML workflow on Kubeflow, using the
<a href="https://github.com/kubeflow/examples/blob/master/mnist/mnist_gcp.ipynb">end-to-end MNIST tutorial</a> or the
<a href="https://github.com/kubeflow/examples/tree/master/github_issue_summarization/pipelines">GitHub issue summarization Pipelines
example</a>.</li>
<li>Learn how to [delete your Kubeflow deployment using the CLI](/{{ .Site.Params.version_url_prefix }}docs/deploy/delete-cli/).</li>
<li>To add users to Kubeflow, go to [a dedicated section in Customizing Kubeflow on GKE](/{{ .Site.Params.version_url_prefix }}docs/customizing-gke/#add-users-to-kubeflow).</li>
<li>To taylor your Kubeflow deployment on GKE, go to [Customizing Kubeflow on GKE](/{{ .Site.Params.version_url_prefix }}docs/customizing-gke/).</li>
<li>For troubleshooting Kubeflow deployments on GKE, go to the [Troubleshooting deployments](/{{ .Site.Params.version_url_prefix }}docs/troubleshooting-gke/) guide.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1a4842b2ce53fddfdc881ee75b5de98b">1.6 - Upgrade Kubeflow</h1>
    <div class="lead">Upgrading your Kubeflow installation on Google Cloud</div>
	<h2 id="before-you-start">Before you start</h2>
<p>To better understand upgrade process, you should read the following sections first:</p>
<ul>
<li>[Understanding the deployment process for management cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/management-setup#understanding-the-deployment-process)</li>
<li>[Understanding the deployment process for Kubeflow cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli#understanding-the-deployment-process)</li>
</ul>
<p>This guide assumes the following settings:</p>
<ul>
<li>The <code>${MGMT_DIR}</code> and <code>${MGMT_NAME}</code> environment variables
are the same as in [Management cluster setup](/{{ .Site.Params.version_url_prefix }}docs/deploy/management-setup#configure-environment-variables).</li>
<li>The <code>${KF_NAME}</code>, <code>${CLIENT_ID}</code> and <code>${CLIENT_SECRET}</code> environment variables
are the same as in [Deploy using kubectl and kpt](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli#environment-variables).</li>
<li>The <code>${KF_DIR}</code> environment variable contains the path to
your Kubeflow application directory, which holds your Kubeflow configuration
files. For example, <code>/opt/kubeflow-distribution/kubeflow/</code>.</li>
</ul>
<h2 id="general-upgrade-instructions">General upgrade instructions</h2>
<p>Starting from Kubeflow v1.5, we have integrated with <a href="https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview">Config Controller</a>. You don&rsquo;t need to manually upgrade Management cluster any more, since it managed by <a href="https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-setup#upgrade">Upgrade Config Controller</a>.</p>
<p>Starting from Kubeflow v1.3, we have reworked on the structure of <code>GoogleCloudPlatform/kubeflow-distribution</code> repository. All resources are located in <code>kubeflow-distribution/management</code> directory. Upgrade to Management cluster v1.3 is not supported.</p>
<p>Before Kubeflow v1.3, both management cluster and Kubeflow cluster follow the same <code>instance</code> and <code>upstream</code> folder convention. To upgrade, you&rsquo;ll typically need to update packages in <code>upstream</code> to the new version and repeat the <code>make apply-&lt;subcommand&gt;</code> commands in their respective deployment process.</p>
<p>However, specific upgrades might need manual actions below.</p>
<h2 id="upgrading-management-cluster">Upgrading management cluster</h2>
<h3 id="upgrading-management-cluster-before-15">Upgrading management cluster before 1.5</h3>
<p>It is strongly recommended to use source control to keep a copy of your working repository for recording changes at each step.</p>
<p>Due to the refactoring of <code>kubeflow/manifests</code> repository, the way we depend on <code>GoogleCloudPlatform/kubeflow-distribution</code> has changed drastically. This section suits for upgrading from Kubeflow 1.3 to higher.</p>
<ol>
<li>
<p>The instructions below assume that your current working directory is</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMT_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Use your management cluster&rsquo;s kubectl context:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Look at all your contexts</span>
</span></span><span class="line"><span class="cl">kubectl config get-contexts
</span></span><span class="line"><span class="cl"><span class="c1"># Select your management cluster&#39;s context</span>
</span></span><span class="line"><span class="cl">kubectl config use-context <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Verify the context connects to the cluster properly</span>
</span></span><span class="line"><span class="cl">kubectl get namespace
</span></span></code></pre></div><p>If you are using a different environment, you can always
reconfigure the context by:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make create-context
</span></span></code></pre></div></li>
<li>
<p>Check your existing config connector version:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># For Kubeflow v1.3, it should be 1.46.0</span>
</span></span><span class="line"><span class="cl">$ kubectl get namespace cnrm-system -ojsonpath<span class="o">=</span><span class="s1">&#39;{.metadata.annotations.cnrm\.cloud\.google\.com\/version}&#39;</span>
</span></span><span class="line"><span class="cl">1.46.0
</span></span></code></pre></div></li>
<li>
<p>Merge the content from new Kubeflow version of <code>GoogleCloudPlatform/kubeflow-distribution</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">WORKING_BRANCH</span><span class="o">=</span>&lt;your-github-working-branch&gt;
</span></span><span class="line"><span class="cl"><span class="nv">VERSION_TAG</span><span class="o">=</span>&lt;targeted-kubeflow-version-tag-on-github&gt;
</span></span><span class="line"><span class="cl">git checkout -b <span class="s2">&#34;</span><span class="si">${</span><span class="nv">WORKING_BRANCH</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">git remote add upstream https://github.com/GoogleCloudPlatform/kubeflow-distribution.git <span class="c1"># This is one time only.</span>
</span></span><span class="line"><span class="cl">git fetch upstream 
</span></span><span class="line"><span class="cl">git merge <span class="s2">&#34;</span><span class="si">${</span><span class="nv">VERSION_TAG</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Make sure your build directory (<code>./build</code> by default) is checked in to source control (git).</p>
</li>
<li>
<p>Run the following command to hydrate Config Connector resources:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make hydrate-kcc
</span></span></code></pre></div></li>
<li>
<p>Compare the difference on your source control tracking after making hydration change. If they are addition or modification only, proceed to next step. If it includes deletion, you need to use <code>kubectl delete</code> to manually clean up the deleted resource for cleanup.</p>
</li>
<li>
<p>After confirmation, run the following command to apply new changes:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply-kcc
</span></span></code></pre></div></li>
<li>
<p>Check version has been upgraded after applying new Config Connector resource:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl get namespace cnrm-system -ojsonpath<span class="o">=</span><span class="s1">&#39;{.metadata.annotations.cnrm\.cloud\.google\.com\/version}&#39;</span>
</span></span></code></pre></div></li>
</ol>
<h3 id="upgrade-management-cluster-from-v11-to-v12">Upgrade management cluster from v1.1 to v1.2</h3>
<ol>
<li>
<p>The instructions below assume that your current working directory is</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMT_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Use your management cluster&rsquo;s kubectl context:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Look at all your contexts</span>
</span></span><span class="line"><span class="cl">kubectl config get-contexts
</span></span><span class="line"><span class="cl"><span class="c1"># Select your management cluster&#39;s context</span>
</span></span><span class="line"><span class="cl">kubectl config use-context <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Verify the context connects to the cluster properly</span>
</span></span><span class="line"><span class="cl">kubectl get namespace
</span></span></code></pre></div><p>If you are using a different environment, you can always
reconfigure the context by:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make create-context
</span></span></code></pre></div></li>
<li>
<p>Check your existing config connector version:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># For Kubeflow v1.1, it should be 1.15.1</span>
</span></span><span class="line"><span class="cl">$ kubectl get namespace cnrm-system -ojsonpath<span class="o">=</span><span class="s1">&#39;{.metadata.annotations.cnrm\.cloud\.google\.com\/version}&#39;</span>
</span></span><span class="line"><span class="cl">1.15.1
</span></span></code></pre></div></li>
<li>
<p>Uninstall the old config connector in the management cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl delete sts,deploy,po,svc,roles,clusterroles,clusterrolebindings --all-namespaces -l cnrm.cloud.google.com/system<span class="o">=</span><span class="nb">true</span> --wait<span class="o">=</span><span class="nb">true</span>
</span></span><span class="line"><span class="cl">kubectl delete validatingwebhookconfiguration abandon-on-uninstall.cnrm.cloud.google.com --ignore-not-found --wait<span class="o">=</span><span class="nb">true</span>
</span></span><span class="line"><span class="cl">kubectl delete validatingwebhookconfiguration validating-webhook.cnrm.cloud.google.com --ignore-not-found --wait<span class="o">=</span><span class="nb">true</span>
</span></span><span class="line"><span class="cl">kubectl delete mutatingwebhookconfiguration mutating-webhook.cnrm.cloud.google.com --ignore-not-found --wait<span class="o">=</span><span class="nb">true</span>
</span></span></code></pre></div><p>These commands uninstall the config connector without removing your resources.</p>
</li>
<li>
<p>Replace your <code>./Makefile</code> with the version in Kubeflow <code>v1.2.0</code>: <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/v1.2.0/management/Makefile">https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/v1.2.0/management/Makefile</a>.</p>
<p>If you made any customizations in <code>./Makefile</code>, you should merge your changes with the upstream version. We&rsquo;ve refactored the Makefile to move substantial commands into the upstream package, so hopefully future upgrades won&rsquo;t require a manual merge of the Makefile.</p>
</li>
<li>
<p>Update <code>./upstream/management</code> package:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make update
</span></span></code></pre></div></li>
<li>
<p>Use kpt to set user values:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kpt cfg <span class="nb">set</span> -R . name <span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">kpt cfg <span class="nb">set</span> -R . gcloud.core.project <span class="si">${</span><span class="nv">MGMT_PROJECT</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">kpt cfg <span class="nb">set</span> -R . location <span class="si">${</span><span class="nv">LOCATION</span><span class="si">}</span>
</span></span></code></pre></div><p>Note, you can find out which setters exist in a package and what there current values are by:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kpt cfg list-setters .
</span></span></code></pre></div></li>
<li>
<p>Apply upgraded config connector:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply-kcc
</span></span></code></pre></div><p>Note, you can optionally also run <code>make apply-cluster</code>, but it should be the same as your existing management cluster.</p>
</li>
<li>
<p>Check that your config connector upgrade is successful:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># For Kubeflow v1.2, it should be 1.29.0</span>
</span></span><span class="line"><span class="cl">$ kubectl get namespace cnrm-system -ojsonpath<span class="o">=</span><span class="s1">&#39;{.metadata.annotations.cnrm\.cloud\.google\.com\/version}&#39;</span>
</span></span><span class="line"><span class="cl">1.29.0
</span></span></code></pre></div></li>
</ol>
<h2 id="upgrading-kubeflow-cluster">Upgrading Kubeflow cluster</h2>


<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">DISCLAIMERS</h4>

    <div>The upgrade process depends on each Kubeflow application to handle the upgrade properly. There's no guarantee on data completeness unless the application provides such a guarantee.</div>
<div>You are recommended to back up your data before an upgrade.</div>
<div>Upgrading Kubeflow cluster can be a disruptive process, please schedule some downtime and communicate with your users.</div>


</div>

<p>To upgrade from specific versions of Kubeflow, you may need to take certain manual actions  refer to specific sections in the guidelines below.</p>
<h3 id="general-instructions-for-upgrading-kubeflow-cluster">General instructions for upgrading Kubeflow cluster</h3>
<ol>
<li>
<p>The instructions below assume that:</p>
<ul>
<li>
<p>Your current working directory is:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span>
</span></span></code></pre></div></li>
<li>
<p>Your kubectl uses a context that connects to your Kubeflow cluster</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># List your existing contexts</span>
</span></span><span class="line"><span class="cl">kubectl config get-contexts
</span></span><span class="line"><span class="cl"><span class="c1"># Use the context that connects to your Kubeflow cluster</span>
</span></span><span class="line"><span class="cl">kubectl config use-context <span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>Merge the new version of <code>GoogleCloudPlatform/kubeflow-distribution</code> (example: v1.3.1), you don&rsquo;t need to do it again if you have already done so during management cluster upgrade.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">WORKING_BRANCH</span><span class="o">=</span>&lt;your-github-working-branch&gt;
</span></span><span class="line"><span class="cl"><span class="nv">VERSION_TAG</span><span class="o">=</span>&lt;targeted-kubeflow-version-tag-on-github&gt;
</span></span><span class="line"><span class="cl">git checkout -b <span class="s2">&#34;</span><span class="si">${</span><span class="nv">WORKING_BRANCH</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">git remote add upstream https://github.com/GoogleCloudPlatform/kubeflow-distribution.git <span class="c1"># This is one time only.</span>
</span></span><span class="line"><span class="cl">git fetch upstream 
</span></span><span class="line"><span class="cl">git merge <span class="s2">&#34;</span><span class="si">${</span><span class="nv">VERSION_TAG</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Change the <code>KUBEFLOW_MANIFESTS_VERSION</code> in <code>./pull-upstream.sh</code> with the targeted kubeflow version same as <code>$VERSION_TAG</code>. Run the following commands to pull new changes from upstream <code>kubeflow/manifests</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">bash ./pull-upstream.sh
</span></span></code></pre></div></li>
<li>
<p>(Optional) If you only want to upgrade some of Kubeflow components, you can comment non-upgrade components in <code>kubeflow/config.yaml</code> file. Commands below will only apply the remaining components.</p>
</li>
<li>
<p>Make sure you have checked in <code>build</code> folders for each component. The following command will change them so you can compare for difference.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make hydrate
</span></span></code></pre></div></li>
<li>
<p>Once you confirm the changes are ready to apply, run the following command to upgrade Kubeflow cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply
</span></span></code></pre></div></li>
</ol>


<div class="alert alert-primary" role="alert">
<h4 class="alert-heading">Note</h4>

    Kubeflow on Google Cloud doesn&rsquo;t guarantee the upgrade for each Kubeflow component always works with the general upgrade guide here. Please refer to corresponding repository in <a href="https://github.com/kubeflow">Kubeflow org</a> for upgrade support.

</div>

<h3 id="upgrade-kubeflow-cluster-to-v16">Upgrade Kubeflow cluster to v1.6</h3>
<p>Starting from Kubeflow v1.6.0:</p>
<ul>
<li>Component with deprecated API versions were upgraded to support GKE v1.22. If you would like to upgrade your GKE cluster, follow <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster">GCP instructions</a>.</li>
<li>ASM was upgraded to v1.14. Follow the instructions on how to <a href="#upgrade-asm-anthos-service-mesh">upgrade ASM (Anthos Service Mesh)</a>. If you want to use ASM version prior to 1.11, refer to <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/asm/deprecated/README.md">the legacy instructions</a>.</li>
<li>Knative was upgraded to v1.2. Follow <a href="https://knative.dev/docs/install/upgrade/upgrade-installation/">Knative instructions</a> to check current version and see if the update includes any breaking changes.</li>
<li>Cert-manager was upgraded to v1.5. To check your current version and see if the update includes any breaking changes, follow the <a href="https://cert-manager.io/docs/installation/upgrading/">cert-manager instructions</a>.</li>
<li>Deprecated kfserving component was removed. To upgrade to KServe, follow the <a href="https://github.com/kserve/kserve/tree/master/hack/kserve_migration">KServe Migration guide</a>.</li>
</ul>
<h3 id="upgrade-kubeflow-cluster-to-v15">Upgrade Kubeflow cluster to v1.5</h3>
<p>In Kubeflow v1.5.1 we use ASM v1.13. See <a href="#upgrade-asm-anthos-service-mesh">how to upgrade ASM</a>. To use ASM versions prior to 1.11, follow <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/asm/deprecated/README.md">the legacy instructions</a>.</p>
<p>Starting from Kubeflow v1.5, Kubeflow manifests have included KServe as an independent component from kfserving, Google Cloud distribution has switched over from kfserving to KServe for default installed components. If you want to upgrade Kubeflow while keeping kfsering, you can comment KServe and uncomment kfserving in <code>kubeflow-distribution/kubeflow/config.yaml</code> file. If you want to upgrade to KServe, follow the <a href="https://github.com/kserve/kserve/tree/master/hack/kserve_migration">KServe Migration guide</a>.</p>
<h3 id="upgrade-kubeflow-cluster-to-v13">Upgrade Kubeflow cluster to v1.3</h3>
<p>Due to the refactoring of <code>kubeflow/manifests</code> repository, the way we depend on <code>GoogleCloudPlatform/kubeflow-distribution</code> has changed drastically. Upgrade to Kubeflow cluster v1.3 is not supported. And individual component upgrade has been deferred to its corresponding repository for support.</p>
<h3 id="upgrade-kubeflow-cluster-from-v11-to-v12">Upgrade Kubeflow cluster from v1.1 to v1.2</h3>
<ol>
<li>
<p>The instructions below assume</p>
<ul>
<li>
<p>Your current working directory is:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span>
</span></span></code></pre></div></li>
<li>
<p>Your kubectl uses a context that connects to your Kubeflow cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># List your existing contexts</span>
</span></span><span class="line"><span class="cl">kubectl config get-contexts
</span></span><span class="line"><span class="cl"><span class="c1"># Use the context that connects to your Kubeflow cluster</span>
</span></span><span class="line"><span class="cl">kubectl config use-context <span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>(Recommended) Replace your <code>./Makefile</code> with the version in Kubeflow <code>v1.2.0</code>: <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/v1.2.0/kubeflow/Makefile">https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/v1.2.0/kubeflow/Makefile</a>.</p>
<p>If you made any customizations in <code>./Makefile</code>, you should merge your changes with the upstream version.</p>
<p>This step is recommended, because we introduced usability improvements and fixed compatibility for newer Kustomize versions (while still being compatible with Kustomize v3.2.1) to the Makefile. However, the deployment process is backward-compatible, so this is recommended, but not required.</p>
</li>
<li>
<p>Update <code>./upstream/manifests</code> package:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make update
</span></span></code></pre></div></li>
<li>
<p>Before applying new resources, you need to delete some immutable resources that were updated in this release:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl delete statefulset kfserving-controller-manager -n kubeflow --wait
</span></span><span class="line"><span class="cl">kubectl delete crds experiments.kubeflow suggestions.kubeflow trials.kubeflow
</span></span></code></pre></div><p><strong>WARNING</strong>: This step <strong>deletes</strong> all Katib running resources.</p>
<p>Refer to <a href="https://github.com/kubeflow/kubeflow/issues/5371#issuecomment-731359384">a github comment in the v1.2 release issue</a> for more details.</p>
</li>
<li>
<p>Redeploy:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply
</span></span></code></pre></div><p>To evaluate the changes before deploying them you can:</p>
<ol>
<li>Run <code>make hydrate</code>.</li>
<li>Compare the contents
of <code>.build</code> with a historic version with tools like <code>git diff</code>.</li>
</ol>
</li>
</ol>
<h2 id="upgrade-asm-anthos-service-mesh">Upgrade ASM (Anthos Service Mesh)</h2>
<p>If you want to upgrade ASM instead of the Kubeflow components, refer to [kubeflow/common/asm/README.md](<a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/asm/README.md">https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/asm/README.md</a> for the latest instructions on upgrading ASM. Detailed explanation is listed below. Note: if you are going to upgrade major or minor version of ASM, it is recommended to read <a href="https://cloud.google.com/service-mesh/docs/upgrade-path-old-versions-gke">the official ASM upgrade documentation</a> before proceeding with the steps below.</p>
<h3 id="install-a-new-asm-workload">Install a new ASM workload</h3>
<p>In order to use the new ASM version, we need to download the corresponding ASM configuration package and <code>asmcli</code> script. Get a list of available ASM packages and the corresponding <code>asmcli</code> scripts by running the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl https://storage.googleapis.com/csm-artifacts/asm/ASMCLI_VERSIONS
</span></span></code></pre></div><p>It should return a list of ASM versions that can be installed with asmcli script. To install older versions, refer to <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/asm/deprecated/README.md">the legacy instructions</a>. The returned list will have a format of <code>${ASM_PACKAGE_VERSION}:${ASMCLI_SCRIPT_VERSION}</code>. For example, in the following output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">1.13.2-asm.5+config2:asmcli_1.13.2-asm.5-config2
</span></span><span class="line"><span class="cl">1.13.2-asm.5+config1:asmcli_1.13.2-asm.5-config1
</span></span><span class="line"><span class="cl">1.13.2-asm.2+config2:asmcli_1.13.2-asm.2-config2
</span></span><span class="line"><span class="cl">1.13.2-asm.2+config1:asmcli_1.13.2-asm.2-config1
</span></span><span class="line"><span class="cl">1.13.1-asm.1+config1:asmcli_1.13.1-asm.1-config1
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>record 1.13.2-asm.5+config2:asmcli_1.13.2-asm.5-config2 corresponds to:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">ASM_PACKAGE_VERSION</span><span class="o">=</span>1.13.2-asm.5+config2
</span></span><span class="line"><span class="cl"><span class="nv">ASMCLI_SCRIPT_VERSION</span><span class="o">=</span>asmcli_1.13.2-asm.5-config2
</span></span></code></pre></div><p>You need to set these two values in <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/asm/Makefile">kubeflow/asm/Makefile</a>. Then, run the following command in <code>kubeflow/asm</code> directory to install the new ASM. Note, the old ASM will not be uninstalled.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply
</span></span></code></pre></div><p>Once installed successfully, you can see istiod <code>Deployment</code> in your cluster with name in pattern <code>istiod-asm-VERSION-REVISION</code>. For example, <code>istiod-asm-1132-5</code> would correspond to ASM version 1.13.2-asm.5.</p>
<h3 id="upgrade-other-kubeflow-components-to-use-new-asm">Upgrade other Kubeflow components to use new ASM</h3>
<p>There are multiple Kubeflow components with ASM namespace label, including user created namespaces. To upgrade them at once, change the following line in <code>kubeflow/env.sh</code> with the new ASM version <code>asm-VERSION-REVISION</code>, like <code>asm-1132-5</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">ASM_LABEL</span><span class="o">=</span>asm-1132-5
</span></span></code></pre></div><p>Then run the following commands in <code>kubeflow/</code> directory to configure the environmental variables:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">source</span> env.sh
</span></span></code></pre></div><p>Run the following command to configure kpt setter:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">bash kpt-set.sh
</span></span></code></pre></div><p>Examine the change using source control after running the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make hydrate
</span></span></code></pre></div><p>Refer to <a href="https://cloud.google.com/service-mesh/docs/unified-install/upgrade#deploying_and_redeploying_workloads">Deploying and redeploying workloads</a> for the complete steps to adopt the new ASM version. As part of the instructions, you can run the following command to update namespaces&rsquo; labels across the cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply
</span></span></code></pre></div><h3 id="optional-uninstall-the-old-asm-workload">(Optional) Uninstall the old ASM workload</h3>
<p>Once you validated that new ASM installation and sidecar-injection for Kubeflow components are working as expected. You can <strong>Complete the transition</strong> to the new ASM or <strong>Rollback</strong> to the old ASM as instructed in <a href="https://cloud.google.com/service-mesh/docs/unified-install/upgrade#deploying_and_redeploying_workloads">Deploy and Redeploy workloads</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b01cda5c30a4cd24f94100af83fdaaa2">1.7 - Monitoring Cloud IAP Setup</h1>
    <div class="lead">Instructions for monitoring and troubleshooting Cloud IAP</div>
	<p><a href="https://cloud.google.com/iap/docs/">Cloud Identity-Aware Proxy (Cloud IAP)</a> is
the recommended solution for accessing your Kubeflow
deployment from outside the cluster, when running Kubeflow on Google Cloud.</p>
<p>This document is a step-by-step guide to ensuring that your IAP-secured endpoint
is available, and to debugging problems that may cause the endpoint to be
unavailable.</p>
<h2 id="introduction">Introduction</h2>
<p>When deploying Kubeflow using the [command-line interface](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli/),
you choose the authentication method you want to use. One of the options is
Cloud IAP. This document assumes that you have already deployed Kubeflow.</p>
<p>Kubeflow uses the <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs">Google-managed certificate</a>
to provide an SSL certificate for the Kubeflow Ingress.</p>
<p>Cloud IAP gives you the following benefits:</p>
<ul>
<li>Users can log in in using their Google Cloud accounts.</li>
<li>You benefit from Google&rsquo;s security expertise to protect your sensitive
workloads.</li>
</ul>
<h2 id="monitoring-your-cloud-iap-setup">Monitoring your Cloud IAP setup</h2>
<p>Follow these instructions to monitor your Cloud IAP setup and troubleshoot any
problems:</p>
<ol>
<li>
<p>Examine the
<a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a>
and Google Cloud Build (GCB) load balancer to make sure it is available:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe ingress
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Name:             envoy-ingress
</span></span><span class="line"><span class="cl">Namespace:        kubeflow
</span></span><span class="line"><span class="cl">Address:          35.244.132.160
</span></span><span class="line"><span class="cl">Default backend:  default-http-backend:80 (10.20.0.10:8080)
</span></span><span class="line"><span class="cl">Annotations:
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Events:
</span></span><span class="line"><span class="cl">   Type     Reason     Age                 From                     Message
</span></span><span class="line"><span class="cl">   ----     ------     ----                ----                     -------
</span></span><span class="line"><span class="cl">   Normal   ADD        12m                 loadbalancer-controller  kubeflow/envoy-ingress
</span></span><span class="line"><span class="cl">   Warning  Translate  12m (x10 over 12m)  loadbalancer-controller  error while evaluating the ingress spec: could not find service &#34;kubeflow/envoy&#34;
</span></span><span class="line"><span class="cl">   Warning  Translate  12m (x2 over 12m)   loadbalancer-controller  error while evaluating the ingress spec: error getting BackendConfig for port &#34;8080&#34; on service &#34;kubeflow/envoy&#34;, err: no BackendConfig for service port exists.
</span></span><span class="line"><span class="cl">   Warning  Sync       12m                 loadbalancer-controller  Error during sync: Error running backend syncing routine: received errors when updating backend service: googleapi: Error 400: The resource &#39;projects/code-search-demo/global/backendServices/k8s-be-32230--bee2fc38fcd6383f&#39; is not ready, resourceNotReady
</span></span><span class="line"><span class="cl"> googleapi: Error 400: The resource &#39;projects/code-search-demo/global/backendServices/k8s-be-32230--bee2fc38fcd6383f&#39; is not ready, resourceNotReady
</span></span><span class="line"><span class="cl">   Normal  CREATE  11m  loadbalancer-controller  ip: 35.244.132.160
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>There should be an annotation indicating that we are using managed certificate:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">annotation:
</span></span><span class="line"><span class="cl">  networking.gke.io/managed-certificates: gke-certificate
</span></span></code></pre></div><p>Any problems with creating the load balancer are reported as Kubernetes
events in the results of the above <code>describe</code> command.</p>
<ul>
<li>
<p>If the address isn&rsquo;t set then there was a problem creating the load
balancer.</p>
</li>
<li>
<p>The <code>CREATE</code> event indicates the load balancer was successfully
created on the specified IP address.</p>
</li>
<li>
<p>The most common error is running out of Google Cloud resource quota. To fix this problem,
you must either increase the quota for the relevant resource on your Google Cloud
project or delete some existing resources.</p>
</li>
</ul>
</li>
<li>
<p>Verify that a managed certificate resource is generated:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl describe -n istio-system managedcertificate gke-certificate
</span></span></code></pre></div><p>The status field should have information about the current status of the Certificate.
Eventually, certificate status should be <code>Active</code>.</p>
</li>
<li>
<p>Wait for the load balancer to report the back ends as healthy:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl describe -n istio-system ingress envoy-ingress
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Annotations:
</span></span><span class="line"><span class="cl"> kubernetes.io/ingress.global-static-ip-name:  kubeflow-ip
</span></span><span class="line"><span class="cl"> kubernetes.io/tls-acme:                       true
</span></span><span class="line"><span class="cl"> certmanager.k8s.io/issuer:                    letsencrypt-prod
</span></span><span class="line"><span class="cl"> ingress.kubernetes.io/backends:               {&#34;k8s-be-31380--5e1566252944dfdb&#34;:&#34;HEALTHY&#34;,&#34;k8s-be-32133--5e1566252944dfdb&#34;:&#34;HEALTHY&#34;}
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>Both backends should be reported as healthy.
It can take several minutes for the load balancer to consider the back ends
healthy.</p>
<p>The service with port <code>31380</code> is the one that handles Kubeflow
traffic. (31380 is the default port of the service <code>istio-ingressgateway</code>.)</p>
<p>If the backend is unhealthy, check the pods in <code>istio-system</code>:</p>
<ul>
<li><code>kubectl get pods -n istio-system</code></li>
<li>The <code>istio-ingressgateway-XX</code> pods should be running</li>
<li>Check the logs of pod <code>backend-updater-0</code>, <code>iap-enabler-XX</code> to see if there is any error</li>
<li>Follow the steps [here](/{{ .Site.Params.version_url_prefix }}docs/troubleshooting/#502-server-error) to check the load balancer and backend service on Google Cloud.</li>
</ul>
</li>
<li>
<p>Try accessing Cloud IAP at the fully qualified domain name in your web
browser:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://&lt;your-fully-qualified-domain-name&gt;     
</span></span></code></pre></div><p>If you get SSL errors when you log in, this typically means that your SSL
certificate is still propagating. Wait a few minutes and try again. SSL
propagation can take up to 10 minutes.</p>
<p>If you do not see a login prompt and you get a 404 error, the configuration
of Cloud IAP is not yet complete. Keep retrying for up to 10 minutes.</p>
</li>
<li>
<p>If you get an error <code>Error: redirect_uri_mismatch</code> after logging in, this means the list of OAuth authorized redirect URIs does not include your domain.</p>
<p>The full error message looks like the following example and includes the 
relevant links:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">The redirect URI in the request, https://&lt;my_kubeflow&gt;.endpoints.&lt;my_project&gt;.cloud.goog/_gcp_gatekeeper/authenticate, does not match the ones authorized for the OAuth client. 	
</span></span><span class="line"><span class="cl">To update the authorized redirect URIs, visit: https://console.developers.google.com/apis/credentials/oauthclient/22222222222-7meeee7a9a76jvg54j0g2lv8lrsb4l8g.apps.googleusercontent.com?project=22222222222	
</span></span></code></pre></div><p>Follow the link in the error message to find the OAuth credential being used
and add the redirect URI listed in the error message to the list of 
authorized URIs. For more information, read the guide to 
[setting up OAuth for Cloud IAP](/{{ .Site.Params.version_url_prefix }}docs/deploy/oauth-setup/).</p>
</li>
</ol>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>The [GKE troubleshooting guide](/{{ .Site.Params.version_url_prefix }}docs/troubleshooting/) for Kubeflow.</li>
<li>Guide to [sharing cluster access](/{{ .Site.Params.version_url_prefix }}docs/components/multi-tenancy/getting-started).</li>
<li>Google Cloud guide to <a href="https://cloud.google.com/iap/docs/">Cloud IAP</a>.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f10ecd64c7b37a30c6eb2c6c15ae2ca2">1.8 - Deleting Kubeflow</h1>
    <div class="lead">Deleting Kubeflow from Google Cloud using the command line interface (CLI)</div>
	<p>This page explains how to delete your Kubeflow cluster or management cluster on
Google Cloud.</p>
<h2 id="before-you-start">Before you start</h2>
<p>This guide assumes the following settings:</p>
<ul>
<li>
<p>For Management cluster: The <code>${MGMT_PROJECT}</code>, <code>${MGMT_DIR}</code> and <code>${MGMT_NAME}</code> environment variables
are the same as in [Deploy Management cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/management-setup#configure-environment-variables).</p>
</li>
<li>
<p>For Kubeflow cluster: The <code>${KF_PROJECT}</code>, <code>${KF_NAME}</code> and <code>${MGMTCTXT}</code> environment variables
are the same as in [Deploy Kubeflow cluster](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli#environment-variables).</p>
</li>
<li>
<p>The <code>${KF_DIR}</code> environment variable contains the path to
your Kubeflow application directory, which holds your Kubeflow configuration
files. For example, <code>/opt/kubeflow-distribution/kubeflow/</code>.</p>
</li>
</ul>
<h2 id="deleting-your-kubeflow-cluster">Deleting your Kubeflow cluster</h2>
<ol>
<li>
<p>To delete the applications running in the Kubeflow namespace, remove that namespace:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl delete namespace kubeflow
</span></span></code></pre></div></li>
<li>
<p>To delete the cluster and all Google Cloud resources, run the following commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">make delete
</span></span></code></pre></div><p><strong>Warning</strong>: this will delete the persistent disks storing metadata. If you want to preserve the disks, don&rsquo;t run this command;
instead selectively delete only those resources you want to delete.</p>
</li>
</ol>
<h2 id="clean-up-your-management-cluster">Clean up your management cluster</h2>
<p>The following instructions introduce how to clean up all resources created when
installing management cluster in management project, and when using management cluster to manage Google Cloud resources in managed Kubeflow projects.</p>
<h3 id="delete-or-keep-managed-google-cloud-resources">Delete or keep managed Google Cloud resources</h3>
<p>There are Google Cloud resources managed by Config Connector in the
management cluster after you deploy Kubeflow clusters with this management
cluster.</p>
<p>To delete all the managed Google Cloud resources, delete the managed project namespace:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl config use-context <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMTCTXT</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">kubectl delete namespace --wait <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div><p>To keep all the managed Google Cloud resources, you can <a href="#delete-management-cluster">delete the management
cluster</a> directly.</p>
<p>If you need fine-grained control, refer to
<a href="https://cloud.google.com/config-connector/docs/how-to/managing-deleting-resources#keeping_resources_after_deletion">Config Connector: Keeping resources after deletion</a>
for more details.</p>
<p>After deleting Config Connector resources for a managed project, you can revoke IAM permission
that let the management cluster manage the project:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud projects remove-iam-policy-binding <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span><span class="s2">&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    <span class="s2">&#34;--member=serviceAccount:</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span><span class="s2">-cnrm-system@</span><span class="si">${</span><span class="nv">MGMT_PROJECT</span><span class="si">}</span><span class="s2">.iam.gserviceaccount.com&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --role<span class="o">=</span>roles/owner
</span></span></code></pre></div><h3 id="delete-management-cluster">Delete management cluster</h3>
<p>To delete the Google service account and the management cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMT_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">make delete-cluster
</span></span></code></pre></div><p>Starting from Kubeflow v1.5, Google Cloud distribution has switched to Config Controller for Google-managed Management cluster. You can learn more detail by reading <a href="https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-setup#delete_your">Delete your Config Controller</a>.</p>
<p>Note, after deleting the management cluster, all the managed Google Cloud
resources will be kept. You will be responsible for managing them by yourself.
If you want to delete the managed Google Cloud resources, make sure to delete resources in the <code>${KF_PROJECT}</code> namespace in the management cluster first.
You can learn more about the <code>${KF_PROJECT}</code> namespace in <code>kubeflow-distribution/kubeflow/kcc</code> folder.</p>
<p>You can create a management cluster to manage them again if you apply the same
Config Connector resources. Refer to <a href="https://cloud.google.com/config-connector/docs/how-to/managing-deleting-resources#acquiring_an_existing_resource">Managing and deleting resources - Acquiring an existing resource</a>.</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-31cedb300807aeaac671a176f8cd692b">2 - Pipelines on Google Cloud</h1>
    <div class="lead">Instructions for customizing and using Kubeflow Pipelines on Google Cloud</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-0b0498d20bf77acec45ad87018b21f47">2.1 - Connecting to Kubeflow Pipelines on Google Cloud using the SDK</h1>
    <div class="lead">How to connect to different Kubeflow Pipelines installations on Google Cloud using the Kubeflow Pipelines SDK</div>
	<p>This guide describes how to connect to your Kubeflow Pipelines cluster on Google
Cloud using [the Kubeflow Pipelines SDK](/{{ .Site.Params.version_url_prefix }}docs/components/pipelines/sdk/sdk-overview/).</p>
<h2 id="before-you-begin">Before you begin</h2>
<ul>
<li>You need a Kubeflow Pipelines deployment on Google Cloud using one of the [installation options](/{{ .Site.Params.version_url_prefix }}docs/components/pipelines/installation/overview/).</li>
<li>[Install the Kubeflow Pipelines SDK](/{{ .Site.Params.version_url_prefix }}docs/components/pipelines/sdk/install-sdk/).</li>
</ul>
<h2 id="how-sdk-connects-to-kubeflow-pipelines-api">How SDK connects to Kubeflow Pipelines API</h2>
<p>Kubeflow Pipelines includes an API service named <code>ml-pipeline-ui</code>. The
<code>ml-pipeline-ui</code> API service is deployed in the same Kubernetes namespace you
deployed Kubeflow Pipelines in.</p>
<p>The Kubeflow Pipelines SDK can send REST API requests to this API service, but
the SDK needs to know the hostname to connect to the API service.</p>
<p>If the hostname can be accessed without authentication, it&rsquo;s very simple to
connect to it. For example, you can use <code>kubectl port-forward</code> to access it via
localhost:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># The Kubeflow Pipelines API service and the UI is available at</span>
</span></span><span class="line"><span class="cl"><span class="c1"># http://localhost:3000 without authentication check.</span>
</span></span><span class="line"><span class="cl">$ kubectl port-forward svc/ml-pipeline-ui 3000:80 --namespace kubeflow
</span></span><span class="line"><span class="cl"><span class="c1"># Change the namespace if you deployed Kubeflow Pipelines in a different</span>
</span></span><span class="line"><span class="cl"><span class="c1"># namespace.</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp</span>
</span></span><span class="line"><span class="cl"><span class="n">client</span> <span class="o">=</span> <span class="n">kfp</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s1">&#39;http://localhost:3000&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>When deploying Kubeflow Pipelines on Google Cloud, a public endpoint for this
API service is auto-configured for you, but this public endpoint has security
checks to protect your cluster from unauthorized access.</p>
<p>The following sections introduce how to authenticate your SDK requests to connect
to Kubeflow Pipelines via the public endpoint.</p>
<h2 id="connecting-to-kubeflow-pipelines-standalone-or-ai-platform-pipelines">Connecting to Kubeflow Pipelines standalone or AI Platform Pipelines</h2>
<p>Refer to <a href="https://cloud.google.com/ai-platform/pipelines/docs/connecting-with-sdk">Connecting to AI Platform Pipelines using the Kubeflow Pipelines SDK</a> for
both Kubeflow Pipelines standalone and AI Platform Pipelines.</p>
<p>Kubeflow Pipelines standalone deployments also show up in <a href="https://console.cloud.google.com/ai-platform/pipelines/clusters">AI Platform Pipelines</a>. They have the
name &ldquo;pipeline&rdquo; by default, but you can customize the name by overriding
<a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/sample/params.env#L1">the <code>appName</code> parameter in <code>params.env</code></a> when [deploying Kubeflow Pipelines standalone](/{{ .Site.Params.version_url_prefix }}docs/components/pipelines/installation/standalone-deployment/).</p>
<h2 id="connecting-to-kubeflow-pipelines-in-a-full-kubeflow-deployment">Connecting to Kubeflow Pipelines in a full Kubeflow deployment</h2>
<p>A full Kubeflow deployment on Google Cloud uses an <a href="https://cloud.google.com/iap/docs">Identity-Aware Proxy (IAP)</a> to manage access to the public Kubeflow endpoint. The steps
below let you connect to Kubeflow Pipelines in a full Kubeflow deployment with
authentication through IAP.</p>
<ol>
<li>
<p>Find out your IAP OAuth 2.0 client ID.</p>
<p>You or your cluster admin followed [Set up OAuth for Cloud IAP](/{{ .Site.Params.version_url_prefix }}docs/deploy/oauth-setup/)
to deploy your full Kubeflow deployment on Google Cloud. You need the OAuth client
ID created in that step.</p>
<p>You can browse all of your existing OAuth client IDs <a href="https://console.cloud.google.com/apis/credentials">in the Credentials page of Google Cloud Console</a>.</p>
</li>
<li>
<p>Create another SDK OAuth Client ID for authenticating Kubeflow Pipelines SDK users.
Follow <a href="https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app">the steps to set up a client ID to authenticate from a desktop app</a>. Take
a note of the <strong>client ID</strong> and <strong>client secret</strong>. This client ID and secret can
be shared among all SDK users, because a separate login step is still needed below.</p>
</li>
<li>
<p>To connect to Kubeflow Pipelines public endpoint, initiate SDK client like the following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp</span>
</span></span><span class="line"><span class="cl"><span class="n">client</span> <span class="o">=</span> <span class="n">kfp</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s1">&#39;https://&lt;KF_NAME&gt;.endpoints.&lt;PROJECT&gt;.cloud.goog/pipeline&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">client_id</span><span class="o">=</span><span class="s1">&#39;&lt;AAAAAAAAAAAAAAAAAAAAAA&gt;.apps.googleusercontent.com&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">other_client_id</span><span class="o">=</span><span class="s1">&#39;&lt;BBBBBBBBBBBBBBBBBBB&gt;.apps.googleusercontent.com&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">other_client_secret</span><span class="o">=</span><span class="s1">&#39;&lt;CCCCCCCCCCCCCCCCCCCC&gt;&#39;</span><span class="p">)</span>
</span></span></code></pre></div><ul>
<li>Pass your <strong>IAP</strong> OAuth client ID found in <strong>step 1</strong> to <code>client_id</code> argument.</li>
<li>Pass your <strong>SDK</strong> OAuth client ID and secret created in <strong>step 2</strong> to <code>other_client_id</code>
and <code>other_client_secret</code> arguments.</li>
</ul>
</li>
<li>
<p>When you init the SDK client for the first time, you will be asked to log in.
The Kubeflow Pipelines SDK stores obtained credentials in <code>$HOME/.config/kfp/credentials.json</code>. You do not need to log in again unless you manually delete the credentials file.</p>
<pre><code>To use the SDK from cron tasks where you cannot log in manually, you can copy the credentials file in `$HOME/.config/kfp/credentials.json` to another machine.
However, you should keep the credentials safe and never expose it to
third parties.
</code></pre>
</li>
<li>
<p>After login, you can use the client.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">list_pipelines</span><span class="p">())</span>
</span></span></code></pre></div></li>
</ol>
<h2 id="troubleshooting">Troubleshooting</h2>
<ul>
<li>
<p>Error &ldquo;Failed to authorize with API resource references: there is no user identity header&rdquo; when using SDK methods.</p>
<p>Direct access to the API service without authentication works for Kubeflow
Pipelines standalone, AI Platform Pipelines, and Kubeflow 1.0 or earlier.</p>
<p>However, it fails authorization checks for Kubeflow Pipelines with multi-user
isolation in the full Kubeflow deployment starting from Kubeflow 1.1.
Multi-user isolation requires all API access to authenticate as a user. Refer to [Kubeflow Pipelines Multi-user isolation documentation](/{{ .Site.Params.version_url_prefix }}docs/components/pipelines/overview/multi-user/#in-cluster-request-authentication)
for more details.</p>
</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-25e5f676d4864c88d4a268fec2aaca06">2.2 - Authenticating Pipelines to Google Cloud</h1>
    <div class="lead">Authentication and authorization to Google Cloud in Pipelines</div>
	<p>This page describes authentication for Kubeflow Pipelines to Google Cloud.
Available options listed below have different tradeoffs. You should choose the one that fits your use-case.</p>
<ul>
<li>Configuring your cluster to access Google Cloud using <a href="#compute-engine-default-service-account">Compute Engine default service account</a> with the &ldquo;cloud-platform&rdquo; scope is easier to set up than the other options. However, this approach grants excessive permissions. Therefore, it is not suitable if you need workload permission separation.</li>
<li><a href="#workload-identity">Workload Identity</a> takes more efforts to set up, but allows fine-grained permission control. It is recommended for production use-cases.</li>
<li><a href="#google-service-account-keys-stored-as-kubernetes-secrets">Google service account keys stored as Kubernetes secrets</a> is the legacy approach and no longer recommended in GKE. However, it&rsquo;s the only option to use Google Cloud APIs when your cluster is an <a href="https://cloud.google.com/anthos">anthos</a> or on-prem cluster.</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<p>There are various options on how to install Kubeflow Pipelines in the <a href="https://www.kubeflow.org/docs/components/pipelines/installation/overview/">Installation Options for Kubeflow Pipelines</a> guide.
Be aware that authentication support and cluster setup instructions will vary depending on the method you used to install Kubeflow Pipelines.</p>
<ul>
<li>For Kubeflow Pipelines standalone, you can compare and choose from all 3 options.</li>
<li>For full Kubeflow starting from Kubeflow 1.1, <a href="#workload-identity">Workload Identity</a> is the recommended and default option.</li>
<li>For AI Platform Pipelines, <a href="#compute-engine-default-service-account">Compute Engine default service account</a> is the only supported option.</li>
</ul>
<h2 id="compute-engine-default-service-account">Compute Engine default service account</h2>
<p>This is good for trying out Kubeflow Pipelines, because it is easy to set up.</p>
<p>However, it does not support permission separation for workloads in the cluster. <strong>Any workload</strong> in the cluster will be able to call <strong>any Google Cloud APIs</strong> in the chosen scope.</p>


<div class="alert alert-warning" role="alert">


    NOTE: Using pipelines with Compute Engine default service account is not supported in Full Kubeflow deployment.

</div>

<h3 id="cluster-setup-to-use-compute-engine-default-service-account">Cluster setup to use Compute Engine default service account</h3>
<p>By default, your GKE nodes use <a href="https://cloud.google.com/compute/docs/access/service-accounts#default_service_account">Compute Engine default service account</a>. If you allowed <code>cloud-platform</code> scope when creating the cluster,
Kubeflow Pipelines can authenticate to Google Cloud and manage resources in your project without further configuration.</p>
<p>Use one of the following options to create a GKE cluster that uses the Compute Engine default service account:</p>
<ul>
<li>If you followed instructions in <a href="https://cloud.google.com/ai-platform/pipelines/docs/setting-up">Setting up AI Platform Pipelines</a> and checked <code>Allow access to the following Cloud APIs</code>, your cluster is already using Compute Engine default service account.</li>
<li>In Google Cloud Console UI, you can enable it in <code>Create a Kubernetes cluster -&gt; default-pool -&gt; Security -&gt; Access Scopes -&gt; Allow full access to all Cloud APIs</code> like the following:
<img src="/{{ .Site.Params.version_url_prefix }}docs/images/pipelines/v1/gke-allow-full-access.png"></li>
<li>Using <code>gcloud</code> CLI, you can enable it with <code>--scopes cloud-platform</code> like the following:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud container clusters create &lt;cluster-name&gt; <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --scopes cloud-platform
</span></span></code></pre></div><p>Please refer to <a href="https://cloud.google.com/sdk/gcloud/reference/container/clusters/create#--scopes">gcloud container clusters create command documentation</a> for other available options.</p>
<h3 id="authoring-pipelines-to-use-default-service-account">Authoring pipelines to use default service account</h3>
<p>Pipelines don&rsquo;t need any specific changes to authenticate to Google Cloud, it will use the default service account transparently.</p>
<p>However, you must update existing pipelines that use the <a href="https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.extensions.html#kfp.gcp.use_gcp_secret">use_gcp_secret kfp sdk operator</a>. Remove the <code>use_gcp_secret</code> usage to let your pipeline authenticate to Google Cloud using the default service account.</p>
<h2 id="securing-the-cluster-with-fine-grained-google-cloud-permission-control">Securing the cluster with fine-grained Google Cloud permission control</h2>
<h3 id="workload-identity">Workload Identity</h3>
<blockquote>
<p>Workload Identity is the recommended way for your GKE applications to consume services provided by Google APIs. You accomplish this by configuring a <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Kubernetes service account</a> to act as a <a href="https://cloud.google.com/iam/docs/service-accounts">Google service account</a>. Any Pods running as the Kubernetes service account then use the Google service account to authenticate to cloud services.</p>
</blockquote>
<p>Referenced from <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity Documentation</a>. Please read this doc for:</p>
<ul>
<li>A detailed introduction to Workload Identity.</li>
<li>Instructions to enable it on your cluster.</li>
<li>Whether its limitations affect your adoption.</li>
</ul>
<h4 id="terminology">Terminology</h4>
<p>This document distinguishes between <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Kubernetes service accounts</a> (KSAs) and <a href="https://cloud.google.com/iam/docs/service-accounts">Google service accounts</a> (GSAs). KSAs are Kubernetes resources, while GSAs are specific to Google Cloud. Other documentation usually refers to both of them as just &ldquo;service accounts&rdquo;.</p>
<h4 id="authoring-pipelines-to-use-workload-identity">Authoring pipelines to use Workload Identity</h4>
<p>Pipelines don&rsquo;t need any specific changes to authenticate to Google Cloud. With Workload Identity, pipelines run as the Google service account that is bound to the KSA.</p>
<p>However, existing pipelines that use <a href="https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.extensions.html#kfp.gcp.use_gcp_secret">use_gcp_secret kfp sdk operator</a> need to remove the <code>use_gcp_secret</code> usage to use the bound GSA.
You can also continue to use <code>use_gcp_secret</code> in a cluster with Workload Identity enabled and <code>use_gcp_secret</code> will take precedence for those workloads.</p>
<h4 id="cluster-setup-to-use-workload-identity-for-full-kubeflow">Cluster setup to use Workload Identity for Full Kubeflow</h4>
<p>Starting from Kubeflow 1.1, Kubeflow Pipelines supports multi-user isolation. Therefore, pipeline runs are executed in user namespaces using the <code>default-editor</code> KSA. The <code>default-editor</code> KSA is auto-bound to the GSA specified in the user profile, which defaults to a shared GSA <code>${KFNAME}-user@${PROJECT}.iam.gserviceaccount.com</code>.</p>
<p>If you want to bind the <code>default-editor</code> KSA with a different GSA for a specific namespace, refer to the [In-cluster authentication to Google Cloud](/{{ .Site.Params.version_url_prefix }}docs/authentication/#in-cluster-authentication) guide.</p>
<p>Additionally, the Kubeflow Pipelines UI, visualization, and TensorBoard server instances are deployed in your user namespace using the <code>default-editor</code> KSA. Therefore, to visualize results in the Pipelines UI, they can fetch artifacts in Google Cloud Storage using permissions of the same GSA you configured for this namespace.</p>
<h4 id="cluster-setup-to-use-workload-identity-for-pipelines-standalone">Cluster setup to use Workload Identity for Pipelines Standalone</h4>
<h5 id="1-create-your-cluster-with-workload-identity-enabled">1. Create your cluster with Workload Identity enabled</h5>
<ul>
<li>
<p>In Google Cloud Console UI, you can enable Workload Identity in <code>Create a Kubernetes cluster -&gt; Security -&gt; Enable Workload Identity</code> like the following:
<img src="/{{ .Site.Params.version_url_prefix }}docs/images/pipelines/v1/gke-enable-workload-identity.png"></p>
</li>
<li>
<p>Using <code>gcloud</code> CLI, you can enable it with:</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud beta container clusters create &lt;cluster-name&gt; <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --release-channel regular <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --workload-pool<span class="o">=</span>project-id.svc.id.goog
</span></span></code></pre></div><p>References:</p>
<ul>
<li>
<p><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#enable_workload_identity_on_a_new_cluster">Enable Workload Identity on a new cluster</a></p>
</li>
<li>
<p><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#enable_workload_identity_on_an_existing_cluster">Enable Workload Identity on an existing cluster</a></p>
</li>
</ul>
<h5 id="2-deploy-kubeflow-pipelines">2. Deploy Kubeflow Pipelines</h5>
<p>Deploy via <a href="https://www.kubeflow.org/docs/components/pipelines/installation/overview/#kubeflow-pipelines-standalone">Pipelines Standalone</a> as usual.</p>
<h5 id="3-bind-workload-identities-for-ksas-used-by-kubeflow-pipelines">3. Bind Workload Identities for KSAs used by Kubeflow Pipelines</h5>
<p>The following helper bash scripts bind Workload Identities for KSAs used by Kubeflow Pipelines:</p>
<ul>
<li><a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/gcp-workload-identity-setup.sh">gcp-workload-identity-setup.sh</a> helps you create GSAs and bind them to KSAs used by pipelines workloads. This script provides an interactive command line dialog with explanation messages.</li>
<li><a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/wi-utils.sh">wi-utils.sh</a> alternatively provides minimal utility bash functions that let you customize your setup. The minimal utilities make it easy to read and use programmatically.</li>
</ul>
<p>For example, to get a default setup using <code>gcp-workload-identity-setup.sh</code>, you can</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ curl -O https://raw.githubusercontent.com/kubeflow/pipelines/master/manifests/kustomize/gcp-workload-identity-setup.sh
</span></span><span class="line"><span class="cl">$ chmod +x ./gcp-workload-identity-setup.sh
</span></span><span class="line"><span class="cl">$ ./gcp-workload-identity-setup.sh
</span></span><span class="line"><span class="cl"># This prints the command&#39;s usage example and introduction.
</span></span><span class="line"><span class="cl"># Then you can run the command with required parameters.
</span></span><span class="line"><span class="cl"># Command output will tell you which GSAs and Workload Identity bindings have been
</span></span><span class="line"><span class="cl"># created.
</span></span></code></pre></div><h5 id="4-configure-iam-permissions-of-used-gsas">4. Configure IAM permissions of used GSAs</h5>
<p>If you used <code>gcp-workload-identity-setup.sh</code> to bind Workload Identities for your cluster, you can simply add the following IAM bindings:</p>
<ul>
<li>Give GSA <code>&lt;cluster-name&gt;-kfp-system@&lt;project-id&gt;.iam.gserviceaccount.com</code> <code>Storage Object Viewer</code> role to let UI load data in GCS in the same project.</li>
<li>Give GSA <code>&lt;cluster-name&gt;-kfp-user@&lt;project-id&gt;.iam.gserviceaccount.com</code> any permissions your pipelines need. For quick tryouts, you can give it <code>Project Editor</code> role for all permissions.</li>
</ul>
<p>If you configured bindings by yourself, here are Google Cloud permission requirements for KFP KSAs:</p>
<ul>
<li>Pipelines use <code>pipeline-runner</code> KSA. Configure IAM permissions of the GSA bound to this KSA to allow pipelines use Google Cloud APIs.</li>
<li>Pipelines UI uses <code>ml-pipeline-ui</code> KSA. Pipelines Visualization Server uses <code>ml-pipeline-visualizationserver</code> KSA. If you need to view artifacts and visualizations stored in Google Cloud Storage (GCS) from pipelines UI, you should add Storage Object Viewer permission (or the minimal required permission) to their bound GSAs.</li>
</ul>
<h3 id="google-service-account-keys-stored-as-kubernetes-secrets">Google service account keys stored as Kubernetes secrets</h3>
<p>It is recommended to use Workload Identity for easier and secure management, but you can also choose to use GSA keys.</p>
<h4 id="authoring-pipelines-to-use-gsa-keys">Authoring pipelines to use GSA keys</h4>
<p>Each pipeline step describes a
container that is run independently. If you want to grant access for a single step to use
one of your service accounts, you can use
<a href="https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.extensions.html#kfp.gcp.use_gcp_secret"><code>kfp.gcp.use_gcp_secret()</code></a>.
Examples for how to use this function can be found in the
<a href="https://github.com/kubeflow/examples/blob/871895c54402f68685c8e227c954d86a81c0575f/pipelines/mnist-pipelines/mnist_pipeline.py#L97">Kubeflow examples repo</a>.</p>
<h4 id="cluster-setup-to-use-use_gcp_secret-for-full-kubeflow">Cluster setup to use use_gcp_secret for Full Kubeflow</h4>
<p>From Kubeflow 1.1, there&rsquo;s no longer a <code>user-gcp-sa</code> secrets deployed for you. Recommend using Workload Identity instead.</p>
<p>For Kubeflow 1.0 or earlier, you don&rsquo;t need to do anything. Full Kubeflow deployment has already deployed the <code>user-gcp-sa</code> secret for you.</p>
<h4 id="cluster-setup-to-use-use_gcp_secret-for-pipelines-standalone">Cluster setup to use use_gcp_secret for Pipelines Standalone</h4>
<p>Pipelines Standalone require your manual setup for the <code>user-gcp-sa</code> secret used by <code>use_gcp_secret</code>.</p>
<p>Instructions to set up the secret:</p>
<ol>
<li>
<p>First download the GCE VM service account token (refer to <a href="https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys">Google Cloud documentation</a> for more information):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud iam service-accounts keys create application_default_credentials.json \
</span></span><span class="line"><span class="cl">  --iam-account [SA-NAME]@[PROJECT-ID].iam.gserviceaccount.com
</span></span></code></pre></div></li>
<li>
<p>Run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl create secret -n [your-namespace] generic user-gcp-sa \
</span></span><span class="line"><span class="cl">  --from-file=user-gcp-sa.json=application_default_credentials.json
</span></span></code></pre></div></li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e8f58dfedb7f24d2f30aab8ff214e5d9">2.3 - Upgrading</h1>
    <div class="lead">How to upgrade your Kubeflow Pipelines deployment on Google Cloud</div>
	<h2 id="before-you-begin">Before you begin</h2>
<p>There are various options on how to install Kubeflow Pipelines in the <a href="https://kubeflow.org/docs/components/pipelines/installation/overview/">Installation Options for Kubeflow Pipelines</a> guide. Be aware that upgrade support and instructions will vary depending on the method you used to install Kubeflow Pipelines.</p>
<h3 id="upgrade-related-feature-matrix">Upgrade-related feature matrix</h3>
<table>
<thead>
<tr>
<th>Installation \ Features</th>
<th>In-place upgrade</th>
<th>Reinstallation on the same cluster</th>
<th>Reinstallation on a different cluster</th>
<th>User customizations across upgrades (via <a href="https://kustomize.io/">Kustomize</a>)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Standalone</td>
<td></td>
<td> Data is deleted by default.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/manifests/kustomize/env/gcp">Standalone (managed storage)</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>full Kubeflow (&gt;= v1.1)</td>
<td></td>
<td></td>
<td>Needs documentation</td>
<td></td>
</tr>
<tr>
<td>full Kubeflow (&lt; v1.1)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AI Platform Pipelines</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AI Platform Pipelines (managed storage)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Notes:</p>
<ul>
<li>When you deploy Kubeflow Pipelines with managed storage on Google Cloud, you pipeline&rsquo;s metadata and artifacts are stored in <a href="https://cloud.google.com/storage/docs">Cloud Storage</a> and <a href="https://cloud.google.com/sql/docs">Cloud SQL</a>. Using managed storage makes it easier to manage, back up, and restore Kubeflow Pipelines data.</li>
</ul>
<h2 id="kubeflow-pipelines-standalone">Kubeflow Pipelines Standalone</h2>
<p>Upgrade Support for Kubeflow Pipelines Standalone is in <strong>Beta</strong>.</p>
<p><a href="https://kubeflow.org/docs/components/pipelines/installation/standalone-deployment/#upgrading-kubeflow-pipelines">Upgrading Kubeflow Pipelines Standalone</a> introduces how to upgrade in-place.</p>
<h2 id="full-kubeflow">Full Kubeflow</h2>
<p>On Google Cloud, the full Kubeflow deployment follows <a href="https://googlecontainertools.github.io/kpt/guides/producer/packages/">the package pattern</a> starting from Kubeflow 1.1.</p>
<p>The package pattern enables you to upgrade the full Kubeflow in-place while keeping user customizations  refer to the [Upgrade Kubeflow on Google Cloud](/{{ .Site.Params.version_url_prefix }}docs/deploy/upgrade) documentation for instructions.</p>
<p>However, there&rsquo;s no current support to upgrade from Kubeflow 1.0 or earlier to Kubeflow 1.1 while keeping Kubeflow Pipelines data. This may change in the future, so provide your feedback in <a href="https://github.com/kubeflow/pipelines/issues/4346">kubeflow/pipelines#4346</a> on GitHub.</p>
<h2 id="ai-platform-pipelines">AI Platform Pipelines</h2>
<p>Upgrade Support for AI Platform Pipelines is in <strong>Alpha</strong>.</p>


<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Warning</h4>

    Kubeflow Pipelines Standalone deployments also show up in the AI Platform Pipelines dashboard, DO NOT follow instructions below if you deployed Kubeflow Pipelines using standalone deployment.
Because data is deleted by default when a Kubeflow Pipelines Standalone deployment is deleted.

</div>

<p>Below are the steps that describe how to upgrade your AI Platform Pipelines instance while keeping existing data:</p>
<h3 id="for-instances-_without_-managed-storage">For instances <em>without</em> managed storage:</h3>
<ol>
<li><a href="https://cloud.google.com/ai-platform/pipelines/docs/getting-started#clean_up">Delete your AI Platform Pipelines instance</a> <strong>WITHOUT</strong> selecting <strong>Delete cluster</strong>. The persisted artifacts and database data are stored in persistent volumes in the cluster. They are kept by default when you do not delete the cluster.</li>
<li><a href="https://console.cloud.google.com/marketplace/details/google-cloud-ai-platform/kubeflow-pipelines">Reinstall Kubeflow Pipelines from the Google Cloud Marketplace</a> using the same <strong>Google Kubernetes Engine cluster</strong>, <strong>namespace</strong>, and <strong>application name</strong>. Persisted data will be automatically picked up during reinstallation.</li>
</ol>
<h3 id="for-instances-_with_-managed-storage">For instances <em>with</em> managed storage:</h3>
<ol>
<li><a href="https://cloud.google.com/ai-platform/pipelines/docs/getting-started#clean_up">Delete your AI Platform Pipelines instance</a>.</li>
<li>If you are upgrading from Kubeflow Pipelines 0.5.1, note that the Cloud Storage bucket is a required starting from 1.0.0. Previously deployed instances should be using a bucket named like &ldquo;<cloudsql instance connection name>-<database prefix or instance name>&rdquo;. Browse <a href="https://console.cloud.google.com/storage/browser">your Cloud Storage buckets</a> to find your existing bucket name and provide it in the next step.</li>
<li><a href="https://console.cloud.google.com/marketplace/details/google-cloud-ai-platform/kubeflow-pipelines">Reinstall Kubeflow Pipelines from the Google Cloud Marketplace</a> using the same application name and managed storage options as before. You can freely install it in any cluster and namespace (not necessarily the same as before), because persisted artifacts and database data are stored in managed storages (Cloud Storage and Cloud SQL), and will be automatically picked up during reinstallation.</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d79816c8e6fac2f07383ee4b56fd17de">2.4 - Enabling GPU and TPU</h1>
    <div class="lead">Enable GPU and TPU for Kubeflow Pipelines on Google Kubernetes Engine (GKE)</div>
	<p>This page describes how to enable GPU or TPU for a pipeline on GKE by using the Pipelines
DSL language.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>To enable GPU and TPU on your Kubeflow cluster, follow the instructions on how to
[customize](/{{ .Site.Params.version_url_prefix }}docs/customizing-gke#common-customizations) the GKE cluster for Kubeflow before
setting up the cluster.</p>
<h2 id="configure-containerop-to-consume-gpus">Configure ContainerOp to consume GPUs</h2>
<p>After enabling the GPU, the Kubeflow setup script installs a default GPU pool with type nvidia-tesla-k80 with auto-scaling enabled.
The following code consumes 2 GPUs in a ContainerOp.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp.dsl</span> <span class="k">as</span> <span class="nn">dsl</span>
</span></span><span class="line"><span class="cl"><span class="n">gpu_op</span> <span class="o">=</span> <span class="n">dsl</span><span class="o">.</span><span class="n">ContainerOp</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;gpu-op&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">set_gpu_limit</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></div><p>The code above will be compiled into Kubernetes Pod spec:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">container</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="l">...</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;2&#34;</span><span class="w">
</span></span></span></code></pre></div><p>If the cluster has multiple node pools with different GPU types, you can specify the GPU type by the following code.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp.dsl</span> <span class="k">as</span> <span class="nn">dsl</span>
</span></span><span class="line"><span class="cl"><span class="n">gpu_op</span> <span class="o">=</span> <span class="n">dsl</span><span class="o">.</span><span class="n">ContainerOp</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;gpu-op&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">set_gpu_limit</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">gpu_op</span><span class="o">.</span><span class="n">add_node_selector_constraint</span><span class="p">(</span><span class="s1">&#39;cloud.google.com/gke-accelerator&#39;</span><span class="p">,</span> <span class="s1">&#39;nvidia-tesla-p4&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>The code above will be compiled into Kubernetes Pod spec:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">container</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="l">...</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;2&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">cloud.google.com/gke-accelerator</span><span class="p">:</span><span class="w"> </span><span class="l">nvidia-tesla-p4</span><span class="w">
</span></span></span></code></pre></div><p>See <a href="https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/tutorials/gpu">GPU tutorial</a> for a complete example to build a Kubeflow pipeline that uses GPUs.</p>
<p>Check the <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/gpus">GKE GPU guide</a> to learn more about GPU settings.</p>
<h2 id="configure-containerop-to-consume-tpus">Configure ContainerOp to consume TPUs</h2>
<p>Use the following code to configure ContainerOp to consume TPUs on GKE:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp.dsl</span> <span class="k">as</span> <span class="nn">dsl</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp.gcp</span> <span class="k">as</span> <span class="nn">gcp</span>
</span></span><span class="line"><span class="cl"><span class="n">tpu_op</span> <span class="o">=</span> <span class="n">dsl</span><span class="o">.</span><span class="n">ContainerOp</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;tpu-op&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">gcp</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">tpu_cores</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">tpu_resource</span> <span class="o">=</span> <span class="s1">&#39;v2&#39;</span><span class="p">,</span> <span class="n">tf_version</span> <span class="o">=</span> <span class="s1">&#39;1.12&#39;</span><span class="p">))</span>
</span></span></code></pre></div><p>The above code uses 8 v2 TPUs with TF version to be 1.12. The code above will be compiled into Kubernetes Pod spec:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">container</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="l">...</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">cloud-tpus.google.com/v2</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;8&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">tf-version.cloud-tpus.google.com</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;1.12&#34;</span><span class="w">
</span></span></span></code></pre></div><p>To learn more, see an <a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/preemptible_tpu_gpu/preemptible_tpu_gpu.py">example pipeline that uses a preemptible node pool with TPU or GPU.</a>.</p>
<p>See the <a href="https://cloud.google.com/tpu/docs/kubernetes-engine-setup">GKE TPU Guide</a> to learn more about TPU settings.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ee70eee465844b15b58f4b4844b00eab">2.5 - Using Preemptible VMs and GPUs on Google Cloud</h1>
    <div class="lead">Configuring preemptible VMs and GPUs for Kubeflow Pipelines on Google Cloud</div>
	<p>This document describes how to configure preemptible virtual machines
(<a href="https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms">preemptible VMs</a>)
and GPUs on preemptible VM instances
(<a href="https://cloud.google.com/compute/docs/instances/preemptible#preemptible_with_gpu">preemptible GPUs</a>)
for your workflows running on Kubeflow Pipelines on Google Cloud.</p>
<h2 id="introduction">Introduction</h2>
<p>Preemptible VMs are <a href="https://cloud.google.com/compute/docs/instances/">Compute Engine VM
instances</a> that last a maximum
of 24 hours and provide no availability guarantees. The
<a href="https://cloud.google.com/compute/pricing">pricing</a> of preemptible VMs is
lower than that of standard Compute Engine VMs.</p>
<p>GPUs attached to preemptible instances
(<a href="https://cloud.google.com/compute/docs/instances/preemptible#preemptible_with_gpu">preemptible GPUs</a>)
work like normal GPUs but persist only for the life of the instance.</p>
<p>Using preemptible VMs and GPUs can reduce costs on Google Cloud.
In addition to using preemptible VMs, your Google Kubernetes Engine (GKE)
cluster can autoscale based on current workloads.</p>
<p>This guide assumes that you have already deployed Kubeflow Pipelines. If not,
follow the guide to [deploying Kubeflow on Google Cloud](/{{ .Site.Params.version_url_prefix }}docs/deploy/).</p>
<h2 id="before-you-start">Before you start</h2>
<p>The variables defined in this page can be found in <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/env.sh">kubeflow-distribution/kubeflow/env.sh</a>. They are the same value as you set based on your [Kubeflow deployment](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli/#environment-variables).</p>
<h2 id="using-preemptible-vms-with-kubeflow-pipelines">Using preemptible VMs with Kubeflow Pipelines</h2>
<p>In summary, the steps to schedule a pipeline to run on <a href="https://cloud.google.com/compute/docs/instances/preemptible">preemptible
VMs</a> are as
follows:</p>
<ol>
<li>Create a
<a href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools">node pool</a>
in your cluster that contains preemptible VMs.</li>
<li>Configure your pipelines to run on the preemptible VMs.</li>
</ol>
<p>The following sections contain more detail about the above steps.</p>
<h3 id="1-create-a-node-pool-with-preemptible-vms">1. Create a node pool with preemptible VMs</h3>
<p>Create a <code>preemptible-nodepool.yaml</code> as below and fulfill all placerholder content <code>KF_NAME</code>, <code>KF_PROJECT</code>, <code>LOCATION</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: container.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ContainerNodePool
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  labels:
</span></span><span class="line"><span class="cl">    kf-name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">  name: PREEMPTIBLE_CPU_POOL
</span></span><span class="line"><span class="cl">  namespace: KF_PROJECT # kpt-set: ${gcloud.core.project}
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  location: LOCATION # kpt-set: ${location}
</span></span><span class="line"><span class="cl">  initialNodeCount: 1
</span></span><span class="line"><span class="cl">  autoscaling:
</span></span><span class="line"><span class="cl">    minNodeCount: 0
</span></span><span class="line"><span class="cl">    maxNodeCount: 5
</span></span><span class="line"><span class="cl">  nodeConfig:
</span></span><span class="line"><span class="cl">    machineType: n1-standard-4
</span></span><span class="line"><span class="cl">    diskSizeGb: 100
</span></span><span class="line"><span class="cl">    diskType: pd-standard
</span></span><span class="line"><span class="cl">    preemptible: true
</span></span><span class="line"><span class="cl">    taint:
</span></span><span class="line"><span class="cl">    - effect: NO_SCHEDULE
</span></span><span class="line"><span class="cl">      key: preemptible
</span></span><span class="line"><span class="cl">      value: &#34;true&#34;
</span></span><span class="line"><span class="cl">    oauthScopes:
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/logging.write&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/monitoring&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/devstorage.read_only&#34;
</span></span><span class="line"><span class="cl">    serviceAccountRef:
</span></span><span class="line"><span class="cl">      external: KF_NAME-vm@KF_PROJECT.iam.gserviceaccount.com # kpt-set: ${name}-vm@${gcloud.core.project}.iam.gserviceaccount.com
</span></span><span class="line"><span class="cl">    metadata:
</span></span><span class="line"><span class="cl">      disable-legacy-endpoints: &#34;true&#34;
</span></span><span class="line"><span class="cl">  management:
</span></span><span class="line"><span class="cl">    autoRepair: true
</span></span><span class="line"><span class="cl">    autoUpgrade: true
</span></span><span class="line"><span class="cl">  clusterRef:
</span></span><span class="line"><span class="cl">    name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">    namespace: KF_PROJECT # kpt-set: ${name}
</span></span></code></pre></div><p>Where:</p>
<ul>
<li><code>PREEMPTIBLE_CPU_POOL</code> is the name of the node pool.</li>
<li><code>KF_NAME</code> is the name of the Kubeflow GKE cluster.</li>
<li><code>KF_PROJECT</code> is the name of your Kubeflow Google Cloud project.</li>
<li><code>LOCATION</code> is the region of this nodepool, for example: us-west1-b.</li>
<li><code>KF_NAME-vm@KF_PROJECT.iam.gserviceaccount.com</code> is your service account, replace the <code>KF_NAME</code> and <code>KF_PROJECT</code> using the value above  in this pattern, you can get vm service account you have already created in Kubeflow cluster deployment</li>
</ul>
<p>Apply the nodepool patch file above by running:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="si">${</span><span class="nv">MGMTCTXT</span><span class="si">}</span> --namespace<span class="o">=</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span> apply -f &lt;path-to-nodepool-file&gt;/preemptible-nodepool.yaml
</span></span></code></pre></div><h4 id="for-kubeflow-pipelines-standalone-only">For Kubeflow Pipelines standalone only</h4>
<p>Alternatively, if you are on Kubeflow Pipelines standalone, or AI Platform Pipelines, you can run this command to create node pool:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud container node-pools create PREEMPTIBLE_CPU_POOL \
</span></span><span class="line"><span class="cl">    --cluster=CLUSTER_NAME \
</span></span><span class="line"><span class="cl">      --enable-autoscaling --max-nodes=MAX_NODES --min-nodes=MIN_NODES \
</span></span><span class="line"><span class="cl">      --preemptible \
</span></span><span class="line"><span class="cl">      --node-taints=preemptible=true:NoSchedule \
</span></span><span class="line"><span class="cl">      --service-account=DEPLOYMENT_NAME-vm@PROJECT_NAME.iam.gserviceaccount.com
</span></span></code></pre></div><p>Below is an example of command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud container node-pools create preemptible-cpu-pool \
</span></span><span class="line"><span class="cl">  --cluster=user-4-18 \
</span></span><span class="line"><span class="cl">    --enable-autoscaling --max-nodes=4 --min-nodes=0 \
</span></span><span class="line"><span class="cl">    --preemptible \
</span></span><span class="line"><span class="cl">    --node-taints=preemptible=true:NoSchedule \
</span></span><span class="line"><span class="cl">    --service-account=user-4-18-vm@ml-pipeline-project.iam.gserviceaccount.com
</span></span></code></pre></div><h3 id="2-schedule-your-pipeline-to-run-on-the-preemptible-vms">2. Schedule your pipeline to run on the preemptible VMs</h3>
<p>After configuring a node pool with preemptible VMs, you must configure your
pipelines to run on the preemptible VMs.</p>
<p>In the DSL code for
your pipeline, add the following to the <code>ContainerOp</code> instance:</p>
<pre><code>.apply(gcp.use_preemptible_nodepool())
</code></pre>
<p>The above function works for both methods of generating the <code>ContainerOp</code>:</p>
<ul>
<li>The <code>ContainerOp</code> generated from
<a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/components/_python_op.py"><code>kfp.components.func_to_container_op</code></a>.</li>
<li>The <code>ContainerOp</code> generated from the task factory function, which is
loaded by <a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/components/_components.py"><code>components.load_component_from_url</code></a>.</li>
</ul>
<p><strong>Note</strong>:</p>
<ul>
<li>Call <code>.set_retry(#NUM_RETRY)</code> on your <code>ContainerOp</code> to retry
the task after the task is preempted.</li>
<li>If you modified the
<a href="https://cloud.google.com/kubernetes-engine/docs/how-to/node-taints">node taint</a>
when creating the node pool, pass the same node toleration to the
<code>use_preemptible_nodepool()</code> function.</li>
<li><code>use_preemptible_nodepool()</code> also accepts a parameter <code>hard_constraint</code>. When the <code>hard_constraint</code> is
<code>True</code>, the system will strictly schedule the task in preemptible VMs. When the <code>hard_constraint</code> is
<code>False</code>, the system will try to schedule the task in preemptible VMs. If it cannot find the preemptible VMs,
or the preemptible VMs are busy, the system will schedule the task in normal VMs.</li>
</ul>
<p>For example:</p>
<pre><code>import kfp.dsl as dsl
import kfp.gcp as gcp

class FlipCoinOp(dsl.ContainerOp):
  &quot;&quot;&quot;Flip a coin and output heads or tails randomly.&quot;&quot;&quot;

  def __init__(self):
    super(FlipCoinOp, self).__init__(
      name='Flip',
      image='python:alpine3.6',
      command=['sh', '-c'],
      arguments=['python -c &quot;import random; result = \'heads\' if random.randint(0,1) == 0 '
                 'else \'tails\'; print(result)&quot; | tee /tmp/output'],
      file_outputs={'output': '/tmp/output'})

@dsl.pipeline(
  name='pipeline flip coin',
  description='shows how to use dsl.Condition.'
)

def flipcoin():
  flip = FlipCoinOp().apply(gcp.use_preemptible_nodepool())

if __name__ == '__main__':
  import kfp.compiler as compiler
  compiler.Compiler().compile(flipcoin, __file__ + '.zip')
</code></pre>
<h2 id="using-preemptible-gpus-with-kubeflow-pipelines">Using preemptible GPUs with Kubeflow Pipelines</h2>
<p>This guide assumes that you have already deployed Kubeflow Pipelines. In
summary, the steps to schedule a pipeline to run with
<a href="https://cloud.google.com/compute/docs/instances/preemptible#preemptible_with_gpu">preemptible GPUs</a>
are as follows:</p>
<ol>
<li>Make sure you have enough GPU quota.</li>
<li>Create a node pool in your GKE cluster that contains preemptible VMs with
preemptible GPUs.</li>
<li>Configure your pipelines to run on the preemptible VMs with preemptible
GPUs.</li>
</ol>
<p>The following sections contain more detail about the above steps.</p>
<h3 id="1-make-sure-you-have-enough-gpu-quota">1. Make sure you have enough GPU quota</h3>
<p>Add GPU quota to your Google Cloud project. The <a href="https://cloud.google.com/compute/docs/gpus/#introduction">Google Cloud
documentation</a> lists
the availability of GPUs across regions. To check the available quota for
resources in your project, go to the
<a href="https://console.cloud.google.com/iam-admin/quotas">Quotas</a> page in the Google Cloud
Console.</p>
<h3 id="2-create-a-node-pool-of-preemptible-vms-with-preemptible-gpus">2. Create a node pool of preemptible VMs with preemptible GPUs</h3>
<p>Create a <code>preemptible-gpu-nodepool.yaml</code> as below and fulfill all placerholder content:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: container.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ContainerNodePool
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  labels:
</span></span><span class="line"><span class="cl">    kf-name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">  name: KF_NAME-containernodepool-gpu
</span></span><span class="line"><span class="cl">  namespace: KF_PROJECT # kpt-set: ${gcloud.core.project}
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  location: LOCATION # kpt-set: ${location}
</span></span><span class="line"><span class="cl">  initialNodeCount: 1
</span></span><span class="line"><span class="cl">  autoscaling:
</span></span><span class="line"><span class="cl">    minNodeCount: 0
</span></span><span class="line"><span class="cl">    maxNodeCount: 5
</span></span><span class="line"><span class="cl">  nodeConfig:
</span></span><span class="line"><span class="cl">    machineType: n1-standard-4
</span></span><span class="line"><span class="cl">    diskSizeGb: 100
</span></span><span class="line"><span class="cl">    diskType: pd-standard
</span></span><span class="line"><span class="cl">    preemptible: true
</span></span><span class="line"><span class="cl">    oauthScopes:
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/logging.write&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/monitoring&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/devstorage.read_only&#34;
</span></span><span class="line"><span class="cl">    serviceAccountRef:
</span></span><span class="line"><span class="cl">      external: KF_NAME-vm@KF_PROJECT.iam.gserviceaccount.com # kpt-set: ${name}-vm@${gcloud.core.project}.iam.gserviceaccount.com
</span></span><span class="line"><span class="cl">    guestAccelerator:
</span></span><span class="line"><span class="cl">    - type: &#34;nvidia-tesla-k80&#34;
</span></span><span class="line"><span class="cl">      count: 1
</span></span><span class="line"><span class="cl">    metadata:
</span></span><span class="line"><span class="cl">      disable-legacy-endpoints: &#34;true&#34;
</span></span><span class="line"><span class="cl">  management:
</span></span><span class="line"><span class="cl">    autoRepair: true
</span></span><span class="line"><span class="cl">    autoUpgrade: true
</span></span><span class="line"><span class="cl">  clusterRef:
</span></span><span class="line"><span class="cl">    name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">    namespace: KF_PROJECT # kpt-set: ${gcloud.core.project}
</span></span></code></pre></div><p>Where:</p>
<ul>
<li><code>PREEMPTIBLE_CPU_POOL</code> is the name of the node pool.</li>
<li><code>KF_NAME</code> is the name of the Kubeflow GKE cluster.</li>
<li><code>KF_PROJECT</code> is the name of your Kubeflow Google Cloud project.</li>
<li><code>LOCATION</code> is the region of this nodepool, for example: us-west1-b.</li>
<li><code>KF_NAME-vm@KF_PROJECT.iam.gserviceaccount.com</code> is your service account, replace the <code>KF_NAME</code> and <code>KF_PROJECT</code> using the value above  in this pattern, you can get vm service account you have already created in Kubeflow cluster deployment.</li>
</ul>
<h4 id="for-kubeflow-pipelines-standalone-only-1">For Kubeflow Pipelines standalone only</h4>
<p>Alternatively, if you are on Kubeflow Pipelines standalone, or AI Platform Pipelines, you can run this command to create node pool:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud container node-pools create PREEMPTIBLE_GPU_POOL \
</span></span><span class="line"><span class="cl">    --cluster=CLUSTER_NAME \
</span></span><span class="line"><span class="cl">    --enable-autoscaling --max-nodes=MAX_NODES --min-nodes=MIN_NODES \
</span></span><span class="line"><span class="cl">    --preemptible \
</span></span><span class="line"><span class="cl">    --node-taints=preemptible=true:NoSchedule \
</span></span><span class="line"><span class="cl">    --service-account=DEPLOYMENT_NAME-vm@PROJECT_NAME.iam.gserviceaccount.com \
</span></span><span class="line"><span class="cl">    --accelerator=type=GPU_TYPE,count=GPU_COUNT
</span></span></code></pre></div><p>Below is an example of command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud container node-pools create preemptible-gpu-pool \
</span></span><span class="line"><span class="cl">    --cluster=user-4-18 \
</span></span><span class="line"><span class="cl">    --enable-autoscaling --max-nodes=4 --min-nodes=0 \
</span></span><span class="line"><span class="cl">    --preemptible \
</span></span><span class="line"><span class="cl">    --node-taints=preemptible=true:NoSchedule \
</span></span><span class="line"><span class="cl">    --service-account=user-4-18-vm@ml-pipeline-project.iam.gserviceaccount.com \
</span></span><span class="line"><span class="cl">    --accelerator=type=nvidia-tesla-t4,count=2
</span></span></code></pre></div><h3 id="3-schedule-your-pipeline-to-run-on-the-preemptible-vms-with-preemptible-gpus">3. Schedule your pipeline to run on the preemptible VMs with preemptible GPUs</h3>
<p>In the [DSL code](/{{ .Site.Params.version_url_prefix }}docs/components/pipelines/sdk/sdk-overview/) for
your pipeline, add the following to the <code>ContainerOp</code> instance:</p>
<pre><code>.apply(gcp.use_preemptible_nodepool()
</code></pre>
<p>The above function works for both methods of generating the <code>ContainerOp</code>:</p>
<ul>
<li>The <code>ContainerOp</code> generated from
<a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/components/_python_op.py"><code>kfp.components.func_to_container_op</code></a>.</li>
<li>The <code>ContainerOp</code> generated from the task factory function, which is
loaded by <a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/components/_components.py"><code>components.load_component_from_url</code></a>.</li>
</ul>
<p><strong>Note</strong>:</p>
<ul>
<li>Call <code>.set_gpu_limit(#NUM_GPUs, GPU_VENDOR)</code> on your
<code>ContainerOp</code> to specify the GPU limit (for example, <code>1</code>) and vendor (for
example, <code>'nvidia'</code>).</li>
<li>Call <code>.set_retry(#NUM_RETRY)</code> on your <code>ContainerOp</code> to retry
the task after the task is preempted.</li>
<li>If you modified the
<a href="https://cloud.google.com/kubernetes-engine/docs/how-to/node-taints">node taint</a>
when creating the node pool, pass the same node toleration to the
<code>use_preemptible_nodepool()</code> function.</li>
<li><code>use_preemptible_nodepool()</code> also accepts a parameter <code>hard_constraint</code>. When the <code>hard_constraint</code> is
<code>True</code>, the system will strictly schedule the task in preemptible VMs. When the <code>hard_constraint</code> is
<code>False</code>, the system will try to schedule the task in preemptible VMs. If it cannot find the preemptible VMs,
or the preemptible VMs are busy, the system will schedule the task in normal VMs.</li>
</ul>
<p>For example:</p>
<pre><code>import kfp.dsl as dsl
import kfp.gcp as gcp

class FlipCoinOp(dsl.ContainerOp):
  &quot;&quot;&quot;Flip a coin and output heads or tails randomly.&quot;&quot;&quot;

  def __init__(self):
    super(FlipCoinOp, self).__init__(
      name='Flip',
      image='python:alpine3.6',
      command=['sh', '-c'],
      arguments=['python -c &quot;import random; result = \'heads\' if random.randint(0,1) == 0 '
                 'else \'tails\'; print(result)&quot; | tee /tmp/output'],
      file_outputs={'output': '/tmp/output'})

@dsl.pipeline(
  name='pipeline flip coin',
  description='shows how to use dsl.Condition.'
)

def flipcoin():
  flip = FlipCoinOp().set_gpu_limit(1, 'nvidia').apply(gcp.use_preemptible_nodepool())
if __name__ == '__main__':
  import kfp.compiler as compiler
  compiler.Compiler().compile(flipcoin, __file__ + '.zip')
</code></pre>
<h2 id="debugging">Debugging</h2>
<p>Run the following command if your nodepool didn&rsquo;t show up or has error during provisioning:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="si">${</span><span class="nv">MGMTCTXT</span><span class="si">}</span> --namespace<span class="o">=</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span> describe containernodepool -l kf-name<span class="o">=</span><span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span>
</span></span></code></pre></div><h2 id="next-steps">Next steps</h2>
<ul>
<li>Explore further options for [customizing Kubeflow on Google Cloud](/{{ .Site.Params.version_url_prefix }}docs/).</li>
<li>See how to <a href="https://kubeflow.org/docs/components/pipelines/sdk/">build pipelines with the SDK</a>.</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0d94edcf690e8a4204c514d9d35c3f61">3 - Customize Kubeflow on GKE</h1>
    <div class="lead">Tailoring a GKE deployment of Kubeflow</div>
	<p>This guide describes how to customize your deployment of Kubeflow on Google
Kubernetes Engine (GKE) on Google Cloud.</p>
<h2 id="before-you-start">Before you start</h2>
<p>The variables defined in this page can be found in <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/env.sh">kubeflow-distribution/kubeflow/env.sh</a>. They are the same value as you set based on your [Kubeflow deployment](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli/#environment-variables).</p>
<h2 id="customizing-kubeflow-before-deployment">Customizing Kubeflow before deployment</h2>
<p>The Kubeflow deployment process is divided into two steps, <strong>hydrate</strong> and
<strong>apply</strong>, so that you can modify your configuration before deploying your
Kubeflow cluster.</p>
<p>Follow the guide to [deploying Kubeflow on Google Cloud](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli/). You can add your patches in corresponding component folder, and include those patches in <code>kustomization.yaml</code> file. Learn more about the usage of <a href="https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/">kustomize</a>. You can also find the existing kustomization in <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution">GoogleCloudPlatform/kubeflow-distribution</a> as example. After adding the patches, you can run <code>make hydrate</code> to validate the resulting resources. Finally, you can run <code>make apply</code> to deploy the customized Kubeflow.</p>
<h2 id="customizing-an-existing-deployment">Customizing an existing deployment</h2>
<p>You can also customize an existing Kubeflow deployment. In that case, this
guide assumes that you have already followed the guide to
[deploying Kubeflow on Google Cloud](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli/) and have deployed
Kubeflow to a GKE cluster.</p>
<h2 id="before-you-start-1">Before you start</h2>
<p>This guide assumes the following settings:</p>
<ul>
<li>
<p>The <code>${KF_DIR}</code> environment variable contains the path to
your Kubeflow application directory, which holds your Kubeflow configuration
files. For example, <code>/opt/kubeflow-distribution/kubeflow/</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export KF_DIR=&lt;path to your Kubeflow application directory&gt;
</span></span><span class="line"><span class="cl">cd &#34;${KF_DIR}&#34;
</span></span></code></pre></div></li>
<li>
<p>Make sure your environment variables are set up for the Kubeflow cluster you want to customize. For further background about the settings, see the guide to
[deploying Kubeflow with the CLI](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli).</p>
</li>
</ul>
<h2 id="customizing-google-cloud-resources">Customizing Google Cloud resources</h2>
<p>To customize Google Cloud resources, such as your Kubernetes Engine cluster, you can
modify the Deployment settings starting in <code>${KF_DIR}/common/cnrm</code>.</p>
<p>This folder contains multiple dependencies on sibling directories for Google Cloud resources. So you can start from here by reviewing <code>kustomization.yaml</code>. Depends on the type of Google Cloud resources you want to customize, you can add patches in corresponding directory.</p>
<ol>
<li>
<p>Make sure you checkin the existing resources in <code>/build</code> folder to source control.</p>
</li>
<li>
<p>Add the patches in corresponding directory, and update <code>kustomization.yaml</code> to include patches.</p>
</li>
<li>
<p>Run <code>make hydrate</code> to build new resources in <code>/build</code> folder.</p>
</li>
<li>
<p>Carefully examine the result resources in <code>/build</code> folder. If the customization is addition only, you can run <code>make apply</code> to directly patch the resources.</p>
</li>
<li>
<p>It is possible that you are modifying immutable resources. In this case, you will need to delete existing resource and applying new resources. Note that this might mean lost of your service and data, please execute carefully. General approach to delete and deploy Google Cloud resources:</p>
<ol>
<li>
<p>Revert to old resources in <code>/build</code> using source control.</p>
</li>
<li>
<p>Carefully delete the resource you need to delete by using <code>kubectl delete</code>.</p>
</li>
<li>
<p>Rebuild and apply new Google Cloud resources</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> common/cnrm
</span></span><span class="line"><span class="cl"><span class="nv">NAME</span><span class="o">=</span><span class="k">$(</span>NAME<span class="k">)</span> <span class="nv">KFCTXT</span><span class="o">=</span><span class="k">$(</span>KFCTXT<span class="k">)</span> <span class="nv">LOCATION</span><span class="o">=</span><span class="k">$(</span>LOCATION<span class="k">)</span> <span class="nv">PROJECT</span><span class="o">=</span><span class="k">$(</span>PROJECT<span class="k">)</span> make apply
</span></span></code></pre></div></li>
</ol>
<h2 id="customizing-kubeflow-resources">Customizing Kubeflow resources</h2>
<p>You can use <a href="https://kustomize.io/">kustomize</a> to customize Kubeflow.
Make sure that you have the minimum required version of kustomize:
<b>2.0.3</b> or later. For more information about
kustomize in Kubeflow, see how Kubeflow uses kustomize.</p>
<p>To customize the Kubernetes resources running within the cluster, you can modify
the kustomize manifests in corresponding component under <code>${KF_DIR}</code>.</p>
<p>For example, to modify settings for the Jupyter web app:</p>
<ol>
<li>
<p>Open <code>${KF_DIR}/apps/jupyter/jupyter-web-app/kustomization.yaml</code> in a text editor.</p>
</li>
<li>
<p>Review the file&rsquo;s inclusion of <code>deployment-patch.yaml</code>, and add your modification to <code>deployment-patch.yaml</code> based on the original content in <code>${KF_DIR}/apps/jupyter/jupyter-web-app/upstream/base/deployment.yaml</code>. For example: change <code>volumeMounts</code>&rsquo;s <code>mountPath</code> if you need to customize it.</p>
</li>
<li>
<p>Verify the output resources in <code>/build</code> folder using <code>Makefile</code>&quot;</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">make hydrate
</span></span></code></pre></div></li>
<li>
<p>Redeploy Kubeflow using <code>Makefile</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">make apply
</span></span></code></pre></div></li>
</ol>
<h2 id="common-customizations">Common customizations</h2>
<h3 id="add-users-to-kubeflow">Add users to Kubeflow</h3>
<p>You must grant each user the minimal permission scope that allows them to
connect to the Kubernetes cluster.</p>
<p>For Google Cloud, you should grant the following Cloud Identity and Access Management (IAM) roles.</p>
<p>In the following commands, replace <code>[PROJECT]</code> with your Google Cloud project and replace <code>[EMAIL]</code> with the user&rsquo;s email address:</p>
<ul>
<li>
<p>To access the Kubernetes cluster, the user needs the <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/iam">Kubernetes Engine
Cluster Viewer</a>
role:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding [PROJECT] --member=user:[EMAIL] --role=roles/container.clusterViewer
</span></span></code></pre></div></li>
<li>
<p>To access the Kubeflow UI through IAP, the user needs the
<a href="https://cloud.google.com/iap/docs/managing-access">IAP-secured Web App User</a>
role:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding [PROJECT] --member=user:[EMAIL] --role=roles/iap.httpsResourceAccessor
</span></span></code></pre></div><p>Note, you need to grant the user <code>IAP-secured Web App User</code> role even if the user is already an owner or editor of the project. <code>IAP-secured Web App User</code> role is not implied by the <code>Project Owner</code> or <code>Project Editor</code> roles.</p>
</li>
<li>
<p>To be able to run <code>gcloud container clusters get-credentials</code> and see logs in Cloud Logging
(formerly Stackdriver), the user needs viewer access on the project:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding [PROJECT] --member=user:[EMAIL] --role=roles/viewer
</span></span></code></pre></div></li>
</ul>
<p>Alternatively, you can also grant these roles on the <a href="https://console.cloud.google.com/iam-admin/iam">IAM page in the Cloud Console</a>. Make sure you are in the same project as your Kubeflow deployment.</p>
<p><a id="gpu-config"></a></p>
<h3 id="add-gpu-nodes-to-your-cluster">Add GPU nodes to your cluster</h3>
<p>To add GPU accelerators to your Kubeflow cluster, you have the following
options:</p>
<ul>
<li>Pick a Google Cloud zone that provides NVIDIA Tesla K80 Accelerators
(<code>nvidia-tesla-k80</code>).</li>
<li>Or disable node-autoprovisioning in your Kubeflow cluster.</li>
<li>Or change your node-autoprovisioning configuration.</li>
</ul>
<p>To see which accelerators are available in each zone, run the following
command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud compute accelerator-types list
</span></span></code></pre></div><p>Create the <a href="https://cloud.google.com/config-connector/docs/reference/resource-docs/container/containernodepool">ContainerNodePool</a> resource adopting GPU, for exmaple, create a new file <code>containernodepool-gpu.yaml</code> file and fulfill the value <code>KUBEFLOW-NAME</code>, <code>KF-PROJECT</code>, <code>LOCATION</code> based on your [Kubeflow deployment](/{{ .Site.Params.version_url_prefix }}docs/deploy/deploy-cli/#environment-variables):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: container.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ContainerNodePool
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  labels:
</span></span><span class="line"><span class="cl">    kf-name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">  name: containernodepool-gpu
</span></span><span class="line"><span class="cl">  namespace: KF_PROJECT # kpt-set: ${gcloud.core.project}
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  location: LOCATION # kpt-set: ${location}
</span></span><span class="line"><span class="cl">  initialNodeCount: 1
</span></span><span class="line"><span class="cl">  autoscaling:
</span></span><span class="line"><span class="cl">    minNodeCount: 0
</span></span><span class="line"><span class="cl">    maxNodeCount: 5
</span></span><span class="line"><span class="cl">  nodeConfig:
</span></span><span class="line"><span class="cl">    machineType: n1-standard-4
</span></span><span class="line"><span class="cl">    diskSizeGb: 100
</span></span><span class="line"><span class="cl">    diskType: pd-standard
</span></span><span class="line"><span class="cl">    preemptible: true
</span></span><span class="line"><span class="cl">    oauthScopes:
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/logging.write&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/monitoring&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/devstorage.read_only&#34;
</span></span><span class="line"><span class="cl">    guestAccelerator:
</span></span><span class="line"><span class="cl">    - type: &#34;nvidia-tesla-k80&#34;
</span></span><span class="line"><span class="cl">      count: 1
</span></span><span class="line"><span class="cl">    metadata:
</span></span><span class="line"><span class="cl">      disable-legacy-endpoints: &#34;true&#34;
</span></span><span class="line"><span class="cl">  management:
</span></span><span class="line"><span class="cl">    autoRepair: true
</span></span><span class="line"><span class="cl">    autoUpgrade: true
</span></span><span class="line"><span class="cl">  clusterRef:
</span></span><span class="line"><span class="cl">    name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">    namespace: KF_PROJECT # kpt-set: ${gcloud.core.project}
</span></span></code></pre></div><p>Note that the <code>metadata:name</code> must be unique in your Kubeflow project. Because the management cluster uses this as ID and your Google Cloud project as a namespace to identify a node pool.</p>
<p>Apply the node pool patch file above by running:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMTCTXT</span><span class="si">}</span><span class="s2">&#34;</span> --namespace<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span><span class="s2">&#34;</span> apply -f &lt;path-to-gpu-nodepool-file&gt;
</span></span></code></pre></div><p>After adding GPU nodes to your cluster, you need to install NVIDIA&rsquo;s device drivers to the nodes. Google provides a DaemonSet that automatically installs the drivers for you.
To deploy the installation DaemonSet, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span><span class="s2">&#34;</span> apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
</span></span></code></pre></div><p>To disable node-autoprovisioning, edit <code>${KF_DIR}/common/cluster/upstream/cluster.yaml</code> to set
<a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/v1.3.0/kubeflow/common/cluster/upstream/cluster.yaml#L30"><code>enabled</code></a>
to <code>false</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">    ...
</span></span><span class="line"><span class="cl">    clusterAutoscaling:
</span></span><span class="line"><span class="cl">      enabled: false
</span></span><span class="line"><span class="cl">      autoProvisioningDefaults:
</span></span><span class="line"><span class="cl">    ...
</span></span></code></pre></div><h3 id="add-cloud-tpus-to-your-cluster">Add Cloud TPUs to your cluster</h3>
<p>Note: The following instruction should be used when creating GKE cluster, because the TPU enablement flag <code>enableTpu</code> is immutable once cluster is created. You need to create new cluster if existing cluster doesn&rsquo;t have TPU enabled.</p>
<p>Set <a href="https://cloud.google.com/config-connector/docs/reference/resource-docs/container/containercluster"><code>enableTpu:true</code></a>
in <code>${KF_DIR}/common/cluster/upstream/cluster.yaml</code> and enable alias IP (VPC-native traffic routing):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: container.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ContainerCluster
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  ...
</span></span><span class="line"><span class="cl">  enableTpu: true
</span></span><span class="line"><span class="cl">  networkingMode: VPC_NATIVE
</span></span><span class="line"><span class="cl">  networkRef:
</span></span><span class="line"><span class="cl">    name: containercluster-dep-vpcnative
</span></span><span class="line"><span class="cl">  subnetworkRef:
</span></span><span class="line"><span class="cl">    name: containercluster-dep-vpcnative
</span></span><span class="line"><span class="cl">  ipAllocationPolicy:
</span></span><span class="line"><span class="cl">    servicesSecondaryRangeName: servicesrange
</span></span><span class="line"><span class="cl">    clusterSecondaryRangeName: clusterrange
</span></span><span class="line"><span class="cl">  ...
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">apiVersion: compute.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ComputeNetwork
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: containercluster-dep-vpcnative
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  routingMode: REGIONAL
</span></span><span class="line"><span class="cl">  autoCreateSubnetworks: false
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">apiVersion: compute.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ComputeSubnetwork
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: containercluster-dep-vpcnative
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  ipCidrRange: 10.2.0.0/16
</span></span><span class="line"><span class="cl">  region: us-west1
</span></span><span class="line"><span class="cl">  networkRef:
</span></span><span class="line"><span class="cl">    name: containercluster-dep-vpcnative
</span></span><span class="line"><span class="cl">  secondaryIpRange:
</span></span><span class="line"><span class="cl">  - rangeName: servicesrange
</span></span><span class="line"><span class="cl">    ipCidrRange: 10.3.0.0/16
</span></span><span class="line"><span class="cl">  - rangeName: clusterrange
</span></span><span class="line"><span class="cl">    ipCidrRange: 10.4.0.0/16
</span></span></code></pre></div><p>You can learn more at <a href="https://cloud.google.com/tpu/docs/kubernetes-engine-setup#new-cluster">Creating a new cluster with Cloud TPU support</a>, and view an example <a href="https://cloud.google.com/config-connector/docs/reference/resource-docs/container/containercluster">Vpc Native Container Cluster</a> config connector yaml file.</p>
<h2 id="more-customizations">More customizations</h2>
<p>Refer to the navigation panel on the left of these docs for more customizations,
including [using your own domain](/{{ .Site.Params.version_url_prefix }}docs/custom-domain) and more.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-969055f2547f60f12c1c4256f95b4eeb">4 - Using Your Own Domain</h1>
    <div class="lead">Using a custom domain with Kubeflow on GKE</div>
	<p>This guide assumes you have already set up Kubeflow on Google Cloud. If you haven&rsquo;t done
so, follow the guide to
[getting started with Kubeflow on Google Cloud](/{{ .Site.Params.version_url_prefix }}docs/deploy/).</p>
<h2 id="using-your-own-domain">Using your own domain</h2>
<p>If you want to use your own domain instead of <strong>${KF_NAME}.endpoints.${PROJECT}.cloud.goog</strong>, follow these instructions after building your cluster:</p>
<ol>
<li>
<p>Remove the substitution <code>hostname</code> in the Kptfile.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kpt cfg delete-subst instance hostname
</span></span></code></pre></div></li>
<li>
<p>Create a new setter <code>hostname</code> in the Kptfile.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kpt cfg create-setter instance/ hostname --field &#34;data.hostname&#34; --value &#34;&#34;
</span></span></code></pre></div></li>
<li>
<p>Configure new setter with your own domain.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kpt cfg set ./instance hostname &lt;enter your domain here&gt;
</span></span></code></pre></div></li>
<li>
<p>Apply the changes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">make apply-kubeflow
</span></span></code></pre></div></li>
<li>
<p>Check Ingress to verify that your domain was properly configured.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe ingresses
</span></span></code></pre></div></li>
<li>
<p>Get the address of the static IP address created.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">IPNAME=${KF_NAME}-ip
</span></span><span class="line"><span class="cl">gcloud compute addresses describe ${IPNAME} --global
</span></span></code></pre></div></li>
<li>
<p>Use your DNS provider to map the fully qualified domain specified in the third step to the above IP address.</p>
</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-361f92eed2eaa3203409b5ccbdfa0802">5 - Authenticating Kubeflow to Google Cloud</h1>
    <div class="lead">Authentication and authorization to Google Cloud</div>
	<p>This page describes in-cluster and local authentication for Kubeflow Google Cloud deployments.</p>
<h2 id="in-cluster-authentication">In-cluster authentication</h2>
<p>Starting from Kubeflow v0.6, you consume Kubeflow from custom namespaces (that is, namespaces other than <code>kubeflow</code>).
The <code>kubeflow</code> namespace is only for running Kubeflow system components. Individual jobs and model deployments
run in separate namespaces.</p>
<h3 id="google-kubernetes-engine-gke-workload-identity">Google Kubernetes Engine (GKE) workload identity</h3>
<p>Starting in v0.7, Kubeflow uses the new GKE feature: <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">workload identity</a>.
This is the recommended way to access Google Cloud APIs from your GKE cluster.
You can configure a Kubernetes service account (KSA) to act as a Google Cloud service account (GSA).</p>
<p>If you deployed Kubeflow following the Google Cloud instructions, then the profiler controller automatically binds the &ldquo;default-editor&rdquo; service account for every profile namespace to a default Google Cloud service account created during kubeflow deployment.
The Kubeflow deployment process also creates a default profile for the cluster admin.</p>
<p>For more info about profiles see the <a href="https://kubeflow.org/docs/components/multi-tenancy/">Multi-user isolation</a> page.</p>
<p>Here is an example profile spec:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: kubeflow/v1beta1
</span></span><span class="line"><span class="cl">kind: Profile
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  plugins:
</span></span><span class="line"><span class="cl">  - kind: WorkloadIdentity
</span></span><span class="line"><span class="cl">    spec:
</span></span><span class="line"><span class="cl">      gcpServiceAccount: ${SANAME}@${PROJECT}.iam.gserviceaccount.com
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>You can verify that there is a KSA called default-editor and that it has an annotation of the corresponding GSA:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n ${PROFILE_NAME} describe serviceaccount default-editor
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Name:        default-editor
</span></span><span class="line"><span class="cl">Annotations: iam.gke.io/gcp-service-account: ${KFNAME}-user@${PROJECT}.iam.gserviceaccount.com
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>You can double check that GSA is also properly set up:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud --project=${PROJECT} iam service-accounts get-iam-policy ${KFNAME}-user@${PROJECT}.iam.gserviceaccount.com
</span></span></code></pre></div><p>When a pod uses KSA default-editor, it can access Google Cloud APIs with the role granted to the GSA.</p>
<p><strong>Provisioning custom Google service accounts in namespaces</strong>:
When creating a profile, you can specify a custom Google Cloud service account for the namespace to control which Google Cloud resources are accessible.</p>
<p>Prerequisite: you must have permission to edit your Google Cloud project&rsquo;s IAM policy and to create a profile custom resource (CR) in your Kubeflow cluster.</p>
<ol>
<li>if you don&rsquo;t already have a Google Cloud service account you want to use, create a new one. For example: <code>user1-gcp@&lt;project-id&gt;.iam.gserviceaccount.com</code>:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud iam service-accounts create user1-gcp@&lt;project-id&gt;.iam.gserviceaccount.com
</span></span></code></pre></div><ol start="2">
<li>You can bind roles to the Google Cloud service account to allow access to the desired Google Cloud resources. For example to run BigQuery job, you can grant access like so:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding &lt;project-id&gt; \
</span></span><span class="line"><span class="cl">      --member=&#39;serviceAccount:user1-gcp@&lt;project-id&gt;.iam.gserviceaccount.com&#39; \
</span></span><span class="line"><span class="cl">      --role=&#39;roles/bigquery.jobUser&#39;
</span></span></code></pre></div><ol start="3">
<li><a href="https://cloud.google.com/sdk/gcloud/reference/iam/service-accounts/add-iam-policy-binding">Grant <code>owner</code> permission</a> of service account <code>user1-gcp@&lt;project-id&gt;.iam.gserviceaccount.com</code> to cluster account <code>&lt;cluster-name&gt;-admin@&lt;project-id&gt;.iam.gserviceaccount.com</code>:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud iam service-accounts add-iam-policy-binding \
</span></span><span class="line"><span class="cl">      user1-gcp@&lt;project-id&gt;.iam.gserviceaccount.com \
</span></span><span class="line"><span class="cl">      --member=&#39;serviceAccount:&lt;cluster-name&gt;-admin@&lt;project-id&gt;.iam.gserviceaccount.com&#39; --role=&#39;roles/owner&#39;
</span></span></code></pre></div><ol start="4">
<li>Manually create a profile for user1 and specify the Google Cloud service account to bind in <code>plugins</code> field:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow/v1beta1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Profile</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">profileName  </span><span class="w"> </span><span class="c"># replace with the name of the profile (the user&#39;s namespace name)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">owner</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">User</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">user1@email.com  </span><span class="w"> </span><span class="c"># replace with the email of the user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">plugins</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">WorkloadIdentity</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">gcpServiceAccount</span><span class="p">:</span><span class="w"> </span><span class="l">user1-gcp@project-id.iam.gserviceaccount.com</span><span class="w">
</span></span></span></code></pre></div><p><strong>Note:</strong>
The profile controller currently doesn&rsquo;t perform any access control checks to see whether the user creating the profile should be able to use the Google Cloud service account.
As a result, any user who can create a profile can get access to any service account for which the admin controller has owner permissions. We will improve this in subsequent releases.</p>
<p>You can find more details on workload identity in the <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">GKE documentation</a>.</p>
<h3 id="authentication-from-kubeflow-pipelines">Authentication from Kubeflow Pipelines</h3>
<p>Starting from Kubeflow v1.1, Kubeflow Pipelines supports multi-user isolation. Therefore, pipeline runs are executed in user namespaces also using the <code>default-editor</code> KSA.</p>
<p>Additionally, the Kubeflow Pipelines UI, visualization, and TensorBoard server instances are deployed in your user namespace using the <code>default-editor</code> KSA. Therefore, to visualize results in the Pipelines UI, they can fetch artifacts in Google Cloud Storage using permissions of the same GSA you configured for this namespace.</p>
<p>For more details, refer to [Authenticating Pipelines to Google Cloud](/{{ .Site.Params.version_url_prefix }}docs/pipelines/authentication-pipelines/).</p>
<hr>
<h2 id="local-authentication">Local authentication</h2>
<h3 id="gcloud">gcloud</h3>
<p>Use the <a href="https://cloud.google.com/sdk/gcloud/"><code>gcloud</code> tool</a> to interact with Google Cloud on the command line.
You can use the <code>gcloud</code> command to <a href="https://cloud.google.com/sdk/gcloud/reference/container/clusters/create">set up Google Kubernetes Engine (GKE) clusters</a>,
and interact with other Google services.</p>
<h5 id="logging-in">Logging in</h5>
<p>You have two options for authenticating the <code>gcloud</code> command:</p>
<ul>
<li>
<p>You can use a <strong>user account</strong> to authenticate using a Google account (typically Gmail).
You can register a user account using <a href="https://cloud.google.com/sdk/gcloud/reference/auth/login"><code>gcloud auth login</code></a>,
which brings up a browser window to start the familiar Google authentication flow.</p>
</li>
<li>
<p>You can create a <strong>service account</strong> within your Google Cloud project. You can then
<a href="https://cloud.google.com/iam/docs/creating-managing-service-account-keys">download a <code>.json</code> key file</a>
associated with the account, and run the
<a href="https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"><code>gcloud auth activate-service-account</code></a>
command to authenticate your <code>gcloud</code> session.</p>
</li>
</ul>
<p>You can find more information in the <a href="https://cloud.google.com/sdk/docs/authorizing">Google Cloud docs</a>.</p>
<h5 id="listing-active-accounts">Listing active accounts</h5>
<p>You can run the following command to verify you are authenticating with the expected account:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud auth list
</span></span></code></pre></div><p>In the output of the command, an asterisk denotes your active account.</p>
<h5 id="viewing-iam-roles">Viewing IAM roles</h5>
<p>Permissions are handled in Google Cloud using <a href="https://cloud.google.com/iam/docs/understanding-roles">IAM Roles</a>.
These roles define which resources your account can read or write to. Provided you have the <a href="https://cloud.google.com/iam/docs/understanding-custom-roles#required_permissions_and_roles_">necessary permissions</a>,
you can check which roles were assigned to your account using the following gcloud command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">PROJECT_ID=your-gcp-project-id-here
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">gcloud projects get-iam-policy $PROJECT_ID --flatten=&#34;bindings[].members&#34; \
</span></span><span class="line"><span class="cl">    --format=&#39;table(bindings.role)&#39; \
</span></span><span class="line"><span class="cl">    --filter=&#34;bindings.members:$(gcloud config list account --format &#39;value(core.account)&#39;)&#34;
</span></span></code></pre></div><p>You can view and modify roles through the
<a href="https://console.cloud.google.com/iam-admin/">Google Cloud IAM console</a>.</p>
<p>You can find more information about IAM in the
<a href="https://cloud.google.com/iam/docs/granting-changing-revoking-access">Google Cloud docs</a>.</p>
<hr>
<h3 id="kubectl">kubectl</h3>
<p>The <a href="https://kubernetes.io/docs/reference/kubectl/overview/"><code>kubectl</code> tool</a> is used for interacting with a Kubernetes cluster through the command line.</p>
<h5 id="connecting-to-a-cluster-using-a-google-cloud-account">Connecting to a cluster using a Google Cloud account</h5>
<p>If you set up your Kubernetes cluster using GKE, you can authenticate with the cluster using a Google Cloud account.
The following commands fetch the credentials for your cluster and save them to your local
<a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/"><code>kubeconfig</code> file</a>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">CLUSTER_NAME=your-gke-cluster
</span></span><span class="line"><span class="cl">ZONE=your-gcp-zone
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">gcloud container clusters get-credentials $CLUSTER_NAME --zone $ZONE
</span></span></code></pre></div><p>You can find more information in the
<a href="https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl">Google Cloud docs</a>.</p>
<h5 id="changing-active-clusters">Changing active clusters</h5>
<p>If you work with multiple Kubernetes clusters, you may have multiple contexts saved in your local
<a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/"><code>kubeconfig</code> file</a>.
You can view the clusters you have saved by run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl config get-contexts
</span></span></code></pre></div><p>You can view which cluster is currently being controlled by <code>kubectl</code> with the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">CONTEXT_NAME=your-new-context
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-context $CONTEXT_NAME
</span></span></code></pre></div><p>You can find more information in the
<a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Kubernetes docs</a>.</p>
<h5 id="checking-rbac-permissions">Checking RBAC permissions</h5>
<p>Like GKE IAM, Kubernetes permissions are typically handled with a &ldquo;role-based authorization control&rdquo; (RBAC) system.
Each Kubernetes service account has a set of authorized roles associated with it. If your account doesn&rsquo;t have the
right roles assigned to it, certain tasks fail.</p>
<p>You can check if an account has the proper permissions to run a command by building a query structured as
<code>kubectl auth can-i [VERB] [RESOURCE] --namespace [NAMESPACE]</code>. For example, the following command verifies
that your account has permissions to create deployments in the <code>kubeflow</code> namespace:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl auth can-i create deployments --namespace kubeflow
</span></span></code></pre></div><p>You can find more information in the
<a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/">Kubernetes docs</a>.</p>
<h5 id="adding-rbac-permissions">Adding RBAC permissions</h5>
<p>If you find you are missing a permission you need, you can grant the missing roles to your service account using
Kubernetes resources.</p>
<ul>
<li><strong>Roles</strong> describe the permissions you want to assign. For example, <code>verbs: [&quot;create&quot;], resources:[&quot;deployments&quot;]</code></li>
<li><strong>RoleBindings</strong> define a mapping between the <code>Role</code>, and a specific service account</li>
</ul>
<p>By default, <code>Roles</code> and <code>RoleBindings</code> apply only to resources in a specific namespace, but there are also
<code>ClusterRoles</code> and <code>ClusterRoleBindings</code> that can grant access to resources cluster-wide</p>
<p>You can find more information in the
<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole">Kubernetes docs</a>.</p>
<h2 id="next-steps">Next steps</h2>
<p>See the [troubleshooting guide](/{{ .Site.Params.version_url_prefix }}docs/troubleshooting/) for help with diagnosing and fixing issues you may encounter with Kubeflow on Google Cloud</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c89dbb2f91c73dbff2bd78b4bc0de1d4">6 - Securing Your Clusters</h1>
    <div class="lead">How to secure Kubeflow clusters using private GKE</div>
	<p>Currently we are collecting interest for supporting private Kubeflow cluster deployment. Please upvote to <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/issues/267">Support private GKE cluster on Google Cloud</a> feature request if it fits your use case.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2ff67fee0173608876ccc012bf3ff6ce">7 - Troubleshooting Deployments on GKE</h1>
    <div class="lead">Help fixing problems on GKE and Google Cloud</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<p>This guide helps diagnose and fix issues you may encounter with Kubeflow on
Google Kubernetes Engine (GKE) and Google Cloud.</p>
<h2 id="before-you-start">Before you start</h2>
<p>This guide covers troubleshooting specifically for
[Kubeflow deployments on Google Cloud](/{{ .Site.Params.version_url_prefix }}docs/gke/deploy/).</p>
<p>For more help, try the
[general Kubeflow troubleshooting guide](/{{ .Site.Params.version_url_prefix }}docs/other-guides/troubleshooting).</p>
<p>This guide assumes the following settings:</p>
<ul>
<li>
<p>The <code>${KF_DIR}</code> environment variable contains the path to
your Kubeflow application directory, which holds your Kubeflow configuration
files. For example, <code>/opt/kubeflow-distribution/kubeflow/</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export KF_DIR=&lt;path to your Kubeflow application directory&gt;
</span></span></code></pre></div></li>
<li>
<p>The <code>${CONFIG_FILE}</code> environment variable contains the path to your
Kubeflow configuration file.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export CONFIG_FILE=${KF_DIR}/kfctl_gcp_iap.v1.0.2.yaml
</span></span></code></pre></div><p>Or:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export CONFIG_FILE=${KF_DIR}/kfctl_gcp_basic_auth.v1.0.2.yaml
</span></span></code></pre></div></li>
<li>
<p>The <code>${KF_NAME}</code> environment variable contains the name of your Kubeflow
deployment. You can find the name in your <code>${CONFIG_FILE}</code>
configuration file, as the value for the <code>metadata.name</code> key.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export KF_NAME=&lt;the name of your Kubeflow deployment&gt;
</span></span></code></pre></div></li>
<li>
<p>The <code>${PROJECT}</code> environment variable contains the ID of your Google Cloud project.
You can find the project ID in
your <code>${CONFIG_FILE}</code> configuration file, as the value for the <code>project</code> key.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export PROJECT=&lt;your Google Cloud project ID&gt;
</span></span></code></pre></div></li>
<li>
<p>The <code>${ZONE}</code> environment variable contains the Google Cloud zone where your
Kubeflow resources are deployed.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export ZONE=&lt;your Google Cloud zone&gt;
</span></span></code></pre></div></li>
<li>
<p>For further background about the above settings, see the guide to
[deploying Kubeflow with the CLI](/{{ .Site.Params.version_url_prefix }}docs/gke/deploy/deploy-cli).</p>
</li>
</ul>
<h2 id="troubleshooting-kubeflow-deployment-on-google-cloud">Troubleshooting Kubeflow deployment on Google Cloud</h2>
<p>Here are some tips for troubleshooting Google Cloud.</p>
<ul>
<li>Make sure you are a Google Cloud project owner.</li>
<li>Make sure you are using HTTPS.</li>
<li>Check project <a href="https://console.cloud.google.com/iam-admin/quotas">quota page</a> to see if any service&rsquo;s current usage reached quota limit, increase them as needed.</li>
<li>Check <a href="https://console.cloud.google.com/deployments">deployment manager page</a> and see if theres a failed deployment.</li>
<li>Check if endpoint is up: do <a href="https://mxtoolbox.com/DNSLookup.aspx">DNS lookup</a> against your Cloud Identity-Aware Proxy (Cloud IAP) URL and see if it resolves to the correct IP address.</li>
<li>Check if certificate succeeded: <code>kubectl describe certificates -n istio-system</code> should give you certificate status.</li>
<li>Check ingress status: <code>kubectl describe ingress -n istio-system</code></li>
<li>Check if <a href="https://console.cloud.google.com/endpoints">endpoint entry</a> is created. There should be one entry with name <code>&lt;deployment&gt;.endpoints.&lt;project&gt;.cloud.goog</code>
<ul>
<li>If endpoint entry doesn&rsquo;t exist, check <code>kubectl describe cloudendpoint -n istio-system</code></li>
</ul>
</li>
<li>If using IAP: make sure you [added](/{{ .Site.Params.version_url_prefix }}docs/gke/deploy/oauth-setup/) <code>https://&lt;deployment&gt;.endpoints.&lt;project&gt;.cloud.goog/_gcp_gatekeeper/authenticate</code>
as an authorized redirect URI for the OAUTH credentials used to create the deployment.</li>
<li>If using IAP: see the guide to
[monitoring your Cloud IAP setup](/{{ .Site.Params.version_url_prefix }}docs/gke/deploy/monitor-iap-setup/).</li>
<li>See the sections below for troubleshooting specific problems.</li>
<li>Please <a href="https://github.com/kubeflow/kubeflow/issues/new?template=bug_report.md">report a bug</a> if you can&rsquo;t resolve the problem by following the above steps.</li>
</ul>
<h3 id="dns-name-not-registered">DNS name not registered</h3>
<p>This section provides troubleshooting information for problems creating a DNS entry for your <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">ingress</a>. The ingress is a K8s resource
that creates a Google Cloud loadbalancer to enable http(s) access to Kubeflow web services from outside
the cluster. This section assumes
you are using <a href="https://cloud.google.com/endpoints/">Cloud Endpoints</a> and a DNS name of the following pattern</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://${KF_NAME}.endpoints.${PROJECT}.cloud.goog
</span></span></code></pre></div><p>Symptoms:</p>
<ul>
<li>
<p>When you access the URL in Chrome you get the error: <strong>server IP address could not be found</strong></p>
</li>
<li>
<p>nslookup for the domain name doesn&rsquo;t return the IP address associated with the ingress</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">nslookup ${KF_NAME}.endpoints.${PROJECT}.cloud.goog
</span></span><span class="line"><span class="cl">Server:   127.0.0.1
</span></span><span class="line"><span class="cl">Address:  127.0.0.1#53
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">** server can&#39;t find ${KF_NAME}.endpoints.${PROJECT}.cloud.goog: NXDOMAIN
</span></span></code></pre></div></li>
</ul>
<p>Troubleshooting</p>
<ol>
<li>
<p>Check the <code>cloudendpoints</code> resource</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get cloudendpoints -o yaml ${KF_NAME}
</span></span><span class="line"><span class="cl">kubectl describe cloudendpoints ${KF_NAME}
</span></span></code></pre></div><ul>
<li>Check if there are errors indicating problems creating the endpoint</li>
</ul>
</li>
<li>
<p>The status of the <code>cloudendpoints</code> object will contain the cloud operation used to register the operation</p>
<ul>
<li>
<p>For example</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"> status:
</span></span><span class="line"><span class="cl">   config: &#34;&#34;
</span></span><span class="line"><span class="cl">   configMapHash: &#34;&#34;
</span></span><span class="line"><span class="cl">   configSubmit: operations/serviceConfigs.jlewi-1218-001.endpoints.cloud-ml-dev.cloud.goog:43fe6c6f-eb9c-41d0-ac85-b547fc3e6e38
</span></span><span class="line"><span class="cl">   endpoint: jlewi-1218-001.endpoints.cloud-ml-dev.cloud.goog
</span></span><span class="line"><span class="cl">   ingressIP: 35.227.243.83
</span></span><span class="line"><span class="cl">   jwtAudiences: null
</span></span><span class="line"><span class="cl">   lastAppliedSig: 4f3b903a06a683b380bf1aac1deca72792472429
</span></span><span class="line"><span class="cl">   observedGeneration: 1
</span></span><span class="line"><span class="cl">   stateCurrent: ENDPOINT_SUBMIT_PENDING
</span></span></code></pre></div></li>
</ul>
</li>
</ol>
<ul>
<li>
<p>You can check the status of the operation by running:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud --project=${PROJECT} endpoints operations describe ${OPERATION}
</span></span></code></pre></div><ul>
<li>Operation is everything after <code>operations/</code> in the <code>configSubmit</code> field</li>
</ul>
</li>
</ul>
<h3 id="404-page-not-found-when-accessing-central-dashboard">404 Page Not Found When Accessing Central Dashboard</h3>
<p>This section provides troubleshooting information for 404s, page not found, being return by the central dashboard which is served at</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://${KUBEFLOW_FQDN}/
</span></span></code></pre></div><ul>
<li><em><strong>KUBEFLOW_FQDN</strong></em> is your project&rsquo;s OAuth web app URI domain name <code>&lt;name&gt;.endpoints.&lt;project&gt;.cloud.goog</code></li>
<li>Since we were able to sign in this indicates the Ambassador reverse proxy is up and healthy we can confirm this is the case by running the following command</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n ${NAMESPACE} get pods -l service=envoy
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">NAME                     READY     STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">envoy-76774f8d5c-lx9bd   2/2       Running   2          4m
</span></span><span class="line"><span class="cl">envoy-76774f8d5c-ngjnr   2/2       Running   2          4m
</span></span><span class="line"><span class="cl">envoy-76774f8d5c-sg555   2/2       Running   2          4m
</span></span></code></pre></div><ul>
<li>
<p>Try other services to see if they&rsquo;re accessible for example</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://${KUBEFLOW_FQDN}/whoami
</span></span><span class="line"><span class="cl">https://${KUBEFLOW_FQDN}/tfjobs/ui
</span></span><span class="line"><span class="cl">https://${KUBEFLOW_FQDN}/hub
</span></span></code></pre></div></li>
<li>
<p>If other services are accessible then we know its a problem specific to the central dashboard and not ingress</p>
</li>
<li>
<p>Check that the centraldashboard is running</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get pods -l app=centraldashboard
</span></span><span class="line"><span class="cl">NAME                                READY     STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">centraldashboard-6665fc46cb-592br   1/1       Running   0          7h
</span></span></code></pre></div></li>
<li>
<p>Check a service for the central dashboard exists</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get service -o yaml centraldashboard
</span></span></code></pre></div></li>
<li>
<p>Check that an Ambassador route is properly defined</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get service centraldashboard -o jsonpath=&#39;{.metadata.annotations.getambassador\.io/config}&#39;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">apiVersion: ambassador/v0
</span></span><span class="line"><span class="cl">  kind:  Mapping
</span></span><span class="line"><span class="cl">  name: centralui-mapping
</span></span><span class="line"><span class="cl">  prefix: /
</span></span><span class="line"><span class="cl">  rewrite: /
</span></span><span class="line"><span class="cl">  service: centraldashboard.kubeflow,
</span></span></code></pre></div></li>
<li>
<p>Check the logs of Ambassador for errors. See if there are errors like the following indicating
an error parsing the route.If you are using the new Stackdriver Kubernetes monitoring you can use the following filter in the <a href="https://console.cloud.google.com/logs/viewer">stackdriver console</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"> resource.type=&#34;k8s_container&#34;
</span></span><span class="line"><span class="cl"> resource.labels.location=${ZONE}
</span></span><span class="line"><span class="cl"> resource.labels.cluster_name=${CLUSTER}
</span></span><span class="line"><span class="cl"> metadata.userLabels.service=&#34;ambassador&#34;
</span></span><span class="line"><span class="cl">&#34;could not parse YAML&#34;
</span></span></code></pre></div></li>
</ul>
<h3 id="502-server-error">502 Server Error</h3>
<p>A 502 usually means traffic isn&rsquo;t even making it to the envoy reverse proxy. And it
usually indicates the loadbalancer doesn&rsquo;t think any backends are healthy.</p>
<ul>
<li>In Cloud Console select Network Services -&gt; Load Balancing
<ul>
<li>
<p>Click on the load balancer (the name should contain the name of the ingress)</p>
</li>
<li>
<p>The exact name can be found by looking at the <code>ingress.kubernetes.io/url-map</code> annotation on your ingress object</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">URLMAP=$(kubectl --namespace=${NAMESPACE} get ingress envoy-ingress -o jsonpath=&#39;{.metadata.annotations.ingress\.kubernetes\.io/url-map}&#39;)
</span></span><span class="line"><span class="cl">echo ${URLMAP}
</span></span></code></pre></div></li>
<li>
<p>Click on your loadbalancer</p>
</li>
<li>
<p>This will show you the backend services associated with the load balancer</p>
<ul>
<li>
<p>There is 1 backend service for each K8s service the ingress rule routes traffic too</p>
</li>
<li>
<p>The named port will correspond to the NodePort a service is using</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">NODE_PORT=$(kubectl --namespace=${NAMESPACE} get svc envoy -o jsonpath=&#39;{.spec.ports[0].nodePort}&#39;)
</span></span><span class="line"><span class="cl">BACKEND_NAME=$(gcloud compute --project=${PROJECT} backend-services list --filter=name~k8s-be-${NODE_PORT}- --format=&#39;value(name)&#39;)
</span></span><span class="line"><span class="cl">gcloud compute --project=${PROJECT} backend-services get-health --global ${BACKEND_NAME}
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>Make sure the load balancer reports the backends as healthy</p>
<ul>
<li>
<p>If the backends aren&rsquo;t reported as healthy check that the pods associated with the K8s service are up and running</p>
</li>
<li>
<p>Check that health checks are properly configured</p>
<ul>
<li>Click on the health check associated with the backend service for envoy</li>
<li>Check that the path is /healthz and corresponds to the path of the readiness probe on the envoy pods</li>
<li>See <a href="https://github.com/kubernetes-retired/contrib/tree/master/ingress/controllers/gce/examples/health_checks">K8s docs</a> for important information about how health checks are determined from readiness probes.</li>
</ul>
</li>
<li>
<p>Check firewall rules to ensure traffic isn&rsquo;t blocked from the Google Cloud loadbalancer</p>
<ul>
<li>
<p>The firewall rule should be added automatically by the ingress but its possible it got deleted if you have some automatic firewall policy enforcement. You can recreate the firewall rule if needed with a rule like this</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud compute firewall-rules create $NAME \
</span></span><span class="line"><span class="cl">--project $PROJECT \
</span></span><span class="line"><span class="cl">--allow tcp:$PORT \
</span></span><span class="line"><span class="cl">--target-tags $NODE_TAG \
</span></span><span class="line"><span class="cl">--source-ranges 130.211.0.0/22,35.191.0.0/16
</span></span></code></pre></div></li>
<li>
<p>To get the node tag</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># From the Kubernetes Engine cluster get the name of the managed instance group
</span></span><span class="line"><span class="cl">gcloud --project=$PROJECT container clusters --zone=$ZONE describe $CLUSTER
</span></span><span class="line"><span class="cl"># Get the template associated with the MIG
</span></span><span class="line"><span class="cl">gcloud --project=kubeflow-rl compute instance-groups managed describe --zone=${ZONE} ${MIG_NAME}
</span></span><span class="line"><span class="cl"># Get the instance tags from the template
</span></span><span class="line"><span class="cl">gcloud --project=kubeflow-rl compute instance-templates describe ${TEMPLATE_NAME}
</span></span></code></pre></div><p>For more info <a href="https://cloud.google.com/compute/docs/load-balancing/health-checks">see Google Cloud HTTP health check docs</a></p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>In Stackdriver Logging look at the Cloud Http Load Balancer logs</p>
<ul>
<li>Logs are labeled with the forwarding rule</li>
<li>The forwarding rules are available via the annotations on the ingress
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ingress.kubernetes.io/forwarding-rule
</span></span><span class="line"><span class="cl">ingress.kubernetes.io/https-forwarding-rule
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>Verify that requests are being properly routed within the cluster</p>
</li>
<li>
<p>Connect to one of the envoy proxies</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl exec -ti `kubectl get pods --selector=service=envoy -o jsonpath=&#39;{.items[0].metadata.name}&#39;` /bin/bash
</span></span></code></pre></div></li>
<li>
<p>Install curl in the pod</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apt-get update &amp;&amp; apt-get install -y curl
</span></span></code></pre></div></li>
<li>
<p>Verify access to the whoami app</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">curl -L -s -i http://envoy:8080/noiap/whoami
</span></span></code></pre></div></li>
<li>
<p>If this doesn&rsquo;t return a 200 OK response; then there is a problem with the K8s resources</p>
<ul>
<li>Check the pods are running</li>
<li>Check services are pointing at the points (look at the endpoints for the various services)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="gke-certificate-fails-to-be-provisioned">GKE Certificate Fails To Be Provisioned</h3>
<p>A common symptom of your certificate failing to be provisioned is SSL errors like <code>ERR_SSL_VERSION_OR_CIPHER_MISMATCH</code> when
you try to access the Kubeflow https endpoint.</p>
<p>To troubleshoot check the status of your GKE managed certificate</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe managedcertificate
</span></span></code></pre></div><p>If the certificate is in status <code>FailedNotVisible</code> then it means Google Cloud failed to provision the certificate
because it could not verify that you owned the domain by doing an ACME challenge. In order for Google Cloud to provision your certificate</p>
<ol>
<li>Your ingress must be created in order to associated a Google Cloud Load Balancer(GCLB) with the IP address for your endpoint</li>
<li>There must be a DNS entry mapping your domain name to the IP.</li>
</ol>
<p>If there is a problem preventing either of the above then Google Cloud will be unable to provision your certificate
and eventually enter the permanent failure state <code>FailedNotVisible</code> indicating your endpoint isn&rsquo;t accessible. The most common
cause is the ingress can&rsquo;t be created because the K8s secret containing OAuth credentials doesn&rsquo;t
exist.</p>
<p>To fix this you must first resolve the underlying problems preventing your ingress or DNS entry from being created.
Once the underlying problem has been fixed you can follow the steps below to force a new certificate to be
generated.</p>
<p>You can fix the certificate by performing the following steps to delete the existing certificate and create a new one.</p>
<ol>
<li>
<p>Get the name of the Google Cloud certificate</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe managedcertificate gke-certificate
</span></span></code></pre></div><ul>
<li>The status will contain <code>Certificate Name</code> which will start with <code>mcrt</code> make a note of this.</li>
</ul>
</li>
<li>
<p>Delete the ingress</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system delete ingress envoy-ingress
</span></span></code></pre></div></li>
<li>
<p>Ensure the certificate was deleted</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud --project=${PROJECT} compute ssl-certificates list
</span></span></code></pre></div><ul>
<li>Make sure the certificate obtained in the first step no longer exists</li>
</ul>
</li>
<li>
<p>Reapply kubeflow in order to recreate the ingress and certificate</p>
<ul>
<li>If you deployed with <code>kfctl</code> rerun <code>kfctl apply</code></li>
<li>If you deployed using the Google Cloud blueprint rerun <code>make apply-kubeflow</code></li>
</ul>
</li>
<li>
<p>Monitor the certificate to make sure it can be provisioned</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl --context=gcp-private-0527 -n istio-system describe managedcertificate gke-certificate
</span></span></code></pre></div></li>
<li>
<p>Since the ingress has been recreated we need to restart the pods that configure it</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system delete pods -l service=backend-updater
</span></span><span class="line"><span class="cl">kubectl -n istio-system delete pods -l service=iap-enabler
</span></span></code></pre></div></li>
</ol>
<h3 id="problems-with-ssl-certificate-from-lets-encrypt">Problems with SSL certificate from Let&rsquo;s Encrypt</h3>
<p>As of Kubeflow 1.0, Kubeflow should be using GKE Managed Certificates and no longer using Let&rsquo;s Encrypt.</p>
<p>See the guide to
[monitoring your Cloud IAP setup](/{{ .Site.Params.version_url_prefix }}docs/gke/deploy/monitor-iap-setup/).</p>
<h2 id="envoy-pods-crash-looping-root-cause-is-backend-quota-exceeded">Envoy pods crash-looping: root cause is backend quota exceeded</h2>
<p>If your logs show the
<a href="https://istio.io/docs/concepts/what-is-istio/#envoy">Envoy</a> pods crash-looping,
the root cause may be that you have exceeded your quota for some
backend services such as loadbalancers.
This is particularly likely if you have multiple, differently named deployments
in the same Google Cloud project using <a href="https://cloud.google.com/iap/">Cloud IAP</a>.</p>
<h3 id="the-error">The error</h3>
<p>The error looks like this for the pod&rsquo;s Envoy container:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl logs -n kubeflow envoy-79ff8d86b-z2snp envoy
</span></span><span class="line"><span class="cl">[2019-01-22 00:19:44.400][1][info][main] external/envoy/source/server/server.cc:184] initializing epoch 0 (hot restart version=9.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363)
</span></span><span class="line"><span class="cl">[2019-01-22 00:19:44.400][1][critical][main] external/envoy/source/server/server.cc:71] error initializing configuration &#39;/etc/envoy/envoy-config.json&#39;: unable to read file: /etc/envoy/envoy-config.json
</span></span></code></pre></div><p>And the Cloud IAP container shows a message like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Waiting for backend id PROJECT=&lt;your-project&gt; NAMESPACE=kubeflow SERVICE=envoy filter=name~k8s-be-30352-...
</span></span></code></pre></div><h3 id="diagnosing-the-cause">Diagnosing the cause</h3>
<p>You can verify the cause of the problem by entering the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe ingress
</span></span></code></pre></div><p>Look for something like this in the output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Events:
</span></span><span class="line"><span class="cl">  Type     Reason  Age                  From                     Message
</span></span><span class="line"><span class="cl">  ----     ------  ----                 ----                     -------
</span></span><span class="line"><span class="cl">  Warning  Sync    14m (x193 over 19h)  loadbalancer-controller  Error during sync: googleapi: Error 403: Quota &#39;BACKEND_SERVICES&#39; exceeded. Limit: 5.0 globally., quotaExceeded
</span></span></code></pre></div><h3 id="fixing-the-problem">Fixing the problem</h3>
<p>If you have any redundant Kubeflow deployments, you can delete them using
the <a href="https://cloud.google.com/deployment-manager/docs/">Deployment Manager</a>.</p>
<p>Alternatively, you can request more backend services quota on the Google Cloud Console.</p>
<ol>
<li>Go to the <a href="https://console.cloud.google.com/iam-admin/quotas?metric=Backend%20services">quota settings for backend services on the Google Cloud
Console</a>.</li>
<li>Click <strong>EDIT QUOTAS</strong>. A quota editing form opens on the right of the
screen.</li>
<li>Follow the form instructions to apply for more quota.</li>
</ol>
<h2 id="legacy-networks-are-not-supported">Legacy networks are not supported</h2>
<p>Cloud Filestore and GKE try to use the network named <code>default</code> by default. For older projects,
this will be a legacy network which is incompatible with Cloud Filestore and newer GKE features
like private clusters. This will
manifest as the error <strong>&ldquo;default is invalid; legacy networks are not supported&rdquo;</strong> when
deploying Kubeflow.</p>
<p>Here&rsquo;s an example error when deploying Cloud Filestore:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ERROR: (gcloud.deployment-manager.deployments.update) Error in Operation [operation-1533189457517-5726d7cfd19c9-e1b0b0b5-58ca11b8]: errors:
</span></span><span class="line"><span class="cl">- code: RESOURCE_ERROR
</span></span><span class="line"><span class="cl">  location: /deployments/jl-0801-b-gcfs/resources/filestore
</span></span><span class="line"><span class="cl">  message: &#39;{&#34;ResourceType&#34;:&#34;gcp-types/file-v1beta1:projects.locations.instances&#34;,&#34;ResourceErrorCode&#34;:&#34;400&#34;,&#34;ResourceErrorMessage&#34;:{&#34;code&#34;:400,&#34;message&#34;:&#34;network
</span></span><span class="line"><span class="cl">    default is invalid; legacy networks are not supported.&#34;,&#34;status&#34;:&#34;INVALID_ARGUMENT&#34;,&#34;statusMessage&#34;:&#34;Bad
</span></span><span class="line"><span class="cl">    Request&#34;,&#34;requestPath&#34;:&#34;https://file.googleapis.com/v1beta1/projects/cloud-ml-dev/locations/us-central1-a/instances&#34;,&#34;httpMethod&#34;:&#34;POST&#34;}}&#39;
</span></span></code></pre></div><p>To fix this we can create a new network:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cd ${KF_DIR}
</span></span><span class="line"><span class="cl">cp .cache/master/deployment/gke/deployment_manager_configs/network.* \
</span></span><span class="line"><span class="cl">   ./gcp_config/
</span></span></code></pre></div><p>Edit <code>network.yaml </code>to set the name for the network.</p>
<p>Edit <code>gcfs.yaml</code> to use the name of the newly created network.</p>
<p>Apply the changes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cd ${KF_DIR}
</span></span><span class="line"><span class="cl">kfctl apply -V -f ${CONFIG}
</span></span></code></pre></div><!-- ## CPU platform unavailable in requested zone

By default, we set minCpuPlatform to `Intel Haswell` to make sure AVX2 is supported.
See [troubleshooting](/{{ .Site.Params.version_url_prefix }}docs/other-guides/troubleshooting/) for more details.

If you encounter this `CPU platform unavailable` error (might manifest as
`Cluster is currently being created, deleted, updated or repaired and cannot be updated.`),
you can change the [zone](https://github.com/kubeflow/manifests/tree/master/gcp/deployment_manager_configs/cluster-kubeflow.yaml#L31)
or change the [minCpuPlatform](https://github.com/kubeflow/manifests/tree/master/gcp/deployment_manager_configs/cluster.jinja#L131).
See [here](https://cloud.google.com/compute/docs/regions-zones/#available)
for available zones and cpu platforms. -->
<h2 id="changing-the-oauth-client-used-by-iap">Changing the OAuth client used by IAP</h2>
<p>If you need to change the OAuth client used by IAP, you can run the following commands
to replace the Kubernetes secret containing the ID and secret.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n kubeflow delete secret kubeflow-oauth
</span></span><span class="line"><span class="cl">kubectl -n kubeflow create secret generic kubeflow-oauth \
</span></span><span class="line"><span class="cl">       --from-literal=client_id=${CLIENT_ID} \
</span></span><span class="line"><span class="cl">       --from-literal=client_secret=${CLIENT_SECRET}
</span></span></code></pre></div><h2 id="troubleshooting-ssl-certificate-errors">Troubleshooting SSL certificate errors</h2>
<p>This section describes how to enable service management API to avoid managed certificates failure.</p>
<p>To check your certificate:</p>
<ol>
<li>
<p>Run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe managedcertificate gke-certificate
</span></span></code></pre></div><p>Make sure the certificate status is either <code>Active</code> or <code>Provisioning</code> which means it is not ready. For more details on certificate status, refer to the <a href="https://cloud.google.com/load-balancing/docs/ssl-certificates?hl=en_US&amp;_ga=2.164380342.-821786221.1568995229#certificate-resource-status">certificate statuses descriptions</a> section. Also, make sure the domain name is correct.</p>
</li>
<li>
<p>Run the following command to look for the errors using the certificate name from the previous step:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud beta --project=${PROJECT} compute ssl-certificates describe --global ${CERTIFICATE_NAME}
</span></span></code></pre></div></li>
<li>
<p>Run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system get ingress envoy-ingress -o yaml
</span></span></code></pre></div><p>Make sure of the following:</p>
<ul>
<li>
<p><code>networking.gke.io/managed-certificates</code> annotation value points to the name of the Kubernetes managed certificate resource and is <code>gke-certificate</code>;</p>
</li>
<li>
<p>public IP address that is displayed in the status is assigned. See the example of IP address below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">status:
</span></span><span class="line"><span class="cl">  loadBalancer:
</span></span><span class="line"><span class="cl">    ingress:
</span></span><span class="line"><span class="cl">     - ip: 35.186.212.202
</span></span></code></pre></div></li>
<li>
<p>DNS entry for the domain has propagated. To verify this, use the following <code>nslookup</code> command example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">`nslookup ${DOMAIN}`
</span></span></code></pre></div></li>
<li>
<p>domain name is the fully qualified domain name which be the host value in the <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">ingress</a>. See the example below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">${KF_APP_NAME}.endpoints.${PROJECT}.cloud.goog
</span></span></code></pre></div></li>
</ul>
<p>Note that managed certificates cannot provision the certificate if the DNS lookup does not work properly.</p>
</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-49d3fab273dc187c2097b2cf2ff485fb">8 - Kubeflow On-premises on Anthos</h1>
    <div class="lead">Running Kubeflow across on-premises and cloud environments with Anthos</div>
	<h2 id="introduction">Introduction</h2>
<p><a href="https://cloud.google.com/anthos">Anthos</a> is a hybrid and multi-cloud
application platform developed and supported by Google. Anthos is built on
open source technologies, including Kubernetes, Istio, and Knative.</p>
<p>Using Anthos, you can create a consistent setup across your on-premises and
cloud environments, helping you to automate policy and security at scale.</p>
<p>We are collecting interest for Kubeflow on GKE On Prem. You can subscribe
to the GitHub issue <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/issues/138">GoogleCloudPlatform/kubeflow-distribution#138</a>.</p>
<h2 id="next-steps">Next steps</h2>
<p>While waiting for a response from the support team, you may like to [deploy Kubeflow on GKE](/{{ .Site.Params.version_url_prefix }}docs/deploy/).</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d9ec0777c4f7a8a41476a65f2ecca6f7">9 - Changelog</h1>
    <div class="lead">Kubeflow on GCP Changelog</div>
	<h2 id="161">1.6.1</h2>
<p><a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/releases/tag/v1.6.1">Release notes</a></p>
<h3 id="changes">Changes:</h3>
<ul>
<li> Upgraded upstream Manifests to v1.6.1.</li>
<li> Upgraded pipelines to v 2.0.0-alpha.6 (fixes #392).</li>
<li> Updated MySQL to 8.0 (#391).</li>
<li> Fixed ASM deployment issue (#389)</li>
<li> Minor improvements of deployment process.</li>
<li> Validated deployment using GKE 1.22.</li>
</ul>
<h2 id="160">1.6.0:</h2>
<p><a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/releases/tag/v1.6.0">Release notes</a></p>
<h3 id="changes-1">Changes:</h3>
<ul>
<li> Upgraded upstream Manifests to v1.6.0</li>
<li> Upgraded ASM to v 1.14 (#385).</li>
<li> Upgraded Knative to v 1.2 (#373).</li>
<li> Upgraded cert-manager to v 1.5 (#372).</li>
<li> Upgraded pipelines to v 2.0.0-alpha.4.</li>
<li> Upgraded APIs to support GKE 1.22 (#349).</li>
<li> Improved deployment stability (#371, #376, #384, #386).</li>
<li> Removed deprecated kfserving, cloud-endpoints, and application manifests (#375, #377).</li>
<li> Validated deployment using GKE 1.21 and GKE 1.22.</li>
</ul>
<h2 id="151">1.5.1</h2>
<p><a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/releases/tag/v1.5.1">Release notes</a></p>
<h3 id="changes-2">Changes:</h3>
<ul>
<li> Upgraded ASM to v 1.13.</li>
<li> Fixed KServe issues with dashboard (#362) and directory(#361).</li>
<li> Increased the maximum length of Kubeflow cluster name (#359).</li>
<li> Moved RequestAuthentication policy creation to iap-enabler to improve GitOps friendliness (#364).</li>
<li> Validated deployment using GKE 1.21.11.</li>
</ul>
<h2 id="150">1.5.0:</h2>
<p><a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/releases/tag/v1.5.0">Release notes</a></p>
<h3 id="changes-3">Changes:</h3>
<ul>
<li> Upgrade Kubeflow components versions as listed in <a href="https://github.com/kubeflow/manifests/tree/v1.5.0#kubeflow-components-versions">components versions table</a></li>
<li> Integrated with Config Controller, simplified management cluster maintenance cost, there is no need to manually upgrade Config Connector CRD.</li>
<li> Switch from kfserving to KServe as default serving component, you can switch back to kfserving in <code>config.yaml</code>.</li>
<li> Fixed cloudsqlproxy issue with livenessProbe configuration.</li>
<li> Validated deployment using GKE 1.20.12.</li>
</ul>
<h2 id="141">1.4.1</h2>
<p><a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/releases/tag/v1.4.1">Release notes</a></p>
<h3 id="changes-4">Changes:</h3>
<p>Changes on top of v1.4.0:</p>
<ul>
<li> Upgrade: Integrate with Kubeflow 1.4.1 manifests (kubeflow/manifests#2084)</li>
<li> Fix: Change cloud endpoint images destination (#343)</li>
<li> Fix: Use yq4 in iap-ingress Makefile.</li>
</ul>
<h2 id="140">1.4.0:</h2>
<p><a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/releases/tag/v1.4.0">Release notes</a></p>
<h3 id="changes-5">Changes:</h3>
<ul>
<li> Upgrade Kubeflow components versions as listed in components versions table</li>
<li> Removed GKE 1.18 image version and k8s runtime pin, now GKE version is default to Stable channel.</li>
<li> Set Emissary Executor as default Argo Workflow executor for Kubeflow Pipelines.</li>
<li> Upgraded kpt versions from 0.X.X to 1.0.0-beta.6.</li>
<li> Upgraded yq from v3 to v4.</li>
<li> Upgraded ASM(Anthos Service Mesh) to 1.10.4-asm.6.</li>
<li> Unblocked KFSserving usage by removing commonLabels from kustomization patch #298 #324.</li>
<li> Integrated with KFServing Web App UI.</li>
<li> Integrated with unified operator: training-operator.</li>
<li> Simplified deployment: Removed requirement for independent installation of yq, jq, kustomize, kpt.</li>
</ul>

</div>



    
      
  
  
  
  

  
  

  

    
	
  



          </main>
        </div>
      </div>
      
<footer class="bg-dark pt-3 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Kubeflow mailing list" aria-label="Kubeflow mailing list">
    <a class="text-white" target="_blank" rel="noopener" href="https://www.kubeflow.org/docs/about/community/#kubeflow-mailing-list" aria-label="Kubeflow mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" rel="noopener" href="https://www.kubeflow.org/docs/about/community/#kubeflow-slack" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" rel="noopener" href="https://twitter.com/kubeflow" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" rel="noopener" href="https://stackoverflow.com/questions/tagged/kubeflow" aria-label="Stack Overflow">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2022 Kubeflow on GCP authors | Documentation Distributed under CC BY 4.0</small>
        <p><small class="ml-1"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></small></p>
	
		
	
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener" href="https://github.com/GoogleCloudPlatform/kubeflow-distribution" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Kubeflow community calendar" aria-label="Kubeflow community calendar">
    <a class="text-white" target="_blank" rel="noopener" href="https://www.kubeflow.org/docs/about/community/#kubeflow-community-calendars" aria-label="Kubeflow community calendar">
      <i class="fa fa-calendar"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="/kubeflow-docs/js/main.min.301c0110334bec1b4445867d27dc7dd69b2df859154ccf252005508bc860cc5a.js" integrity="sha256-MBwBEDNL7BtERYZ9J9x91pst&#43;FkVTM8lIAVQi8hgzFo=" crossorigin="anonymous"></script>
<script src='/kubeflow-docs/js/tabpane-persist.js'></script>

  </body>
</html>
