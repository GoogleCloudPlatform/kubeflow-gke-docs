<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.105.0">
<link rel="canonical" type="text/html" href="https://gkcalat.github.io/kubeflow-docs/docs/distributions/">
<link rel="alternate" type="application/rss&#43;xml" href="https://gkcalat.github.io/kubeflow-docs/docs/distributions/index.xml">
<meta name="robots" content="noindex, nofollow">

<script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "WebSite",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gkcalat.github.io\/kubeflow-docs"
      },
      "articleSection" : "docs",
      "name" : "Distributions",
      "headline" : "Distributions",
      "description" : "A list of available Kubeflow distributions",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "0001",
      "datePublished": "0001-01-01 00:00:00 \u002b0000 UTC",
      "dateModified" : "0001-01-01 00:00:00 \u002b0000 UTC",
      "url" : "https:\/\/gkcalat.github.io\/kubeflow-docs\/docs\/distributions\/",
      "wordCount" : "0",
      "keywords" : [ "Kubeflow" ]
  }
  </script>

<link rel="shortcut icon" href="/kubeflow-docs/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/kubeflow-docs/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/kubeflow-docs/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/kubeflow-docs/favicons/favicon-32x32.png" sizes="32x32">
<link rel="manifest" href="/kubeflow-docs/favicons/manifest.json">
<meta name="msapplication-config" content="/kubeflow-docs/favicons/browserconfig.xml" />
<meta name="msapplication-TileColor" content="#4279f4">
<meta name="theme-color" content="#4279f4">

<title>Distributions | Kubeflow on Google Cloud Platform</title>
<meta name="description" content="A list of available Kubeflow distributions">
<meta property="og:title" content="Distributions" />
<meta property="og:description" content="A list of available Kubeflow distributions" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://gkcalat.github.io/kubeflow-docs/docs/distributions/" /><meta property="og:site_name" content="Kubeflow on Google Cloud Platform" />

<meta itemprop="name" content="Distributions">
<meta itemprop="description" content="A list of available Kubeflow distributions"><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Distributions"/>
<meta name="twitter:description" content="A list of available Kubeflow distributions"/>




<link rel="preload" href="/kubeflow-docs/scss/main.min.5006b434b14f73c3d5537a7ba4c8ecef723ddc93f3a3c8530e19d24f75486ea9.css" as="style">
<link href="/kubeflow-docs/scss/main.min.5006b434b14f73c3d5537a7ba4c8ecef723ddc93f3a3c8530e19d24f75486ea9.css" rel="stylesheet" integrity="">

<script
  src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
  crossorigin="anonymous"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-GK9XL47N6S"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-GK9XL47N6S', { 'anonymize_ip': false });
}
</script>

  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand-md navbar-dark  td-navbar">
        <a class="navbar-brand" href="/kubeflow-docs/">
		<span class="navbar-logo"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 276.93 274.55"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M95.9 62.15l4.1 102.1 73.75-94.12a6.79 6.79.0 019.6-1.11l46 36.92-15-65.61z" fill="#4279f4"/><path fill="#0028aa" d="M102.55 182.98h65.42l-40.17-32.23-25.25 32.23z"/><path fill="#014bd1" d="M180.18 83.92l-44 56.14 46.88 37.61 44.47-55.76-47.35-37.99z"/><path fill="#bedcff" d="M83.56 52.3l.01-.01 38.69-48.52-62.39 30.05-15.41 67.51 39.1-49.03z"/><path fill="#6ca1ff" d="M45.32 122.05l41.44 51.96-3.95-98.98-37.49 47.02z"/><path fill="#a1c3ff" d="M202.31 28.73 142.65.0l-37.13 46.56 96.79-17.83z"/><path d="M1.6 272v-44.78h5.74v23.41l20.48-23.41h6.4l-17.39 19.7 19 25.07H29.1l-15.92-20.8-5.84 6.65V272zm40.02-9.79V240h5.43v22.39a4.67 4.67.0 002.35 4.19 11 11 0 0011 0 4.69 4.69.0 002.33-4.19V240h5.43v22.19a9.08 9.08.0 01-4.1 7.87 16.2 16.2.0 01-18.37.0 9.07 9.07.0 01-4.07-7.85zM77.46 272v-48h5.43v16.81a29.29 29.29.0 019.32-1.73 13.1 13.1.0 016.2 1.41 10.71 10.71.0 014.18 3.74 18.07 18.07.0 012.23 5.06 21.26 21.26.0 01.73 5.58q0 8.43-4.38 12.79T87.35 272zm5.43-4.87h4.55q6.77.0 9.72-2.95t3-9.51a14.21 14.21.0 00-2-7.52 6.55 6.55.0 00-6-3.22 24.73 24.73.0 00-9.25 1.54zm29.47-11.19q0-7.71 4.09-12.3a13.75 13.75.0 0110.8-4.59q13.35.0 13.36 18.86h-22.82a12.3 12.3.0 002.9 7.07q2.59 3.11 7.9 3.1a24.92 24.92.0 0010.55-2v5a27.74 27.74.0 01-9.86 1.87 19.83 19.83.0 01-7.7-1.37 13.31 13.31.0 01-5.28-3.76 16.21 16.21.0 01-3-5.38 20.84 20.84.0 01-.94-6.5zm5.62-2.12h17.26a14.91 14.91.0 00-2.37-7.12 6.44 6.44.0 00-5.62-2.78 8.2 8.2.0 00-6.21 2.72 12.07 12.07.0 00-3.04 7.18z" fill="#4279f4" stroke="#4279f4" stroke-miterlimit="10" stroke-width="3.2"/><path d="M147.32 244.89V240h5v-7.59a8.14 8.14.0 012.31-6.05 7.79 7.79.0 015.69-2.28h7.86V229h-5c-2.21.0-3.67.45-4.37 1.34s-1.06 2.55-1.06 5V240h8.46v4.87h-8.46V272h-5.44v-27.1zM175.26 272v-48h5.43v48zm19.15-3.95a17.86 17.86.0 1112.33 4.9 16.57 16.57.0 01-12.33-4.9zm3.84-20.65a13.16 13.16.0 000 17.2 12.07 12.07.0 0017 0 13.09 13.09.0 000-17.2 12.07 12.07.0 00-17 0zm30.2-7.4h5.75l7.3 25.32 7.43-25.32h5.36l7.34 25.34L269 240h5.74l-10.04 32h-6.12l-6.83-24.58L245 272h-6.47z" fill="#0028aa" stroke="#0028aa" stroke-miterlimit="10" stroke-width="3.2"/></g></g></svg></span><span class="text-uppercase font-weight-bold">Kubeflow on Google Cloud Platform</span>
	</a>
	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main_navbar" aria-controls="main_navbar" aria-expanded="false" aria-label="Toggle navigation">
		<span class="navbar-toggler-icon"></span>
	</button>
	<div class="collapse navbar-collapse ml-md-auto" id="main_navbar">
		<ul class="navbar-nav ml-auto pt-4 pt-md-0 my-2 my-md-1">
			
			
			<li class="nav-item mr-2 mr-lg-4 mt-1 mt-lg-0">
				
				
				
				
				
				
				<a class="nav-link" href="/kubeflow-docs/docs/" ><span>Docs</span></a>
			</li>
			
			<li class="nav-item mr-2 mr-lg-4 mt-1 mt-lg-0">
				
				
				
				
				
				
				<a class="nav-link" href="/kubeflow-docs/news/" ><span>News</span></a>
			</li>
			
			<li class="nav-item mr-2 mr-lg-4 mt-1 mt-lg-0">
				
				
				
				
				
				
				<a class="nav-link" href="https://github.com/GoogleCloudPlatform/kubeflow-distribution" target="_blank" ><i class='fa-brands fa-github pr-2'></i><span>Source</span></a>
			</li>
			
			
			<li class="nav-item dropdown mt-1 mt-lg-0 mr-2">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Releases
</a>
<div class="dropdown-menu dropdown-menu-md-right dropdown-menu-lg-left" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/v1.6-release/docs">latest</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/v1.6-release/docs">v1.6</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/v1.5-release/docs">v1.5</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/v1.4-release/docs">v1.4</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/main/docs">dev</a>
	
</div>

			</li>
			
			
		</ul>
	</div>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This is the multi-page printable view of this section.
<a href="#" onclick="print();return false;">Click here to print</a>.
</p><p>
<a href="/kubeflow-docs/docs/distributions/">Return to the regular view of this page</a>.
</p>
</div>



<h1 class="title">Distributions</h1>
<div class="lead">A list of available Kubeflow distributions</div>




    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-a2cb35e1501be764c45cf3ac447895b3">Kubeflow on AWS</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>2: <a href="#pg-02f07162ae9199cda8c367edfd6b1147">Kubeflow on Azure</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1: <a href="#pg-296b8f4fb73897b806532822e3f3a641">Deployment</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1.1: <a href="#pg-ae0075ca9e9477c52b15bd0ce935d756">Install Kubeflow</a></li>


    
  
    
    
	
<li>2.1.2: <a href="#pg-d80a4f1149392569f5fa32bfeab74ec8">Initial cluster setup for existing cluster</a></li>


    
  
    
    
	
<li>2.1.3: <a href="#pg-cca4667a521d523791f9eca2edebf780">Uninstall Kubeflow</a></li>


    
  

    </ul>
    
  
    
    
	
<li>2.2: <a href="#pg-ceb950e97fd05a822eae67c3c1550705">Authentication using OIDC in Azure</a></li>


    
  
    
    
	
<li>2.3: <a href="#pg-a7982f9de61c52e0a0d173d95c15010d">Azure Machine Learning Components</a></li>


    
  
    
    
	
<li>2.4: <a href="#pg-76b20e0deb3b1a126cffda28d5d7870f">End-to-End Pipeline Example on Azure</a></li>


    
  
    
    
	
<li>2.5: <a href="#pg-8db8e540d9b02097d039dac1656bed32">Access Control for Azure Deployment</a></li>


    
  
    
    
	
<li>2.6: <a href="#pg-ef86fdffe53ad5bdd20b56381dd21142">Configure Azure MySQL database to store metadata</a></li>


    
  
    
    
	
<li>2.7: <a href="#pg-f40c9e23375af11c53e0a087013f0bf9">Troubleshooting Deployments on Azure AKS</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3: <a href="#pg-dd001bce04ba89733d9fb27c3064e6d8">Kubeflow on Google Cloud</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.1: <a href="#pg-0596cc9dd7074b5fe9d6610a9a4e4e6c">Deployment</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.1.1: <a href="#pg-84248b9ed230aeb39f1ad762d9ae0433">Overview</a></li>


    
  
    
    
	
<li>3.1.2: <a href="#pg-2276d0fa53a9f77a5aadb482040fbab4">Set up Project</a></li>


    
  
    
    
	
<li>3.1.3: <a href="#pg-698b42ce1b2b84b41dc49341fef9cec4">Set up OAuth client</a></li>


    
  
    
    
	
<li>3.1.4: <a href="#pg-c3209c70c33cf5aaf9eddafee3d5e0b0">Deploy Management cluster</a></li>


    
  
    
    
	
<li>3.1.5: <a href="#pg-e5d8863c4a5c353e271efdad4aa70269">Deploy Kubeflow cluster</a></li>


    
  
    
    
	
<li>3.1.6: <a href="#pg-8c408287dd856705815307c8c153e537">Upgrade Kubeflow</a></li>


    
  
    
    
	
<li>3.1.7: <a href="#pg-67037ec2f502c6d0b59e7689879bec78">Monitor Cloud IAP Setup</a></li>


    
  
    
    
	
<li>3.1.8: <a href="#pg-214814226b05ed257a32f69d832f8eaf">Delete Kubeflow</a></li>


    
  
    
    
	
<li>3.1.9: <a href="#pg-b1bfa0ba85c77f3778011197ed05b225">Deploy using UI</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3.2: <a href="#pg-91e76f8533117a222af5493748b5922e">Pipelines on Google Cloud</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>3.2.1: <a href="#pg-bf0071210c5937ae0322be8c4a921718">Connecting to Kubeflow Pipelines on Google Cloud using the SDK</a></li>


    
  
    
    
	
<li>3.2.2: <a href="#pg-1d8bca1af0a199f092910444014260e4">Authenticating Pipelines to Google Cloud</a></li>


    
  
    
    
	
<li>3.2.3: <a href="#pg-29022b460eea866cfda0585aa6967335">Upgrading</a></li>


    
  
    
    
	
<li>3.2.4: <a href="#pg-0b1f7130f0e8cf9fc127b06695b4a7e5">Enabling GPU and TPU</a></li>


    
  
    
    
	
<li>3.2.5: <a href="#pg-74493451fbef92c5f85e353ba719ee9d">Using Preemptible VMs and GPUs on Google Cloud</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3.3: <a href="#pg-5df0e440bd50642775398ffe0c30eff1">Customize Kubeflow on GKE</a></li>


    
  
    
    
	
<li>3.4: <a href="#pg-7f39074f07a3befd5b88014d70e65b71">Using Your Own Domain</a></li>


    
  
    
    
	
<li>3.5: <a href="#pg-58c78f426cfb6021c769a20fdcc5669d">Authenticating Kubeflow to Google Cloud</a></li>


    
  
    
    
	
<li>3.6: <a href="#pg-555bf215221d6c0d84a913c030f14a50">Securing Your Clusters</a></li>


    
  
    
    
	
<li>3.7: <a href="#pg-3f09966772a7188fe111e48915b76580">Troubleshooting Deployments on GKE</a></li>


    
  
    
    
	
<li>3.8: <a href="#pg-e2cfa50f92f61f2b363d831f5c905d72">Kubeflow On-premises on Anthos</a></li>


    
  

    </ul>
    
  
    
    
	
<li>4: <a href="#pg-50e7e176be038e50da2a8ffef395d701">Kubeflow on IBM Cloud</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.1: <a href="#pg-9da47466a15cd177f0b055b42c21f06a">Create or access an IBM Cloud Kubernetes cluster</a></li>


    
  
    
    
	
<li>4.2: <a href="#pg-a047fc4918a90f69c5ffaf6fbbf06a68">Create or access an IBM Cloud Kubernetes cluster on a VPC</a></li>


    
  
    
    
	
<li>4.3: <a href="#pg-66865d260b69636513b82f1bb3490e7e">Kubeflow Deployment on IBM Cloud</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.3.1: <a href="#pg-eaa47a4d7381236a0b84e713fc392ffa">Kubeflow Deployment Process</a></li>


    
  
    
    
	
<li>4.3.2: <a href="#pg-bb5daad82a3658275d7a80ec50f67eae">IBM Cloud Kubernetes and Kubeflow Compatibility</a></li>


    
  
    
    
	
<li>4.3.3: <a href="#pg-73f567b57211c57a9dfb28f8048e2037">Install Kubeflow on IKS</a></li>


    
  
    
    
	
<li>4.3.4: <a href="#pg-07a1bb96654c4b0fb7dfd8a4e48f6770">Install Kubeflow on OpenShift</a></li>


    
  
    
    
	
<li>4.3.5: <a href="#pg-7c1de18991e035a057289cfa97453b61">Securing the Kubeflow authentication with HTTPS</a></li>


    
  
    
    
	
<li>4.3.6: <a href="#pg-4befd9d928cd0502f971527786e1a7ec">Uninstall Kubeflow</a></li>


    
  

    </ul>
    
  
    
    
	
<li>4.4: <a href="#pg-365cfb913c393415a0eac321a2efbf41">Pipelines on IBM Cloud Kubernetes Service (IKS)</a></li>


    
  
    
    
	
<li>4.5: <a href="#pg-5da5a179b77d3d1cc1c87723ac674d84">Using IBM Cloud Container Registry (ICR)</a></li>


    
  
    
    
	
<li>4.6: <a href="#pg-d51039ac0de408f10e0b3a994766675d">End-to-end Kubeflow on IBM Cloud</a></li>


    
  

    </ul>
    
  
    
    
	
<li>5: <a href="#pg-f0cfc318fbada4d21ed442fc9bc7b049">Kubeflow on Nutanix Karbon</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.1: <a href="#pg-c4b39053be31a6a0e8e923da0b738a30">Install Kubeflow on Nutanix Karbon</a></li>


    
  
    
    
	
<li>5.2: <a href="#pg-f1ea8fcd5328b8423a137feaddd395e9">Integrate with Nutanix Storage</a></li>


    
  
    
    
	
<li>5.3: <a href="#pg-dfe39f17cfbea245375f242bad9886ed">Uninstall Kubeflow</a></li>


    
  

    </ul>
    
  
    
    
	
<li>6: <a href="#pg-a58acf930788dfecacd2359cbd4fd9c2">Arrikto Enterprise Kubeflow</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>7: <a href="#pg-9dd844c1b66939a21475d04273f4463e">Arrikto Kubeflow as a Service</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>8: <a href="#pg-3c2bf8504fc1ae4887ce2cd776b9fc62">Charmed Kubeflow from Canonical</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>9: <a href="#pg-e2fc0dc1a2cade96e8c94789ed8dc553">Kubeflow Operator</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>9.1: <a href="#pg-03edfaefabf5db9d82dbeaab04c99345">Introduction</a></li>


    
  
    
    
	
<li>9.2: <a href="#pg-bbe2c4524a0d40b13e62b3d1b362c63c">Installing Kubeflow Operator</a></li>


    
  
    
    
	
<li>9.3: <a href="#pg-56551ff9a7eea6eaffb455e46cbe6039">Installing Kubeflow</a></li>


    
  
    
    
	
<li>9.4: <a href="#pg-c27380d2054257759993453b91fecf50">Uninstalling Kubeflow</a></li>


    
  
    
    
	
<li>9.5: <a href="#pg-d080e1444da199bf0366d4c6b830fc29">Uninstalling Kubeflow Operator</a></li>


    
  
    
    
	
<li>9.6: <a href="#pg-2d2c088c1d636b7a383c0f880da89c73">Troubleshooting</a></li>


    
  

    </ul>
    
  
    
    
	
<li>10: <a href="#pg-9a55a407eb867eaec1ec07fe3a2078f4">Kubeflow on OpenShift</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>10.1: <a href="#pg-74cfacc63bdd2d6abb4a692b4893c7c3">Install Kubeflow on OpenShift</a></li>


    
  

    </ul>
    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-a2cb35e1501be764c45cf3ac447895b3">1 - Kubeflow on AWS</h1>
    <div class="lead">Running Kubeflow on Amazon EKS and Amazon Web Services</div>
	<p><a href="https://awslabs.github.io/kubeflow-manifests/">Kubeflow on AWS</a> is an open source distribution of Kubeflow that allows customers to build machine learning systems with ready-made AWS service integrations. Use Kubeflow on AWS to streamline data science tasks and build highly reliable, secure, and scalable machine learning systems with reduced operational overheads.</p>
<p>For more information, see the <a href="https://awslabs.github.io/kubeflow-manifests/docs/">Kubeflow on AWS documentation</a>.</p>

</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-02f07162ae9199cda8c367edfd6b1147">2 - Kubeflow on Azure</h1>
    <div class="lead">Running Kubeflow on Kubernetes Engine and Microsoft Azure</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-296b8f4fb73897b806532822e3f3a641">2.1 - Deployment</h1>
    <div class="lead">Instructions for deploying Kubeflow on Azure</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-ae0075ca9e9477c52b15bd0ce935d756">2.1.1 - Install Kubeflow</h1>
    <div class="lead">Instructions for deploying Kubeflow</div>
	<p>This guide describes how to use the kfctl binary to
deploy Kubeflow on Azure.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Install <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux">kubectl</a></li>
<li>Install and configure the <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest">Azure Command Line Interface (Az)</a>
<ul>
<li>Log in with <code>az login</code></li>
</ul>
</li>
<li>Install Docker
<ul>
<li>For Windows and WSL: <a href="https://docs.docker.com/docker-for-windows/wsl/">Guide</a></li>
<li>For other OS: <a href="https://docs.docker.com/docker-hub/">Docker Desktop</a></li>
</ul>
</li>
</ul>
<p>You do not need to have an existing Azure Resource Group or Cluster for AKS (Azure Kubernetes Service). You can create a cluster in the deployment process.</p>
<h2 id="understanding-the-deployment-process">Understanding the deployment process</h2>
<p>The deployment process is controlled by the following commands:</p>
<ul>
<li><strong>build</strong> - (Optional) Creates configuration files defining the various
resources in your deployment. You only need to run <code>kfctl build</code> if you want
to edit the resources before running <code>kfctl apply</code>.</li>
<li><strong>apply</strong> - Creates or updates the resources.</li>
<li><strong>delete</strong> - Deletes the resources.</li>
</ul>
<h3 id="app-layout">App layout</h3>
<p>Your Kubeflow application directory <strong>${KF_DIR}</strong> contains the following files and
directories:</p>
<ul>
<li>
<p><strong>${CONFIG_FILE}</strong> is a YAML file that defines configurations related to your
Kubeflow deployment.</p>
<ul>
<li>This file is a copy of the GitHub-based configuration YAML file that
you used when deploying Kubeflow. For example, <a href="https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_azure.v1.2.0.yaml">https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_azure.v1.2.0.yaml</a>
.</li>
<li>When you run <code>kfctl apply</code> or <code>kfctl build</code>, kfctl creates
a local version of the configuration file, <code>${CONFIG_FILE}</code>,
which you can further customize if necessary.</li>
</ul>
</li>
<li>
<p><strong>kustomize</strong> is a directory that contains the kustomize packages for Kubeflow applications.</p>
<ul>
<li>The directory is created when you run <code>kfctl build</code> or <code>kfctl apply</code>.</li>
<li>You can customize the Kubernetes resources (modify the manifests and run <code>kfctl apply</code> again).</li>
</ul>
</li>
</ul>
<p>If you experience any issues running these scripts, see the <a href="/docs/azure/troubleshooting-azure">troubleshooting guidance</a> for more information.</p>
<h2 id="azure-setup">Azure setup</h2>
<h3 id="to-log-into-azure-from-the-command-line-interface-run-the-following-commands">To log into Azure from the command line interface, run the following commands</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">az login
</span></span><span class="line"><span class="cl">az account set --subscription &lt;NAME OR ID OF SUBSCRIPTION&gt;
</span></span></code></pre></div><h3 id="initial-cluster-setup-for-new-cluster">Initial cluster setup for new cluster</h3>
<p>Create a resource group:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">az group create -n &lt;RESOURCE_GROUP_NAME&gt; -l &lt;LOCATION&gt;
</span></span></code></pre></div><p>Example variables:</p>
<ul>
<li><code>RESOURCE_GROUP_NAME=KubeTest</code></li>
<li><code>LOCATION=westus</code></li>
</ul>
<p>Create a specifically defined cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">az aks create -g &lt;RESOURCE_GROUP_NAME&gt; -n &lt;NAME&gt; -s &lt;AGENT_SIZE&gt; -c &lt;AGENT_COUNT&gt; -l &lt;LOCATION&gt; --generate-ssh-keys
</span></span></code></pre></div><p>Example variables:</p>
<ul>
<li><code>NAME=KubeTestCluster</code></li>
<li><code>AGENT_SIZE=Standard_D4s_v3</code></li>
<li><code>AGENT_COUNT=2</code></li>
<li><code>RESOURCE_GROUP_NAME=KubeTest</code></li>
</ul>
<p><strong>NOTE</strong>:  If you are using a GPU based AKS cluster (For example: AGENT_SIZE=Standard_NC6), you also need to <a href="https://docs.microsoft.com/azure/aks/gpu-cluster#install-nvidia-drivers">install the NVidia drivers</a> on the cluster nodes before you can use GPUs with Kubeflow.</p>
<h2 id="kubeflow-installation">Kubeflow installation</h2>
<p><strong>Important</strong>: To deploy Kubeflow on Azure with multi-user authentication and namespace separation, use the instructions for <a href="/docs/azure/authentication-oidc">Authentication using OICD in Azure</a>. The instructions in this guide apply only to a single-user Kubeflow deployment. Such a deployment cannot be upgraded to a multi-user deployment at this time.</p>
<p><strong>Note</strong>: kfctl is currently available for Linux and macOS users only. If you use Windows, you can install kfctl on Windows Subsystem for Linux (WSL). Refer to the official <a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">instructions</a> for setting up WSL.</p>
<p>Run the following commands to set up and deploy Kubeflow.</p>
<ol>
<li>
<p>Create user credentials. You only need to run this command once.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">az aks get-credentials -n &lt;NAME&gt; -g &lt;RESOURCE_GROUP_NAME&gt;
</span></span></code></pre></div></li>
<li>
<p>Download the kfctl v1.2.0 release from the
<a href="https://github.com/kubeflow/kfctl/releases/tag/v1.2.0">Kubeflow releases
page</a>.</p>
</li>
<li>
<p>Unpack the tar ball:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">tar -xvf kfctl_v1.2.0_&lt;platform&gt;.tar.gz
</span></span></code></pre></div></li>
<li>
<p>Run the following commands to set up and deploy Kubeflow. The code below includes an optional command to add the
binary kfctl to your path. If you don’t add the binary to your path, you must use the full path to the kfctl binary each time you run it.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># The following command is optional. It adds the kfctl binary to your path.
</span></span><span class="line"><span class="cl"># If you don&#39;t add kfctl to your path, you must use the full path
</span></span><span class="line"><span class="cl"># each time you run kfctl.
</span></span><span class="line"><span class="cl"># Use only alphanumeric characters or - in the directory name.
</span></span><span class="line"><span class="cl">export PATH=$PATH:&#34;&lt;path-to-kfctl&gt;&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># Set KF_NAME to the name of your Kubeflow deployment. You also use this
</span></span><span class="line"><span class="cl"># value as directory name when creating your configuration directory.
</span></span><span class="line"><span class="cl"># For example, your deployment name can be &#39;my-kubeflow&#39; or &#39;kf-test&#39;.
</span></span><span class="line"><span class="cl">export KF_NAME=&lt;your choice of name for the Kubeflow deployment&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># Set the path to the base directory where you want to store one or more 
</span></span><span class="line"><span class="cl"># Kubeflow deployments. For example, /opt/.
</span></span><span class="line"><span class="cl"># Then set the Kubeflow application directory for this deployment.
</span></span><span class="line"><span class="cl">export BASE_DIR=&lt;path to a base directory&gt;
</span></span><span class="line"><span class="cl">export KF_DIR=${BASE_DIR}/${KF_NAME}
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># Set the configuration file to use when deploying Kubeflow.
</span></span><span class="line"><span class="cl"># The following configuration installs Istio by default. Comment out 
</span></span><span class="line"><span class="cl"># the Istio components in the config file to skip Istio installation. 
</span></span><span class="line"><span class="cl"># See https://github.com/kubeflow/kubeflow/pull/3663
</span></span><span class="line"><span class="cl">export CONFIG_URI=&#34;https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_k8s_istio.v1.2.0.yaml&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">mkdir -p ${KF_DIR}
</span></span><span class="line"><span class="cl">cd ${KF_DIR}
</span></span><span class="line"><span class="cl">kfctl apply -V -f ${CONFIG_URI}
</span></span></code></pre></div><ul>
<li>
<p><strong>${KF_NAME}</strong> - The name of your Kubeflow deployment.
If you want a custom deployment name, specify that name here.
For example,  <code>my-kubeflow</code> or <code>kf-test</code>.
The value of KF_NAME must consist of lower case alphanumeric characters or
&lsquo;-&rsquo;, and must start and end with an alphanumeric character.
The value of this variable cannot be greater than 25 characters. It must
contain just a name, not a directory path.
You also use this value as directory name when creating the directory where
your Kubeflow  configurations are stored, that is, the Kubeflow application
directory.</p>
</li>
<li>
<p><strong>${KF_DIR}</strong> - The full path to your Kubeflow application directory.</p>
</li>
<li>
<p><strong>${CONFIG_URI}</strong> - The GitHub address of the configuration YAML file that
you want to use to deploy Kubeflow. The URI used in this guide is
<a href="https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_k8s_istio.v1.2.0.yaml">https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_k8s_istio.v1.2.0.yaml</a>.
When you run <code>kfctl apply</code> or <code>kfctl build</code> (see the next step), kfctl creates
a local version of the configuration YAML file which you can further
customize if necessary.</p>
</li>
</ul>
</li>
<li>
<p>Run this command to check that the resources have been deployed correctly in namespace <code>kubeflow</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get all -n kubeflow
</span></span></code></pre></div></li>
<li>
<p>Open the Kubeflow Dashboard</p>
<p>The default installation does not create an external endpoint but you can use port-forwarding to visit your cluster.
Run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80
</span></span></code></pre></div><p>Next, open <code>http://localhost:8080</code> in your browser.</p>
<p>To open the dashboard to a public IP address, you should first implement a solution to prevent unauthorized access.
You can read more about Azure authentication options from <a href="/docs/azure/authentication">Access Control for Azure Deployment</a>.</p>
</li>
</ol>
<h2 id="additional-information">Additional information</h2>
<p>You can find general information about Kubeflow configuration in the guide to <a href="/docs/methods/kfctl/kustomize/">configuring Kubeflow with kfctl and kustomize</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d80a4f1149392569f5fa32bfeab74ec8">2.1.2 - Initial cluster setup for existing cluster</h1>
    <div class="lead">Set up a cluster if you already have one</div>
	<h2 id="initial-setup-for-existing-cluster">Initial Setup for Existing Cluster</h2>
<p>Get the Kubeconfig file:</p>
<pre><code>az aks get-credentials -n &lt;NAME&gt; -g &lt;RESOURCE_GROUP_NAME&gt;
</code></pre>
<p>From here on, please see <a href="/docs/azure/deploy/install-kubeflow">Install Kubeflow</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-cca4667a521d523791f9eca2edebf780">2.1.3 - Uninstall Kubeflow</h1>
    <div class="lead">Instructions for uninstalling Kubeflow</div>
	<p>Uninstall Kubeflow on your Azure AKS cluster.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># Go to your Kubeflow deployment directory
</span></span><span class="line"><span class="cl">cd ${KF_DIR}
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># Remove Kubeflow
</span></span><span class="line"><span class="cl">kfctl delete -f ${CONFIG_FILE}
</span></span></code></pre></div>
</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ceb950e97fd05a822eae67c3c1550705">2.2 - Authentication using OIDC in Azure</h1>
    <div class="lead">Authentication and authorization support through OIDC for Kubeflow in Azure</div>
	<p>This section shows the how to set up Kubeflow with authentication and authorization support through OIDC in Azure using <a href="https://azure.microsoft.com/en-us/services/active-directory/">Azure Active Directory</a>.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>
<p>Install the <a href="/docs/azure/deploy/install-kubeflow">prerequisites for Kubeflow in Azure</a></p>
</li>
<li>
<p><a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app#register-an-application">Register an application with the Microsoft Identity Platform</a></p>
</li>
<li>
<p><a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app#add-a-client-secret">Add a client secret</a></p>
<p><strong>Note:</strong>  Save your client ID, client secret, and tenant ID in a secure place to be used in the next steps to configure OIDC Auth Service.
<strong>Note:</strong> The following installation steps automatically install a specific Istio version that must be used.</p>
</li>
</ul>
<h2 id="kubeflow-configuration">Kubeflow configuration</h2>
<ol>
<li>
<p>Download the kfctl v1.2.0 release from the
<a href="https://github.com/kubeflow/kfctl/releases/tag/v1.2.0">Kubeflow releases
page</a>.</p>
</li>
<li>
<p>Unpack the tar ball:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">tar -xvf kfctl_v1.2.0_&lt;platform&gt;.tar.gz
</span></span></code></pre></div></li>
<li>
<p>Run the below commands to build configuration files before deploying Kubeflow. The code below includes an optional command to add the binary kfctl to your path - if you don’t add it, you must use the full path to the kfctl binary each time you run it.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># The following command is optional, to make kfctl binary easier to use.
</span></span><span class="line"><span class="cl">export PATH=$PATH:&lt;path to where kfctl was unpacked&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># Set KF_NAME to the name of your Kubeflow deployment. This also becomes the
</span></span><span class="line"><span class="cl"># name of the directory containing your configuration.
</span></span><span class="line"><span class="cl"># For example, your deployment name can be &#39;my-kubeflow&#39; or &#39;kf-test&#39;.
</span></span><span class="line"><span class="cl">export KF_NAME=&lt;your choice of name for the Kubeflow deployment&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># Set the path to the base directory where you want to store one or more
</span></span><span class="line"><span class="cl"># Kubeflow deployments. For example, &#39;/opt/&#39;.
</span></span><span class="line"><span class="cl"># Then set the Kubeflow application directory for this deployment.
</span></span><span class="line"><span class="cl">export BASE_DIR=&lt;path to a base directory&gt;
</span></span><span class="line"><span class="cl">export KF_DIR=${BASE_DIR}/${KF_NAME}
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># Set the configuration file to use, such as the file specified below:
</span></span><span class="line"><span class="cl">export CONFIG_URI=&#34;https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_azure_aad.v1.2.0.yaml
</span></span></code></pre></div></li>
</ol>
<p>&quot;</p>
<pre><code># Generate and deploy Kubeflow:
mkdir -p ${KF_DIR}
cd ${KF_DIR}
kfctl build -V -f ${CONFIG_URI}
```

* **${KF_NAME}** - The name of your Kubeflow deployment.
  If you want a custom deployment name, specify that name here.
  For example,  `my-kubeflow` or `kf-test`.
  The value of `KF_NAME` must consist of lower case alphanumeric characters or
  '-', and must start and end with an alphanumeric character.
  The value of this variable cannot be greater than 25 characters. It must
  contain just a name, not a directory path.
  This value also becomes the name of the directory where your Kubeflow
  configurations are stored, that is, the Kubeflow application directory.

* **${KF_DIR}** - The full path to your Kubeflow application directory.
</code></pre>
<ol>
<li>
<p>Configure OIDC Auth service settings:</p>
<p>In <code>.cache/manifests/manifests-{kubeflow version}-branch/stacks/azure/application/oidc-authservice/kustomization.yaml</code> update the settings with values corresponding your app registration as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">- client_id=&lt;client_id&gt;
</span></span><span class="line"><span class="cl">- oidc_provider=https://login.microsoftonline.com/&lt;tenant_id&gt;/v2.0
</span></span><span class="line"><span class="cl">- oidc_redirect_uri=https://&lt;load_balancer_ip&gt; or dns_name&gt;/login/oidc
</span></span><span class="line"><span class="cl">- oidc_auth_url=https://login.microsoftonline.com/&lt;tenant_id&gt;/oauth2/v2.0/authorize
</span></span><span class="line"><span class="cl">- application_secret=&lt;client_secret&gt;
</span></span><span class="line"><span class="cl">- skip_auth_uri=
</span></span><span class="line"><span class="cl">- namespace=istio-system
</span></span><span class="line"><span class="cl">- userid-header=kubeflow-userid
</span></span><span class="line"><span class="cl">- userid-prefix=
</span></span></code></pre></div></li>
<li>
<p>Configure OIDC scopes:</p>
<p>In <code>.cache/manifests/manifests-{kkubeflow version}-branch/istio/oidc-authservice/base/statefulset.yaml</code> update <a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-permissions-and-consent#openid-connect-scopes">OIDC scopes</a> to remove groups and keep profile and email.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">- name: OIDC_SCOPES
</span></span><span class="line"><span class="cl"> value: &#34;profile email&#34;
</span></span></code></pre></div></li>
<li>
<p>Deploy Kubeflow:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kfctl apply -V -f <span class="si">${</span><span class="nv">CONFIG_URI</span><span class="si">}</span>
</span></span></code></pre></div></li>
<li>
<p>Check that the resources were deployed correctly in namespace <code>kubeflow</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get all -n kubeflow
</span></span></code></pre></div></li>
</ol>
<h2 id="expose-kubeflow-securely-over-https">Expose Kubeflow securely over HTTPS</h2>
<ol>
<li>
<p>Update Istio Gateway to expose port 443 with HTTPS and make port 80 redirect to 443:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl edit -n kubeflow gateways.networking.istio.io kubeflow-gateway
</span></span></code></pre></div><p>The Gateway spec should look like the following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">networking.istio.io/v1alpha3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow-gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">istio</span><span class="p">:</span><span class="w"> </span><span class="l">ingressgateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">servers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="s1">&#39;*&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">HTTP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="c"># Upgrade HTTP to HTTPS</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">tls</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">httpsRedirect</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="s1">&#39;*&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="m">443</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">HTTPS</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">tls</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l">SIMPLE</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">privateKey</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/istio/ingressgateway-certs/tls.key</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">serverCertificate</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/istio/ingressgateway-certs/tls.crt</span></span></span></code></pre></div>
</li>
<li>
<p>Expose Kubeflow with a load balancer service:</p>
<p>To expose Kubeflow with a load balancer service, change the type of the <code>istio-ingressgateway</code> service to <code>LoadBalancer</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl patch service -n istio-system istio-ingressgateway -p <span class="s1">&#39;{&#34;spec&#34;: {&#34;type&#34;: &#34;LoadBalancer&#34;}}&#39;</span>
</span></span></code></pre></div><p>After that, obtain the <code>LoadBalancer</code> IP address or Hostname from its status and create the necessary certificate.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get svc -n istio-system istio-ingressgateway -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.status.loadBalancer.ingress[0]}&#39;</span>
</span></span></code></pre></div><p><strong>Note</strong>: If you are exposing <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a> gateway through public IP, make sure it matches the IP address of the OIDC <code>REDIRECT_URL</code> by running:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get statefulset authservice -n istio-system -o yaml
</span></span></code></pre></div><p>If it doesn&rsquo;t match, update <code>REDIRECT_URL</code> in the StatefulSet to be the public IP address from the last step, by running:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl edit statefulset authservice -n istio-system
</span></span><span class="line"><span class="cl">kubectl rollout restart statefulset authservice -n istio-system
</span></span></code></pre></div></li>
<li>
<p>Create a self-signed Certificate with cert-manager:</p>
<p>Create a new file <code>certficate.yaml</code> with the YAML below to create a self-signed Certificate with cert-manager. For production environments, you should use appropriate trusted CA Certificate.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cert-manager.io/v1alpha2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Certificate</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">istio-ingressgateway-certs</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">istio-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">commonName</span><span class="p">:</span><span class="w"> </span><span class="l">istio-ingressgateway.istio-system.svc</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c"># Use ipAddresses if your LoadBalancer issues an IP address</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">ipAddresses</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="l">&lt;LoadBalancer IP&gt;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c"># Use dnsNames if your LoadBalancer issues a hostname</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">dnsNames</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="l">&lt;LoadBalancer HostName&gt;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">isCA</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">issuerRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterIssuer</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow-self-signing-issuer</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">istio-ingressgateway-certs</span></span></span></code></pre></div>
<p>Apply <code>certificate.yaml</code> in <code>istio-system</code> namespace</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl apply -f certificate.yaml -n istio-system
</span></span></code></pre></div><p>After applying the above Certificate, cert-manager will generate the TLS certificate inside the istio-ingressgateway-certs secrets. The istio-ingressgateway-certs secret is mounted on the istio-ingressgateway deployment and used to serve HTTPS.</p>
</li>
<li>
<p><a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app#add-a-redirect-uri">Configure Redirect URI for your registered App</a></p>
<p>Add the redirect URI below to the app registered with Microsoft Identity:</p>
<p><code>https://&lt;YOUR_LOADBALANCER_IP_ADDRESS_OR_DNS_NAME&gt;/login/oidc</code></p>
<p><strong>Note:</strong> Make sure the app&rsquo;s redirect URI matches the <code>oidc_redirect_uri</code> value in OIDC auth service settings.</p>
<p>Navigate to <code>https://&lt;YOUR_LOADBALANCER_IP_ADDRESS_OR_DNS_NAME&gt;/</code> and start using Kubeflow.</p>
</li>
</ol>
<h2 id="authenticate-kubeflow-pipelines-using-kubeflow-pipelines-sdkhttpswwwkubefloworgdocscomponentspipelinessdksdk-overview">Authenticate Kubeflow pipelines using <a href="https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/">Kubeflow Pipelines SDK</a></h2>
<p>Perform interactive login from browser by visitng <code>https://&lt;YOUR_LOADBALANCER_IP_ADDRESS_OR_DNS_NAME&gt;/</code> and copy the value of cookie <code>authservice_session</code> to authenticate using SDK with below code:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">import kfp
</span></span><span class="line"><span class="cl"><span class="nv">authservice_session_cookie</span><span class="o">=</span><span class="s1">&#39;authservice_session=&lt;cookie&gt;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="nv">client</span> <span class="o">=</span> kfp.Client<span class="o">(</span><span class="nv">host</span><span class="o">=</span><span class="s1">&#39;https://&lt;YOUR_LOADBALANCER_IP_ADDRESS_OR_DNS_NAME&gt;/pipeline&#39;</span>,
</span></span><span class="line"><span class="cl">                    <span class="nv">cookies</span><span class="o">=</span>authservice_session_cookie<span class="o">)</span>
</span></span><span class="line"><span class="cl">client.list_experiments<span class="o">(</span><span class="nv">namespace</span><span class="o">=</span><span class="s1">&#39;&lt;your_namespace&gt;&#39;</span><span class="o">)</span>
</span></span></code></pre></div><p><strong>Limitation:</strong> The current OIDC auth service in Kubeflow system supports only <a href="https://openid.net/specs/openid-connect-basic-1_0.html#CodeFlow">Authorization Code Flow</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a7982f9de61c52e0a0d173d95c15010d">2.3 - Azure Machine Learning Components</h1>
    <div class="lead">Overview of Azure Machine Learning pipeline components for workflow improvements</div>
	<p>Azure Machine Learning (Azure ML) components are pipeline components that integrate with <a href="https://docs.microsoft.com/en-us/azure/machine-learning/">Azure ML</a> to manage the lifecycle of your machine learning (ML) models to improve the quality and consistency of your machine learning solution. A pipeline component is a self-contained set of code that performs one step in the ML workflow. A component is analogous to a function, in that it has a name, parameters, return value and a body.</p>
<p>You can use Azure Machine Learning components to increase the efficiency of your workflow with Azure Machine Learning, such as continuous integration, delivery, and deployment.</p>
<p>The components provide capabilities for:</p>
<ul>
<li>Faster experimentation and development of models</li>
<li>Faster deployment of models into production</li>
<li>Quality assurance</li>
</ul>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>You should have Kubeflow installed on your AKS cluster. If you don&rsquo;t, follow the <a href="https://www.kubeflow.org/docs/azure/deploy/install-kubeflow/">Kubeflow installation (for Azure) guide</a>.</li>
<li>To interact with Azure resources, you may need to configure them before using a particular pipeline component. Check the README for each component to learn about what Azure resources are required.</li>
<li>The <code>kfp.azure</code> extension can be used to create a secret to interact with Azure resources. To create Azure credentials, run:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="c1"># Initialize variables:</span>
</span></span><span class="line"><span class="cl"><span class="nv">AZ_SUBSCRIPTION_ID</span><span class="o">={</span>Your_Azure_subscription_ID<span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="nv">AZ_TENANT_ID</span><span class="o">={</span>Your_Tenant_ID<span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="nv">AZ_CLIENT_ID</span><span class="o">={</span>Your_client_ID<span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="nv">AZ_CLIENT_SECRET</span><span class="o">={</span>Your_client_secret<span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="nv">KUBEFLOW_NAMESPACE</span><span class="o">=</span>kubeflow
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl create secret generic azcreds --from-literal<span class="o">=</span><span class="nv">AZ_SUBSCRIPTION_ID</span><span class="o">=</span><span class="nv">$AZ_SUBSCRIPTION_ID</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>                                      --from-literal<span class="o">=</span><span class="nv">AZ_TENANT_ID</span><span class="o">=</span><span class="nv">$AZ_TENANT_ID</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>                                      --from-literal<span class="o">=</span><span class="nv">AZ_CLIENT_ID</span><span class="o">=</span><span class="nv">$AZ_CLIENT_ID</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>                                      --from-literal<span class="o">=</span><span class="nv">AZ_CLIENT_SECRET</span><span class="o">=</span><span class="nv">$AZ_CLIENT_SECRET</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>                                      -n <span class="nv">$KUBEFLOW_NAMESPACE</span>
</span></span></code></pre></div><h2 id="azure-ml-register-model-component">Azure ML Register Model component</h2>
<p>Model registration allows you to store and version your models in Azure Machine Learning in your workspace. The model registry makes it easy to organize and keep track of your trained models. After you register the model, you can then download or deploy it and receive all the registered files.</p>
<p>To learn more about the Azure ML Register Model pipeline component, refer to the <a href="https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/components/azure/azureml/aml-register-model">official repository</a>.</p>
<p>To learn more about using Azure ML to manage the lifecycle of your models, go to <a href="https://docs.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment">Model management, deployment, and monitoring</a>.</p>
<h2 id="azure-ml-deploy-model-component">Azure ML Deploy Model component</h2>
<p>Trained machine learning models are deployed as web services in the cloud and you can use your model by accessing its endpoint. When using the model as a web service, the following items are included in the Azure ML Deploy Model component:</p>
<ul>
<li>An entry script</li>
<li>Azure ML environment configurations</li>
</ul>
<p>For more information about the Azure ML Deploy Model pipeline component, check the <a href="https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/components/azure/azureml/aml-deploy-model">official repository</a>.</p>
<p>To learn more about using Azure ML to manage the lifecycle of your models, go to <a href="https://docs.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment">Model management, deployment, and monitoring</a>.</p>
<h2 id="other-azure-ml-capabilities">Other Azure ML capabilities</h2>
<p>You can learn more about the capabilities of Azure ML and how to improve your ML workflow by checking the <a href="https://docs.microsoft.com/en-us/azure/machine-learning/">Azure ML documentation</a></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-76b20e0deb3b1a126cffda28d5d7870f">2.4 - End-to-End Pipeline Example on Azure</h1>
    <div class="lead">An end-to-end guide to creating a pipeline in Azure that can train, register, and deploy an ML model that can recognize the difference between tacos and burritos</div>
	<h2 id="introductions">Introductions</h2>
<h3 id="overview-of-azure-and-aks">Overview of Azure and AKS</h3>
<p>Microsoft Azure is an open, flexible, enterprise-grade cloud computing platform running on Microsoft infrastructure. The platform has various services, many of which are extremely useful in a pipeline that works with ML models.</p>
<p>The <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest">Azure CLI</a> is a set of tools that you can use to interact with Azure from the command line.</p>
<p>Azure Kubernetes Service (AKS) on Azure allows you to deploy containerized applications, within which you describe the resources your application needs, and AKS will manage the underlying resources automatically. This workflow is especially efficient at scale.</p>
<h3 id="the-overall-workflow">The overall workflow</h3>
<p>This guide takes you through using your Kubeflow deployment to build a machine learning (ML) pipeline on Azure. This guide uses a sample pipeline to detail the process of creating an ML workflow from scratch. You will learn how to create and run a pipeline that processes data, trains a model, and then registers and deploys that model as a webservice.</p>
<p>To build your pipeline, you must create and build containers using Docker images. Containers are used to abstract the dependencies for each step of the pipeline. You can manage your containers using <a href="https://ms.portal.azure.com/#home">Azure&rsquo;s portal</a>, specifically using the Container Registry to store the containers in the cloud. Kubeflow pulls the containers from this registry as they are needed in each step of the pipeline.</p>
<p>By following this guide, you will learn how to:</p>
<ul>
<li>Set up Kubeflow in an AKS Cluster</li>
<li>Create and compile a pipeline that can:
<ul>
<li>Preprocess data</li>
<li>Train a model</li>
<li>Register the model to ACR (<a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/languages/acr-template?view=azure-devops">Azure Container Registry</a>)</li>
<li>Profile the model to optimize compute resources in AML (Azure Machine Learning)</li>
<li>Deploy the model to AML</li>
</ul>
</li>
<li>Interact with and customize your deployment</li>
<li>Test and use your deployed model</li>
</ul>
<p>When your pipeline has finished running, you will be able to see a registered image, model, and deployment in your Azure ML workspace. You will then be able to visit the scoring URI and upload images for scoring in real time.</p>
<h2 id="set-up-your-environment">Set up your environment</h2>
<h3 id="download-the-project-files">Download the project files</h3>
<p>This tutorial uses the Azure Pipelines example in the Kubeflow examples repo. You can optionally use a pipeline of your own, but several key steps may differ.</p>
<p>Clone the project files and go to the directory containing the <a href="https://github.com/kubeflow/examples/tree/master/pipelines">Azure Pipelines (Tacos and Burritos)</a> example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">git clone https://github.com/kubeflow/examples.git
</span></span><span class="line"><span class="cl">cd examples/pipelines/azurepipeline
</span></span></code></pre></div><p>As an alternative to cloning, you can download the <a href="https://github.com/kubeflow/examples/archive/master.zip">Kubeflow examples repository zip file</a>.</p>
<h2 id="deploy-kubeflow">Deploy Kubeflow</h2>
<p>If you don&rsquo;t already have one, create an Azure account. If you have not used Azure services before, you can receive up to <a href="https://azure.microsoft.com/en-ca/free/">1 year of free services and free credits.</a></p>
<blockquote>
<p>Note: that some of the services used in this guide may not be included in the free services, but can be covered by free credits.</p>
</blockquote>
<p>First, install the <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest">Azure CLI</a>, then follow the instructions in the <a href="https://www.kubeflow.org/docs/azure/deploy/install-kubeflow/">guide to deploying Kubeflow on Azure</a>.</p>
<blockquote>
<p>Ensure that the agent size you use has the proper memory and storage requirements. For the Azure Pipelines example, most machine sizes will work, but <strong>premium storage</strong> is required. Use <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes">this guide</a> to choose the right agent size for your deployment. (We chose an agent size of Standard_D4s_v3.)</p>
</blockquote>
<h2 id="configuring-azure-resources">Configuring Azure resources</h2>
<h3 id="create-an-ml-workspace-in-azure">Create an ML workspace in Azure</h3>
<p>Throughout your pipeline&rsquo;s run, all of your models, images, and deployments will be pushed to your ML workspace in Azure. Your ML workspace also has support for managing your active deployments, which will be displayed later in this tutorial.</p>
<p>To create an ML workspace:</p>
<ol>
<li>Go to <a href="https://portal.azure.com">the Azure portal</a> and click on your resource group.</li>
<li>Select the <strong>add a new resource</strong> option.</li>
<li>Search for <strong>Machine Learning Studio Workspace</strong> and use the default options, taking note of the name you decide for it.</li>
</ol>
<p><img src="/docs/azure/images/creatingWS.PNG"
alt="Creating a Workspace"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
<h3 id="create-an-azure-container-registry">Create an Azure container registry</h3>
<p>Kubeflow uses Docker images to describe each pipeline step&rsquo;s dependencies. You need to create a container registry to store those images in the cloud so that Kubeflow can pull the images as they are needed.</p>
<p>To create a container registry:</p>
<ol>
<li>Go to <a href="https://portal.azure.com">the Azure portal</a> and click on your resource group.</li>
<li>From there, select the <strong>add a new resource</strong> option.</li>
<li>Search for <strong>Container Registry</strong> and add it to your resource group.</li>
<li>Configure your registry by selecting and noting the name you use for it. Enable an <strong>admin user</strong>, and change the SKU option to <strong>Premium</strong>.</li>
</ol>
<p><img src="/docs/azure/images/createContainerReg.PNG"
alt="Creating a Container Registry"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
<h3 id="allow-your-aks-cluster-access-to-your-azure-container-registry">Allow your AKS Cluster access to your Azure Container Registry</h3>
<p>In order for the AKS cluster to have access to pulling images created for execution of the pipeline, you will need to update your cluster so that it is able to pull the images from the container registry we just created. More references can be found <a href="https://docs.microsoft.com/en-us/azure/aks/cluster-container-registry-integration">here</a>, on the Microsoft web site.</p>
<p>Using a bash shell, use the following commands to attach the container registry created above to your AKS cluster, using the proper AKS cluster name and resource group:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">az aks update -n &lt;myAKSCluster&gt; -g &lt;MyResourceGroup&gt; --attach-acr &lt;REGISTRY_NAME&gt;
</span></span></code></pre></div><p>The execution of the command will take a few minutes.</p>
<h3 id="create-a-persistent-volume-claim-pvc">Create a persistent volume claim (PVC)</h3>
<p>A persistent volume claim is a dynamically provisioned storage resource attached to a Kubernetes cluster. It is used in the pipeline to store data and files across pipeline steps.</p>
<p>Using a bash shell, navigate to the <code>azurepipeline</code> directory. Use the following commands to create a persistent volume claim for your cluster.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cd kubernetes
</span></span><span class="line"><span class="cl">kubectl apply -f pvc.yaml
</span></span></code></pre></div><h2 id="authenticate-your-service-principal">Authenticate your service principal</h2>
<p>A service principal is used to allow your pipeline to securely interface with your Azure services without having to directly login in the pipeline and use admin privileges. To create a service principal with Contributor access to your Azure account, use the following steps.</p>
<h3 id="create-an-app-registration">Create an App Registration</h3>
<p>To create an app registration:</p>
<ol>
<li>
<p>In the Azure Portal, navigate to <a href="https://ms.portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview"><strong>Azure Active Directory</strong></a>.</p>
</li>
<li>
<p>Select <strong>App registrations</strong> and click <strong>New registration</strong>. Name it, noting the name and use the default options.</p>
</li>
<li>
<p>Click <strong>Register</strong>. <img src="/docs/azure/images/appReg.PNG"
alt="Creating a App Registration"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
<li>
<p>You should be redirected to your app registration’s dashboard. Select <strong>Overview</strong> from the sidebar.</p>
</li>
<li>
<p>Make note of the <strong>Application (client) ID</strong> and the <strong>Directory (tenant) ID</strong>. The client ID is your service principal username. Save these in a secure location. <img src="/docs/azure/images/clientID2.PNG"
alt="Client ID location"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
<li>
<p>Select <strong>Certificates and Secrets</strong> from the sidebar.</p>
</li>
<li>
<p>Select <strong>New client secret</strong>. Give the client secret a description and select how long you would like it to remain active for. Once you click the <strong>Add</strong> button, make sure you take note of the client secret value and save it in a secure place. This is your service principal password. <img src="/docs/azure/images/password.PNG"
alt="Client secret location"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
</ol>
<h3 id="add-a-role-assignment">Add a role assignment</h3>
<p>To add a role assignment for your service principal:</p>
<ol>
<li>Go to your resource group page on the Azure Portal.</li>
<li>Select <strong>Access control (IAM)</strong> from the sidebar. Select <strong>Add a role assignment</strong>.</li>
<li>Set the role to <strong>Contributor</strong> and search for the name you gave your app registration in the <strong>Select</strong> dropdown.</li>
<li>Click <strong>Save</strong>.</li>
</ol>
<p><img src="/docs/azure/images/roleAssign.PNG"
alt="Creating a Role Assignment"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
<h2 id="creating-containers-from-docker-images">Creating containers from Docker images</h2>
<h3 id="install-docker">Install Docker</h3>
<p>You need to install Docker to be able to push and pull images to/from your Container registry.</p>
<p>For Windows and WSL: <a href="https://nickjanetakis.com/blog/setting-up-docker-for-windows-and-wsl-to-work-flawlessly">Guide</a></p>
<p>For other OS: <a href="https://hub.docker.com/?overlay=onboarding">Docker Desktop</a></p>
<h3 id="build-images">Build images</h3>
<p>To deploy your code to Kubernetes, you must build your local project’s Docker images and push the containers to your Container Registry so that they are available in the cloud.</p>
<ol>
<li>Set the path in Container Registry that you want to push the containers to:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export REGISTRY_PATH=&lt;REGISTRY_NAME&gt;.azurecr.io
</span></span></code></pre></div><ol start="2">
<li>Run the following command to authenticate your Container Registry:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">az acr login --name &lt;REGISTRY_NAME&gt;
</span></span></code></pre></div><ol start="3">
<li>Create a version, to be associated with your model each time it runs (change this accordingly):</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export VERSION_TAG=1
</span></span></code></pre></div><p>Each docker image will be built and uploaded to the cloud using the Container Registry.</p>
<blockquote>
<p>Note: If you would like to test a container locally, you can use the <code>docker run -it ${REGISTRY_PATH}&lt;CONTAINER NAME&gt;:$(VERSION_TAG}</code> before pushing to Container Registry.</p>
</blockquote>
<pre><code>//Starting in the 'code' directory of the azurepipeline folder

cd preprocess
docker build . -t ${REGISTRY_PATH}/preprocess:${VERSION_TAG}
docker push ${REGISTRY_PATH}/preprocess:${VERSION_TAG}

cd ../training
docker build . -t ${REGISTRY_PATH}/training:${VERSION_TAG}
docker push ${REGISTRY_PATH}/training:${VERSION_TAG}

cd ../register
docker build . -t ${REGISTRY_PATH}/register:${VERSION_TAG}
docker push ${REGISTRY_PATH}/register:${VERSION_TAG}

cd ../deploy
docker build . -t ${REGISTRY_PATH}/deploy:${VERSION_TAG}
docker push ${REGISTRY_PATH}/deploy:${VERSION_TAG}
</code></pre>
<p>When all of the images are pushed successfully, modify the <code>pipeline.py</code> file to use the appropriate image for each pipeline step.</p>
<h2 id="running-and-deploying-your-pipeline">Running and deploying your pipeline</h2>
<h3 id="compile">Compile</h3>
<p>To compile the pipeline, simply open a terminal and navigate to the azurepipeline/code folder. Run the following command to generate a pipeline in the tar.gz format:
<code>python pipeline.py</code></p>
<h3 id="run-and-deploy">Run and deploy</h3>
<p>Upload the pipeline.tar.gz file to the pipelines dashboard on your Kubeflow deployment.</p>
<p><img src="/docs/azure/images/pipelinedash.PNG"
alt="Pipeline Dashboard"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
<p>Create an experiment and then create a run using the pipeline you just uploaded.</p>
<p><img src="/docs/azure/images/pipelinesInput.png"
alt="Pipelines input example"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
<p>The finished pipeline should have four completed steps.</p>
<p><img src="/docs/azure/images/finishedRunning.PNG"
alt="Finished Pipeline"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
<h3 id="pushing-images-for-scoring">Pushing images for scoring</h3>
<p>Once your pipeline has finished successfully, you can visit the Azure portal to find your deployment url. Go to your ML workspace dashboard and select “Deployments” from the sidebar. Click on the most recent deployment. You should see a link under “Scoring URI”. You can whatever method you know best to send a GET/POST request with an image of a taco or a burrito to this url and it should return whether or not the image is of a taco or a burrito.</p>
<p>The easiest method is to find a url of an image of a taco or a burrito and append it to your scoring url as follows: <code>&lt;scoring_url&gt;?image=&lt;image_url&gt;</code></p>
<p><img src="/docs/azure/images/finalOutput.PNG"
alt="Final ML model deployment"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
<h2 id="clean-up-your-azure-environment">Clean up your Azure environment</h2>
<p>When you are done, make sure you delete your resource group to avoid extra charges.</p>
<pre><code>az group delete -n MyResourceGroup
</code></pre>
<p>You can optionally choose to delete individual resources on your clusters using the <a href="https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-tutorial-delete-cluster">Azure cluster docs</a>.</p>
<h2 id="next-steps">Next steps</h2>
<p>Build your own pipeline using the <a href="/docs/components/pipelines/sdk/sdk-overview/">Kubeflow Pipelines SDK</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8db8e540d9b02097d039dac1656bed32">2.5 - Access Control for Azure Deployment</h1>
    <div class="lead">Restrict access of your deployment to specified IP addresses</div>
	<p>This section shows how to restrict access to only certain IP addresses for your LoadBalancer Service on Azure. At a later date, it will also include formal authentication through Azure. This method is not the most ideal way to secure your Kubernetes cluster, as it requires that you access the service from the same IP address every time. This process was adapted from <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service">the Kubernetes guide to configuring a firewall</a>.</p>
<p>When using a service with <code>spec.type: LoadBalancer</code>, you can specify the IP ranges that are allowed to access the load balancer by using <code>spec.loadBalancerSourceRanges</code>. This is currently supported on all major cloud providers.</p>
<h2 id="editing-the-loadbalancer-service">Editing the LoadBalancer Service</h2>
<p>Use the <code>kubectl edit svc &lt;loadbalancer-name&gt; -n kubeflow</code> to add your source ranges. This command will open the editor defined by you KUBE_EDITOR or EDITOR environment variables or fall back to &lsquo;vi&rsquo; for Linux or &rsquo;notepad&rsquo; for Windows. More information about using alternative editors and options for this command can be found in <a href="https://www.mankier.com/1/kubectl-edit">the kubectl edit documentation</a>.</p>
<h2 id="internal-subnet-access">Internal Subnet Access</h2>
<p>Assuming 10.0.0.0/8 is the address for the internal subnet, a load balancer will be created such that the deployment is only accessible from internal Kubernetes cluster IPs. This will not allow clients from outside your Kubernetes cluster to access the load balancer.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">kind: Service
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: myapp
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  ports:
</span></span><span class="line"><span class="cl">  - port: 8765
</span></span><span class="line"><span class="cl">    targetPort: 9376
</span></span><span class="line"><span class="cl">  selector:
</span></span><span class="line"><span class="cl">    app: example
</span></span><span class="line"><span class="cl">  type: LoadBalancer
</span></span><span class="line"><span class="cl">  loadBalancerSourceRanges:
</span></span><span class="line"><span class="cl">  - 10.0.0.0/8
</span></span></code></pre></div><h2 id="external-ip-addresses">External IP Addresses</h2>
<p>In the following example, a load balancer will be created that is only accessible to clients with IP addresses from 130.211.204.1 and 130.211.204.2.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">kind: Service
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: myapp
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  ports:
</span></span><span class="line"><span class="cl">  - port: 8765
</span></span><span class="line"><span class="cl">    targetPort: 9376
</span></span><span class="line"><span class="cl">  selector:
</span></span><span class="line"><span class="cl">    app: example
</span></span><span class="line"><span class="cl">  type: LoadBalancer
</span></span><span class="line"><span class="cl">  loadBalancerSourceRanges:
</span></span><span class="line"><span class="cl">  - 130.211.204.1/32
</span></span><span class="line"><span class="cl">  - 130.211.204.2/32
</span></span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ef86fdffe53ad5bdd20b56381dd21142">2.6 - Configure Azure MySQL database to store metadata</h1>
    <div class="lead">How to configure an external Azure Database for MySQL with kustomize to store metadata</div>
	<p>This section shows how to use Kubeflow kustomize to configure an external Azure
MySQL database to store metadata.</p>
<p>Microsoft <a href="https://docs.microsoft.com/en-us/azure/mysql">Azure Database for
MySQL</a> is a relational database
service based on <a href="https://www.mysql.com/products/community/">MySQL</a>. It provides
built-in high availability, data protection using automatic backups and
point-in-time-restore for up to 35 days, and automated maintenance for
underlying hardware, operating system and database engine to keep the service
secure and up to date. <a href="https://docs.microsoft.com/en-us/azure/mysql/overview">Learn more about Azure Database for
MySQL</a>.</p>
<p>Table of contents:</p>
<ol>
<li>Create an Azure database for MySQL</li>
<li>Deploy Kubeflow to use the Azure metadata overlay</li>
<li>Update Kubeflow resources</li>
</ol>
<h1 id="create-an-azure-database-for-mysql">Create an Azure database for MySQL</h1>
<p>First, you need to create an Azure database for MySQL on Azure through either Azure
Portal or using Azure CLI:</p>
<ul>
<li>
<p><strong>Azure Portal</strong>: follow the
<a href="https://docs.microsoft.com/en-us/azure/mysql/quickstart-create-mysql-server-database-using-azure-portal">quickstart</a>
in the Azure documentation.</p>
</li>
<li>
<p><strong>Azure CLI</strong>: follow the
<a href="https://docs.microsoft.com/en-us/azure/mysql/quickstart-create-mysql-server-database-using-azure-cli">quickstart</a>.</p>
</li>
</ul>
<p>Remember your <code>Server Name</code>, <code>Admin username</code>, and <code>Password</code> - you&rsquo;ll be using
them later in this guide.</p>


<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Warning</h4>

    By default, the created server is protected with a firewall and is not
accessible publicly. You can refer to the section on <a href="https://docs.microsoft.com/en-us/azure/mysql/quickstart-create-mysql-server-database-using-azure-portal#configure-a-server-level-firewall-rule">configuring a server-level
firewall
rule</a>
in the official documentation to allow your database to be accessible from
external IP addresses. Depending on your configuration, you may also be able to
enable all IP addresses and disable <code>Enforce SSL connection</code>.

</div>

<h1 id="deploy-kubeflow-to-use-the-azure-metadata-overlay">Deploy Kubeflow to use the Azure metadata overlay</h1>
<p>You have created the MySQL server database. Next, you need to deploy Kubeflow
to use the Azure metadata overlay.</p>
<ol>
<li>
<p>Follow the <a href="https://www.kubeflow.org/docs/azure/deploy/install-kubeflow/">Install
Kubeflow</a> on
AKS (Azure Kubernetes Service) guide until the step where you have to build
and apply the <code>CONFIG_URI</code>.</p>
</li>
<li>
<p>If you followed the previous step, you should have downloaded the
configuration file. Next, you need to customize the configuration before
deploying Kubeflow. Run the following command:</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">wget -O kfctl_azure.yaml <span class="si">${</span><span class="nv">CONFIG_URI</span><span class="si">}</span>
</span></span></code></pre></div><p>where the <code>${CONFIG_URL}</code> is the URL pointing to Kubeflow manifest for Azure
(for example,
<code>.../kubeflow/manifests/v1.1-branch/kfdef/kfctl_azure.v1.1.0.yaml</code>) that you
specified in step 1.</p>
<ol start="3">
<li>Before deploying Kubeflow, use <code>kfctl build</code> to create configuration files:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kfctl build -V -f kfctl_azure.yaml
</span></span></code></pre></div><ol start="4">
<li>Under <code>/stacks/azure</code> and <code>resources</code> replace <code>- ../../metadata/v3</code> with
<code>metadata</code> to  enable using Azure MySQL.</li>
</ol>
<p>The updated <code>kustomization.yaml</code> in <code>stacks/azure</code> should look similar to this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">  # Metadata
</span></span><span class="line"><span class="cl">  # - ../../metadata/v3
</span></span><span class="line"><span class="cl">  # Uncomment the line below if you want to use Azure MySQL
</span></span><span class="line"><span class="cl">  - ./metadata
</span></span></code></pre></div><ol start="5">
<li>Edit <code>params.env</code> to provide parameters to configuration map as follows:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">MYSQL_HOST={YOUR_DB_SERVER_NAME}.mysql.database.azure.com
</span></span><span class="line"><span class="cl">MYSQL_DATABASE=mlmetadata
</span></span><span class="line"><span class="cl">MYSQL_PORT=3306
</span></span><span class="line"><span class="cl">MYSQL_ALLOW_EMPTY_PASSWORD=true
</span></span></code></pre></div><p>where <code>{YOUR_DB_SERVER_NAME}</code> is your server name.</p>
<ol start="6">
<li>Edit <code>secrets.env</code> to create a secret with the admin username and password
based on your database configuration. Make sure the user name follows the
pattern with an &ldquo;@&rdquo;, like the one showed below:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">MYSQL_USERNAME={ADMIN_USERNAME}@{YOUR_DB_SERVER_NAME}
</span></span><span class="line"><span class="cl">MYSQL_PASSWORD={ADMIN_PASSWORD}
</span></span></code></pre></div><h1 id="deploy-kubeflow">Deploy Kubeflow</h1>
<p>Having completed the previous steps to set up the MySQL server, you can deploy
Kubeflow:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kfctl apply -V -f kfctl_azure.yaml
</span></span></code></pre></div><p>Your metadata database should now be using the Azure Database for MySQL.</p>
<p>To configure the pipeline database using an external Azure Database for MySQL,
go to <a href="https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/manifests/kustomize/env/azure">KFP customizations for
Azure</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f40c9e23375af11c53e0a087013f0bf9">2.7 - Troubleshooting Deployments on Azure AKS</h1>
    <div class="lead">Help diagnose and fix issues you may encounter in your Kubeflow deployment</div>
	<h3 id="jupyter-notebook-is-not-a-valid-page-when-accessing-notebook">Jupyter Notebook ‘is not a valid page’ when accessing notebook</h3>
<p>Restarting the ambassador pods will often fix this issue:
<code>kubectl delete pods -l service=ambassador</code></p>
<h3 id="the-client-does-not-have-authorization-to-perform-action">The client does not have authorization to perform action&hellip;</h3>
<p>This is likely an issue with ensuring your subscriptions are correctly setup. To fix the issue, list your subscriptions and then change the active subscription.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">#list subscription 
</span></span><span class="line"><span class="cl">az account list --output table 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">#change the active subscription 
</span></span><span class="line"><span class="cl">az account set --subscription &#34;My Demos&#34;
</span></span></code></pre></div>
</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-dd001bce04ba89733d9fb27c3064e6d8">3 - Kubeflow on Google Cloud</h1>
    <div class="lead">Running Kubeflow on Kubernetes Engine and Google Cloud Platform</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-0596cc9dd7074b5fe9d6610a9a4e4e6c">3.1 - Deployment</h1>
    <div class="lead">Instructions for deploying Kubeflow on Google Cloud</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-84248b9ed230aeb39f1ad762d9ae0433">3.1.1 - Overview</h1>
    <div class="lead">Full fledged Kubeflow deployment on Google Cloud</div>
	<p>This guide describes how to deploy Kubeflow and a series of Kubeflow components on GKE (Google Kubernetes Engine).
If you want to use Kubeflow Pipelines only, refer to <a href="/docs/components/pipelines/installation/overview/">Installation Options for Kubeflow Pipelines</a>
for choosing an installation option.</p>
<h2 id="deployment-structure">Deployment Structure</h2>
<p>As a high level overview, you need to create one Management cluster which allows you to manage Google Cloud resources via <a href="https://cloud.google.com/config-connector/docs/overview">Config Connector</a>. Management cluster can create, manage and delete multiple Kubeflow clusters, while being independent from Kubeflow clusters&rsquo; activities. Below is a simplified view of deployment structure. Note that Management cluster can live in a different Google Cloud project from Kubeflow clusters, admin should assign owner permission to Management cluster&rsquo;s service account. It will be explained in detail during Deployment steps.</p>
<p><img src="/docs/images/gke/full-deployment-structure.png" 
alt="Full Kubeflow deployment structure"
class="mt-3 mb-3 border border-info rounded"></p>
<h2 id="deployment-steps">Deployment steps</h2>
<p>Follow the steps below to set up Kubeflow environment on Google Cloud. Some of these steps are one-time only, for example: OAuth Client can be shared by multiple Kubeflow clusters in the same Google Cloud project.</p>
<ol>
<li><a href="/docs/distributions/gke/deploy/project-setup/">Set up Google Cloud project</a>.</li>
<li><a href="/docs/distributions/gke/deploy/oauth-setup/">Set up OAuth Client</a>.</li>
<li><a href="/docs/distributions/gke/deploy/management-setup/">Deploy Management Cluster</a>.</li>
<li><a href="/docs/distributions/gke/deploy/deploy-cli/">Deploy Kubeflow Cluster</a>.</li>
</ol>
<p>If you encounter any issue during the deployment steps, refer to <a href="/docs/distributions/gke/troubleshooting-gke/">Troubleshooting deployments on GKE</a> to find common issues
and debugging approaches. If this issue is new, file a bug to <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution">GoogleCloudPlatform/kubeflow-distribution</a> for GKE related issue, or file a bug to the corresponding component in <a href="https://github.com/kubeflow/">Kubeflow on GitHub</a> if the issue is component specific.</p>
<h2 id="features">Features</h2>
<p>Once you finish deployment, you will be able to:</p>
<ol>
<li>manage a running Kubernetes cluster with multiple Kubeflow components installed.</li>
<li>get a <a href="https://cloud.google.com/endpoints/docs">Cloud Endpoint</a> which is accessible via <a href="https://cloud.google.com/iap">IAP (Identity-aware Proxy)</a>.</li>
<li>enable <a href="/docs/components/multi-tenancy/">Multi-user feature</a> for resource and access isolation.</li>
<li>take advantage of GKE&rsquo;s
<a href="https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler">Cluster Autoscaler</a>
to automatically resize the number of nodes in a node pool.</li>
<li>choose GPUs and <a href="https://cloud.google.com/tpu/">Cloud TPU</a> to accelerate your workload.</li>
<li>use <a href="https://cloud.google.com/logging/docs/">Cloud Logging</a> to help debugging and troubleshooting.</li>
<li>access to many managed services offered by Google Cloud.</li>
</ol>
<p><img src="/docs/images/gke/full-kf-home.png" 
alt="Full Kubeflow Central Dashboard"
class="mt-3 mb-3 border border-info rounded"></p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>Repeat <a href="/docs/distributions/gke/deploy/deploy-cli/">Deploy Kubeflow Cluster</a> if you want to deploy multiple clusters.</li>
<li>Run a full ML workflow on Kubeflow, using the <a href="https://github.com/kubeflow/pipelines/blob/e42d9d2609369b96973c821dca11fe5b2565e705/samples/contrib/kubeflow-e2e-mnist/kubeflow-e2e-mnist.ipynb">end-to-end MNIST tutorial</a>.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2276d0fa53a9f77a5aadb482040fbab4">3.1.2 - Set up Project</h1>
    <div class="lead">Creating a Google Cloud project for your Kubeflow deployment</div>
	<p>In order to create GKE cluster and deploy Kubeflow on it, you need to set up a Google Cloud project
and enable necessary APIs for the deployment.</p>
<h2 id="set-up-project-and-api-scopes">Set up project and API scopes</h2>
<p>Follow these steps to set up your Google Cloud project:</p>
<ul>
<li>
<p>Select or create a project on the
<a href="https://console.cloud.google.com/cloud-resource-manager">Google Cloud Console</a>. If you plan to use different Google Cloud projects for <strong>Management Cluster</strong> and <strong>Kubeflow Clusters</strong>: create <strong>one Management project</strong> for Management Cluster, and create <strong>one or more Kubeflow projects</strong> for Kubeflow Clusters.</p>
</li>
<li>
<p>Make sure that you have the
<a href="https://cloud.google.com/iam/docs/understanding-roles#primitive_role_definitions">Owner role</a>
for the project in Cloud IAM (Identity and Access Management).
The deployment process creates various service accounts with
appropriate roles in order to enable seamless integration with
Google Cloud services. This process requires that you have the
owner role for the project in order to deploy Kubeflow.</p>
</li>
<li>
<p>Make sure that billing is enabled for your project. Refer to
<a href="https://cloud.google.com/billing/docs/how-to/modify-project">Enable billing for a project</a>.</p>
</li>
<li>
<p>Open following pages on the Google Cloud Console and ensure that the
specified APIs are enabled for all projects:</p>
<ul>
<li><a href="https://cloud.google.com/service-usage/docs/reference/rest">Service Usage API</a></li>
<li><a href="https://console.cloud.google.com/apis/library/compute.googleapis.com">Compute Engine API</a></li>
<li><a href="https://console.cloud.google.com/apis/library/container.googleapis.com">Kubernetes Engine API</a></li>
<li><a href="https://console.cloud.google.com/apis/library/iam.googleapis.com">Identity and Access Management (IAM) API</a></li>
<li><a href="https://console.cloud.google.com/apis/api/servicemanagement.googleapis.com">Service Management API</a></li>
<li><a href="https://console.developers.google.com/apis/library/cloudresourcemanager.googleapis.com">Cloud Resource Manager API</a></li>
<li><a href="https://console.developers.google.com/apis/library/ml.googleapis.com">AI Platform Training &amp; Prediction API</a></li>
<li><a href="https://console.cloud.google.com/apis/library/iap.googleapis.com">Cloud Identity-Aware Proxy API</a></li>
<li><a href="https://console.cloud.google.com/apis/library/cloudbuild.googleapis.com">Cloud Build API</a> (It&rsquo;s required if you plan to use <a href="https://www.kubeflow.org/docs/external-add-ons/fairing/">Fairing</a> in your Kubeflow cluster)</li>
<li><a href="https://console.cloud.google.com/apis/library/sqladmin.googleapis.com">Cloud SQL Admin API</a></li>
<li><a href="https://console.cloud.google.com/apis/library/krmapihosting.googleapis.com">Config Controller (KRM API Hosting API)</a></li>
<li><a href="https://console.cloud.google.com/apis/library/servicecontrol.googleapis.com">Service Control API</a></li>
<li><a href="https://console.cloud.google.com/apis/library/endpoints.googleapis.com">Google Cloud Endpoints</a></li>
</ul>
<p>You can also enable these APIs by running the following command in Cloud Shell:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud services <span class="nb">enable</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  serviceusage.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  compute.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  container.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  iam.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  servicemanagement.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  cloudresourcemanager.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  ml.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  iap.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  sqladmin.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  meshconfig.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  krmapihosting.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  servicecontrol.googleapis.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  endpoints.googleapis.com
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Cloud Build API is optional, you need it if using Fairing.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># gcloud services enable cloudbuild.googleapis.com</span>
</span></span></code></pre></div></li>
<li>
<p>If you are using the
<a href="https://cloud.google.com/free/docs/gcp-free-tier">Google Cloud Free Program</a> or the
12-month trial period with $300 credit, note that the free tier does not offer enough
resources for default full Kubeflow installation. You need to
<a href="https://cloud.google.com/free/docs/gcp-free-tier#how-to-upgrade">upgrade to a paid account</a>.</p>
<p>For more information, see the following issues:</p>
<ul>
<li><a href="https://github.com/kubeflow/website/issues/1065">kubeflow/website #1065</a>
reports the problem.</li>
<li><a href="https://github.com/kubeflow/kubeflow/issues/3936">kubeflow/kubeflow #3936</a>
requests a Kubeflow configuration to work with a free trial project.</li>
</ul>
<p>Read the Google Cloud <a href="https://cloud.google.com/compute/quotas">Resource quotas</a>
to understand quotas on resource usage that Compute Engine enforces, and
to learn how to check and increase your quotas.</p>
</li>
<li>
<p>Initialize your project to prepare it for Anthos Service Mesh installation:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">PROJECT_ID</span><span class="o">=</span>&lt;YOUR_PROJECT_ID&gt;
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl --request POST <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --header <span class="s2">&#34;Authorization: Bearer </span><span class="k">$(</span>gcloud auth print-access-token<span class="k">)</span><span class="s2">&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --data <span class="s1">&#39;&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  https://meshconfig.googleapis.com/v1alpha1/projects/<span class="si">${</span><span class="nv">PROJECT_ID</span><span class="si">}</span>:initialize
</span></span></code></pre></div><p>Refer to <a href="https://cloud.google.com/service-mesh/docs/archive/1.4/docs/gke-install-new-cluster#setting_credentials_and_permissions">Anthos Service Mesh documentation</a> for details.</p>
<p>If you encounter a <code>Workload Identity Pool does not exist</code> error, refer to the following issue:</p>
<ul>
<li><a href="https://github.com/kubeflow/website/issues/2121">kubeflow/website #2121</a>
describes that creating and then removing a temporary Kubernetes cluster may
be needed for projects that haven&rsquo;t had a cluster set up beforehand.</li>
</ul>
</li>
</ul>
<p>You do not need a running GKE cluster. The deployment process creates a
cluster for you.</p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li><a href="/docs/distributions/gke/deploy/oauth-setup">Set up an OAuth credential</a> to use
<a href="https://cloud.google.com/iap/docs/">Cloud Identity-Aware Proxy (Cloud IAP)</a>.
Cloud IAP is recommended for production deployments or deployments with access
to sensitive data.</li>
<li><a href="/docs/distributions/gke/deploy/management-setup">Set up Management Cluster</a> to deploy and manage Kubeflow clusters.</li>
<li><a href="/docs/distributions/gke/deploy/deploy-cli">Deploy Kubeflow</a> using kubectl, kustomize and kpt.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-698b42ce1b2b84b41dc49341fef9cec4">3.1.3 - Set up OAuth client</h1>
    <div class="lead">Creating an OAuth client for Cloud IAP on Google Cloud</div>
	<h2 id="set-up-oauth-consent-screen-and-client-credential">Set up OAuth Consent Screen and Client Credential</h2>
<p>If you want to use
<a href="https://cloud.google.com/iap/docs/">Cloud Identity-Aware Proxy (Cloud IAP)</a>
when deploying Kubeflow on Google Cloud,
then you must follow these instructions to create an OAuth client for use
with Kubeflow.</p>
<p>Cloud IAP is recommended for production deployments or deployments with access
to sensitive data.</p>
<p>Follow the steps below to create an OAuth client ID that identifies Cloud IAP
when requesting access to a user&rsquo;s email account. Kubeflow uses the email
address to verify the user&rsquo;s identity.</p>
<ol>
<li>
<p>Set up your OAuth <a href="https://console.cloud.google.com/apis/credentials/consent">consent screen</a>:</p>
<ul>
<li>
<p>In the <strong>Application name</strong> box, enter the name of your application.
The example below uses the name &ldquo;Kubeflow&rdquo;.</p>
</li>
<li>
<p>Under <strong>Support email</strong>, select the email address that you want to display
as a public contact. You must use either your email address or a Google
Group that you own.</p>
</li>
<li>
<p>If you see <strong>Authorized domains</strong>, enter</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&lt;project&gt;.cloud.goog
</span></span></code></pre></div><ul>
<li>where &lt;project&gt; is your Google Cloud project ID.</li>
<li>If you are using your own domain, such as <strong>acme.com</strong>, you should add
that as well</li>
<li>The <strong>Authorized domains</strong> option appears only for certain project
configurations. If you don&rsquo;t see the option, then there&rsquo;s nothing you
need to set.</li>
</ul>
</li>
<li>
<p>Click <strong>Save</strong>.</p>
</li>
<li>
<p>Here&rsquo;s an example of the completed form:<br>
<img src="/docs/images/consent-screen.png" 
alt="OAuth consent screen"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
</ul>
</li>
<li>
<p>On the <a href="https://console.cloud.google.com/apis/credentials">credentials screen</a>:</p>
<ul>
<li>Click <strong>Create credentials</strong>, and then click <strong>OAuth client ID</strong>.</li>
<li>Under <strong>Application type</strong>, select <strong>Web application</strong>.</li>
<li>In the <strong>Name</strong> box enter any name for your OAuth client ID. This is <em>not</em>
the name of your application nor the name of your Kubeflow deployment. It&rsquo;s
just a way to help you identify the OAuth client ID.</li>
</ul>
</li>
<li>
<p>Click <strong>Create</strong>. A dialog box appears, like the one below:</p>
<p><img src="/docs/images/new-oauth.png" 
alt="OAuth consent screen"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
<li>
<p>Copy the <strong>client ID</strong> shown in the dialog box, because you need the client
ID in the next step.</p>
</li>
<li>
<p>On the <strong>Create credentials</strong> screen, find your newly created OAuth
credential and click the pencil icon to edit it:</p>
<p><img src="/docs/images/oauth-edit.png" 
alt="OAuth consent screen"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
<li>
<p>In the <strong>Authorized redirect URIs</strong> box, enter the following (if it&rsquo;s not
already present in the list of authorized redirect URIs):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://iap.googleapis.com/v1/oauth/clientIds/&lt;CLIENT_ID&gt;:handleRedirect
</span></span></code></pre></div><ul>
<li><code>&lt;CLIENT_ID&gt;</code> is the OAuth client ID that you copied from the dialog box in
step four. It looks like <code>XXX.apps.googleusercontent.com</code>.</li>
<li>Note that the URI is not dependent on the Kubeflow deployment or endpoint.
Multiple Kubeflow deployments can share the same OAuth client without the
need to modify the redirect URIs.</li>
</ul>
</li>
<li>
<p>Press <strong>Enter/Return</strong> to add the URI. Check that the URI now appears as
a confirmed item under <strong>Authorized redirect URIs</strong>. (The URI should no longer be
editable.)</p>
<p>Here&rsquo;s an example of the completed form:
<img src="/docs/images/oauth-credential.png" 
alt="OAuth credentials"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
</li>
<li>
<p>Click <strong>Save</strong>.</p>
</li>
<li>
<p>Make note that you can find your OAuth client credentials in the credentials
section of the Google Cloud Console. You need to retrieve the <strong>client ID</strong> and
<strong>client secret</strong> later when you&rsquo;re ready to enable Cloud IAP.</p>
</li>
</ol>
<h2 id="next-steps">Next steps</h2>
<ul>
<li><a href="/docs/distributions/gke/deploy/management-setup/">Set up your management cluster</a>.</li>
<li><a href="https://cloud.google.com/iam/docs/granting-changing-revoking-access#granting-console">Grant your users the IAP-secured Web App User IAM role</a> so they can access the Kubeflow console through IAP.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c3209c70c33cf5aaf9eddafee3d5e0b0">3.1.4 - Deploy Management cluster</h1>
    <div class="lead">Setting up a management cluster on Google Cloud</div>
	<p>This guide describes how to setup a management cluster which you will use to deploy one or more instances of Kubeflow.</p>
<p>The management cluster is used to run <a href="https://cloud.google.com/config-connector/docs/overview">Cloud Config Connector</a>. Cloud Config Connector is a Kubernetes addon that allows you to manage Google Cloud resources through Kubernetes.</p>
<p>While the management cluster can be deployed in the same project as your Kubeflow cluster, typically you will want to deploy
it in a separate project used for administering one or more Kubeflow instances, because it will run with escalated permissions to create Google Cloud resources in the managed projects.</p>
<p>Optionally, the cluster can be configured with <a href="https://cloud.google.com/anthos-config-management/docs">Anthos Config Management</a>
to manage Google Cloud infrastructure using GitOps.</p>
<h2 id="deployment-steps">Deployment steps</h2>
<h3 id="install-the-required-tools">Install the required tools</h3>
<ol>
<li>
<p><a href="https://cloud.google.com/sdk/docs/components">gcloud components</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud components install kubectl kustomize kpt anthoscli beta
</span></span><span class="line"><span class="cl">gcloud components update
</span></span><span class="line"><span class="cl"><span class="c1"># If the output said the Cloud SDK component manager is disabled for installation, copy the command from output and run it.</span>
</span></span></code></pre></div><p>You can install specific version of kubectl by following instruction (Example: <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/">Install kubectl on Linux</a>). Latest patch version of kubectl from <code>v1.17</code> to <code>v1.19</code> works well too.</p>
<p>Note: Starting from Kubeflow 1.4, it requires <code>kpt v1.0.0-beta.6</code> or above to operate in <code>GoogleCloudPlatform/kubeflow-distribution</code> repository. gcloud hasn&rsquo;t caught up with this kpt version yet, <a href="https://kpt.dev/installation/">install kpt</a> separately from <a href="https://github.com/GoogleContainerTools/kpt/tags">https://github.com/GoogleContainerTools/kpt/tags</a> for now. Note that kpt requires docker to be installed.</p>
</li>
</ol>
<h3 id="fetch-googlecloudplatformkubeflow-distribution-package">Fetch GoogleCloudPlatform/kubeflow-distribution package</h3>
<p>The management cluster manifests live in GitHub repository <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/">GoogleCloudPlatform/kubeflow-distribution</a>, use the following commands to pull Kubeflow manifests:</p>
<ol>
<li>
<p>Clone the GitHub repository and check out the v1.6.1 tag:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git clone https://github.com/GoogleCloudPlatform/kubeflow-distribution.git 
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kubeflow-distribution
</span></span><span class="line"><span class="cl">git checkout tags/v1.6.1 -b v1.6.1
</span></span></code></pre></div><p>Alternatively, you can get the package by using <code>kpt</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Check out Kubeflow v1.6.1 blueprints</span>
</span></span><span class="line"><span class="cl">kpt pkg get https://github.com/GoogleCloudPlatform/kubeflow-distribution.git@v1.6.1 kubeflow-distribution
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kubeflow-distribution
</span></span></code></pre></div></li>
<li>
<p>Go to <code>kubeflow-distribution/management</code> directory for Management cluster configurations.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> management
</span></span></code></pre></div></li>
</ol>


<div class="alert alert-info" role="alert">
<h4 class="alert-heading">Tip</h4>

    To continuously manage the management cluster, you are recommended to check
the management configuration directory into source control. For example, <code>MGMT_DIR=~/kubeflow-distribution/management/</code>.

</div>

<h3 id="configure-environment-variables">Configure Environment Variables</h3>
<p>Fill in environment variables in <code>kubeflow-distribution/management/env.sh</code> as followed:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">MGMT_PROJECT</span><span class="o">=</span>&lt;the project where you deploy your management cluster&gt;
</span></span><span class="line"><span class="cl"><span class="nv">MGMT_NAME</span><span class="o">=</span>&lt;name of your management cluster&gt;
</span></span><span class="line"><span class="cl"><span class="nv">LOCATION</span><span class="o">=</span>&lt;location of your management cluster, use either us-central1 or us-east1&gt;
</span></span></code></pre></div><p>And run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">source</span> env.sh
</span></span></code></pre></div><p>This guide assumes the following convention:</p>
<ul>
<li>
<p>The <code>${MGMT_PROJECT}</code> environment variable contains the Google Cloud project
ID where management cluster is deployed to.</p>
</li>
<li>
<p><code>${MGMT_NAME}</code> is the cluster name of your management cluster and the prefix for other Google Cloud resources created in the deployment process. Management cluster
should be a different cluster from your Kubeflow cluster.</p>
<p>Note, <code>${MGMT_NAME}</code> should</p>
<ul>
<li>start with a lowercase letter</li>
<li>only contain lowercase letters, numbers and <code>-</code></li>
<li>end with a number or a letter</li>
<li>contain no more than 18 characters</li>
</ul>
</li>
<li>
<p>The <code>${LOCATION}</code> environment variable contains the location of your management cluster.
you can choose between regional or zonal, see <a href="https://cloud.google.com/compute/docs/regions-zones#available">Available regions and zones</a>.</p>
</li>
</ul>
<h3 id="configure-kpt-setter-values">Configure kpt setter values</h3>
<p>Use kpt to <a href="https://catalog.kpt.dev/apply-setters/v0.2/">set values</a> for the name, project, and location of your management cluster. Run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">bash kpt-set.sh
</span></span></code></pre></div><p>Note, you can find out which setters exist in a package and what their
current values are by running the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kpt fn <span class="nb">eval</span> -i list-setters:v0.1 ./manifests
</span></span></code></pre></div><h3 id="prerequisite-for-config-controller">Prerequisite for Config Controller</h3>
<p>In order to deploy Google Cloud Services like Kubernetes resources, we need to create a management cluster with Config Controller installed. Follow <a href="https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-setup#before_you_begin">Before you begin</a> to create default network if not existed. Make sure to use <code>${MGMT_PROJECT}</code> for PROJECT_ID.</p>
<h3 id="deploy-management-cluster">Deploy Management Cluster</h3>
<ol>
<li>
<p>Deploy the management cluster by applying cluster resources:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make create-cluster
</span></span></code></pre></div></li>
<li>
<p>Create a kubectl <strong>context</strong> for the management cluster, it will be named <code>${MGMT_NAME}</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make create-context
</span></span></code></pre></div></li>
<li>
<p>Grant permission to Config Controller service account:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make grant-owner-permission
</span></span></code></pre></div><p>Config Controller has created a default service account, this step grants owner permission to this service account in order to
allow Config Controller to manage Google Cloud resources. Refer to <a href="https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-setup#set_up">Config Controller setup</a>.</p>
</li>
</ol>
<h2 id="understanding-the-deployment-process">Understanding the deployment process</h2>
<p>This section gives you more details about the configuration and
deployment process, so that you can customize your management cluster if necessary.</p>
<h3 id="config-controller">Config Controller</h3>
<p>Management cluster is a tool for managing Google Cloud services like KRM, for example: GKE container cluster, MySQL database, etc.
And you can use one Managment cluster for multiple Kubeflow clusters, across multiple Google Cloud projects.
This capability is offered by <a href="https://cloud.google.com/config-connector/docs/how-to/getting-started">Config Connector</a>.</p>
<p>Starting with Kubeflow 1.5, we leveraged the managed version of Config Connector, which is called <a href="https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview">Config Controller</a>.
Therefore, The Management cluster is the Config Controller cluster deployed using <a href="https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-setup">Config Controller setup</a> process.
Note that you can create only one Management cluster within a Google Cloud project, and you usually need just one Management cluster.</p>
<h3 id="management-cluster-layout">Management cluster layout</h3>
<p>Inside the Config Controller, we manange Google Cloud resources in namespace mode. That means one namespace is responsible to manage Google Cloud resources deployed to the Google Cloud project with the same name. Your management cluster contains following namespaces:</p>
<ol>
<li>config-control</li>
<li>namespace with the same name as your Kubeflow clusters&rsquo; Google Cloud project name</li>
</ol>
<p><code>config-control</code> is the default namespace which is installed while creating Management cluster, you have granted the default service account (like <code>service-&lt;management-project-id&gt;@gcp-sa-yakima.iam.gserviceaccount.com</code>)
within this project to manage Config Connector. It is the prerequisite for managing resources in other Google Cloud projects.</p>
<p><code>namespace with the same name as your Kubeflow clusters' Google Cloud project name</code> is the resource pool for Kubeflow cluster&rsquo;s Google Cloud project.
For each Kubeflow Google Cloud project, you will have service account with pattern <code>kcc-&lt;kf-project-name&gt;@&lt;management-project-name&gt;.iam.gserviceaccount.com</code> in <code>config-control</code> namespace, and it needs to have owner permission to <code>${KF_PROJECT}</code>, you will perform this step during <a href="/docs/gke/deploy/deploy-cli/">Deploy Kubeflow cluster</a>. After setup, your Google Cloud resources in Kubeflow cluster project will be deployed to the namespace with name <code>${KF_PROJECT}</code> in the management cluster.</p>
<p>Your management cluster directory contains the following file:</p>
<ul>
<li><strong>Makefile</strong> is a file that defines rules to automate deployment process. You can refer to <a href="https://www.gnu.org/software/make/manual/make.html#Introduction">GNU make documentation</a> for more introduction. The Makefile we provide is designed to be user maintainable. You are encouraged to read, edit and maintain it to suit your own deployment customization needs.</li>
</ul>
<h3 id="debug">Debug</h3>
<p>If you encounter issue creating Google Cloud resources using Config Controller. You can list resources in the <code>${KF_PROJECT}</code> namespace of management cluster to learn about the detail.
Learn more with <a href="https://cloud.google.com/config-connector/docs/how-to/monitoring-your-resources">Monitoring your resources</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span> get all -n <span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># If you want to check the service account creation status</span>
</span></span><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span> get IAMServiceAccount -n <span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span> get IAMServiceAccount &lt;service-account-name&gt; -n <span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span> -oyaml
</span></span></code></pre></div><h3 id="faqs">FAQs</h3>
<ul>
<li>
<p>Where is <code>kfctl</code>?</p>
<p><code>kfctl</code> is no longer being used to apply resources for Google Cloud, because required functionalities are now supported by generic tools including <a href="https://www.gnu.org/software/make/">Make</a>, <a href="https://kustomize.io">Kustomize</a>, <a href="https://googlecontainertools.github.io/kpt/">kpt</a>, and <a href="https://cloud.google.com/config-connector/docs/overview">Cloud Config Connector</a>.</p>
</li>
<li>
<p>Why do we use an extra management cluster to manage Google Cloud resources?</p>
<p>The management cluster is very lightweight cluster that runs <a href="https://cloud.google.com/config-connector/docs/overview">Cloud Config Connector</a>. Cloud Config Connector makes it easier to configure Google Cloud resources using YAML and Kustomize.</p>
</li>
</ul>
<p>For a more detailed explanation of the drastic changes happened in Kubeflow v1.1 on Google Cloud, read <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/issues/123">GoogleCloudPlatform/kubeflow-distribution #123</a>.</p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li><a href="/docs/distributions/gke/deploy/deploy-cli">Deploy Kubeflow</a> using kubectl, kustomize and kpt.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e5d8863c4a5c353e271efdad4aa70269">3.1.5 - Deploy Kubeflow cluster</h1>
    <div class="lead">Instructions for using kubectl and kpt to deploy Kubeflow on Google Cloud</div>
	<p>This guide describes how to use <code>kubectl</code> and <a href="https://googlecontainertools.github.io/kpt/">kpt</a> to
deploy Kubeflow on Google Cloud.</p>
<h2 id="deployment-steps">Deployment steps</h2>
<h3 id="prerequisites">Prerequisites</h3>
<p>Before installing Kubeflow on the command line:</p>
<ol>
<li>
<p>You must have created a management cluster and installed Config Connector.</p>
<ul>
<li>
<p>If you don&rsquo;t have a management cluster follow the <a href="/docs/distributions/gke/deploy/management-setup/">instructions</a></p>
</li>
<li>
<p>Your management cluster will need a namespace setup to administer the Google Cloud project where Kubeflow will be deployed. This step will be included in later step of current page.</p>
</li>
</ul>
</li>
<li>
<p>You need to use Linux or <a href="https://cloud.google.com/shell/">Cloud Shell</a> for ASM installation. Currently ASM installation doesn&rsquo;t work on macOS because it <a href="https://cloud.google.com/service-mesh/docs/scripted-install/asm-onboarding#installing_required_tools">comes with an old version of bash</a>.</p>
</li>
<li>
<p>Make sure that your Google Cloud project meets the minimum requirements
described in the <a href="/docs/distributions/gke/deploy/project-setup/">project setup guide</a>.</p>
</li>
<li>
<p>Follow the guide
<a href="/docs/distributions/gke/deploy/oauth-setup/">setting up OAuth credentials</a>
to create OAuth credentials for <a href="https://cloud.google.com/iap/docs/">Cloud Identity-Aware Proxy (Cloud
IAP)</a>.</p>
<ul>
<li>Unfortunately <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/backendconfig">GKE&rsquo;s BackendConfig</a>
currently doesn&rsquo;t support creating <a href="https://cloud.google.com/iap/docs/programmatic-oauth-clients">IAP OAuth clients programmatically</a>.</li>
</ul>
</li>
</ol>
<h3 id="install-the-required-tools">Install the required tools</h3>
<ol>
<li>
<p>Install <a href="https://cloud.google.com/sdk/">gcloud</a>.</p>
</li>
<li>
<p>Install gcloud components</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud components install kubectl kustomize kpt anthoscli beta
</span></span><span class="line"><span class="cl">gcloud components update
</span></span></code></pre></div><p>You can install specific version of kubectl by following instruction (Example: <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/">Install kubectl on Linux</a>). Latest patch version of kubectl from <code>v1.17</code> to <code>v1.19</code> works well too.</p>
<p>Note: Starting from Kubeflow 1.4, it requires <code>kpt v1.0.0-beta.6</code> or above to operate in <code>GoogleCloudPlatform/kubeflow-distribution</code> repository. gcloud hasn&rsquo;t caught up with this kpt version yet, <a href="https://kpt.dev/installation/">install kpt</a> separately from <a href="https://github.com/GoogleContainerTools/kpt/tags">https://github.com/GoogleContainerTools/kpt/tags</a> for now. Note that kpt requires docker to be installed.</p>
<p>Note: You also need to <a href="https://cloud.google.com/service-mesh/v1.10/docs/scripted-install/asm-onboarding#installing_required_tools">install required tools</a> for ASM installation tool <code>install_asm</code>.</p>
</li>
</ol>
<h3 id="fetch-googlecloudplatformkubeflow-distribution-and-upstream-packages">Fetch GoogleCloudPlatform/kubeflow-distribution and upstream packages</h3>
<ol>
<li>
<p>If you have already installed Management cluster, you have <code>GoogleCloudPlatform/kubeflow-distribution</code> locally. You just need to run <code>cd kubeflow</code> to access Kubeflow cluster manifests. Otherwise, you can run the following commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Check out Kubeflow v1.6.1 blueprints</span>
</span></span><span class="line"><span class="cl">git clone https://github.com/GoogleCloudPlatform/kubeflow-distribution.git 
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kubeflow-distribution
</span></span><span class="line"><span class="cl">git checkout tags/v1.6.1 -b v1.6.1
</span></span></code></pre></div><p>Alternatively, you can get the package by using <code>kpt</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Check out Kubeflow v1.6.1 blueprints</span>
</span></span><span class="line"><span class="cl">kpt pkg get https://github.com/GoogleCloudPlatform/kubeflow-distribution.git@v1.6.1 kubeflow-distribution
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kubeflow-distribution
</span></span></code></pre></div></li>
<li>
<p>Run the following command to pull upstream manifests from <code>kubeflow/manifests</code> repository.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Visit Kubeflow cluster related manifests</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kubeflow
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">bash ./pull-upstream.sh
</span></span></code></pre></div></li>
</ol>
<h3 id="environment-variables">Environment Variables</h3>
<p>Log in to gcloud. You only need to run this command once:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud auth login
</span></span></code></pre></div><ol>
<li>
<p>Review and fill all the environment variables in <code>kubeflow-distribution/kubeflow/env.sh</code>, they will be used by <code>kpt</code> later on, and some of them will be used in this deployment guide. Review the comment in <code>env.sh</code> for the explanation for each environment variable. After defining these environment variables, run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">source</span> env.sh
</span></span></code></pre></div></li>
<li>
<p>Set environment variables with OAuth Client ID and Secret for IAP:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLIENT_ID</span><span class="o">=</span>&lt;Your CLIENT_ID&gt;
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLIENT_SECRET</span><span class="o">=</span>&lt;Your CLIENT_SECRET&gt;
</span></span></code></pre></div>

<div class="alert alert-primary" role="alert">
<h4 class="alert-heading">Note</h4>

    Do not omit the <b>export</b> because scripts triggered by <b>make</b> need these environment variables. Do not check in these two environment variables configuration to source control, they are secrets.

</div>

</li>
</ol>
<h4 id="kpt-setter-config">kpt setter config</h4>
<p>Run the following commands to configure kpt setter for your Kubeflow cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">bash ./kpt-set.sh
</span></span></code></pre></div><p>Everytime you change environment variables, make sure you run the command above to apply
kpt setter change to all packages. Otherwise, kustomize build will not be able to pick up
new changes.</p>
<p>Note, you can find out which setters exist in a package and their
current values by running the following commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kpt fn <span class="nb">eval</span> -i list-setters:v0.1 ./apps
</span></span><span class="line"><span class="cl">kpt fn <span class="nb">eval</span> -i list-setters:v0.1 ./common
</span></span></code></pre></div><p>You can learn more about <code>list-setters</code> in <a href="https://catalog.kpt.dev/list-setters/v0.1/">kpt documentation</a>.</p>
<h4 id="authorize-cloud-config-connector-for-each-kubeflow-project">Authorize Cloud Config Connector for each Kubeflow project</h4>
<p>In the <a href="/docs/distributions/gke/deploy/management-setup/">Management cluster deployment</a> we created the Google Cloud service account <strong>serviceAccount:kcc-${KF_PROJECT}@${MGMT_PROJECT}.iam.gserviceaccount.com</strong>
this is the service account that Config Connector will use to create any Google Cloud resources in <code>${KF_PROJECT}</code>. You need to grant this Google Cloud service account sufficient privileges to create the desired resources in Kubeflow project.
You only need to perform steps below once for each Kubeflow project, but make sure to do it even when KF_PROJECT and MGMT_PROJECT are the same project.</p>
<p>The easiest way to do this is to grant the Google Cloud service account owner permissions on one or more projects.</p>
<ol>
<li>
<p>Set the Management environment variable if you haven&rsquo;t:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">MGMT_PROJECT</span><span class="o">=</span>&lt;the project where you deploy your management cluster&gt;
</span></span><span class="line"><span class="cl"><span class="nv">MGMT_NAME</span><span class="o">=</span>&lt;the kubectl context name <span class="k">for</span> management cluster&gt;
</span></span></code></pre></div></li>
<li>
<p>Apply ConfigConnectorContext for <code>${KF_PROJECT}</code> in management cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply-kcc
</span></span></code></pre></div></li>
</ol>
<h3 id="configure-kubeflow">Configure Kubeflow</h3>
<p>Make sure you are using KF_PROJECT in the gcloud CLI tool:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud config <span class="nb">set</span> project <span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span>
</span></span></code></pre></div><h3 id="deploy-kubeflow">Deploy Kubeflow</h3>
<p>To deploy Kubeflow, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply
</span></span></code></pre></div><ul>
<li>
<p>If deployment returns an error due to missing resources in <code>serving.kserve.io</code> API group, rerun <code>make apply</code>. This is due to a race condition between CRD and runtime resources in KServe.</p>
<ul>
<li>This issue is being tracked in <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/issues/384">GoogleCloudPlatform/kubeflow-distribution#384</a></li>
</ul>
</li>
<li>
<p>If resources can&rsquo;t be created because <code>webhook.cert-manager.io</code> is unavailable wait and
then rerun <code>make apply</code></p>
<ul>
<li>This issue is being tracked in <a href="https://github.com/kubeflow/manifests/issues/1234">kubeflow/manifests#1234</a></li>
</ul>
</li>
<li>
<p>If resources can&rsquo;t be created with an error message like:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">error: unable to recognize <span class="s2">&#34;.build/application/app.k8s.io_v1beta1_application_application-controller-kubeflow.yaml&#34;</span>: no matches <span class="k">for</span> kind <span class="s2">&#34;Application&#34;</span> in version <span class="s2">&#34;app.k8s.io/v1beta1”
</span></span></span></code></pre></div><p>This issue occurs when the CRD endpoint isn&rsquo;t established in the Kubernetes API server when the CRD&rsquo;s custom object is applied.
This issue is expected and can happen multiple times for different kinds of resource. To resolve this issue, try running <code>make apply</code> again.</p>
</li>
</ul>
<h3 id="check-your-deployment">Check your deployment</h3>
<p>Follow these steps to verify the deployment:</p>
<ol>
<li>
<p>When the deployment finishes, check the resources installed in the namespace
<code>kubeflow</code> in your new cluster.  To do this from the command line, first set
your <code>kubectl</code> credentials to point to the new cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud container clusters get-credentials <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span><span class="s2">&#34;</span> --zone <span class="s2">&#34;</span><span class="si">${</span><span class="nv">ZONE</span><span class="si">}</span><span class="s2">&#34;</span> --project <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div><p>Then, check what&rsquo;s installed in the <code>kubeflow</code> namespace of your GKE cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl -n kubeflow get all
</span></span></code></pre></div></li>
</ol>
<h3 id="access-the-kubeflow-user-interface-ui">Access the Kubeflow user interface (UI)</h3>
<p>To access the Kubeflow central dashboard, follow these steps:</p>
<ol>
<li>
<p>Use the following command to grant yourself the <a href="https://cloud.google.com/iap/docs/managing-access">IAP-secured Web App User</a> role:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span><span class="s2">&#34;</span> --member<span class="o">=</span>user:&lt;EMAIL&gt; --role<span class="o">=</span>roles/iap.httpsResourceAccessor
</span></span></code></pre></div><p>Note, you need the <code>IAP-secured Web App User</code> role even if you are already an owner or editor of the project. <code>IAP-secured Web App User</code> role is not implied by the <code>Project Owner</code> or <code>Project Editor</code> roles.</p>
</li>
<li>
<p>Enter the following URI into your browser address bar. It can take 20
minutes for the URI to become available: <code>https://${KF_NAME}.endpoints.${KF_PROJECT}.cloud.goog/</code></p>
<p>You can run the following command to get the URI for your deployment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl -n istio-system get ingress
</span></span><span class="line"><span class="cl">NAME            HOSTS                                                      ADDRESS         PORTS   AGE
</span></span><span class="line"><span class="cl">envoy-ingress   your-kubeflow-name.endpoints.your-gcp-project.cloud.goog   34.102.232.34   <span class="m">80</span>      5d13h
</span></span></code></pre></div><p>The following command sets an environment variable named <code>HOST</code> to the URI:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">HOST</span><span class="o">=</span><span class="k">$(</span>kubectl -n istio-system get ingress envoy-ingress -o<span class="o">=</span><span class="nv">jsonpath</span><span class="o">={</span>.spec.rules<span class="o">[</span>0<span class="o">]</span>.host<span class="o">}</span><span class="k">)</span>
</span></span></code></pre></div></li>
<li>
<p>Follow the instructions on the UI to create a namespace. Refer to this guide on
<a href="/docs/components/multi-tenancy/getting-started/#automatic-profile-creation">creation of profiles</a>.</p>
</li>
</ol>
<p>Notes:</p>
<ul>
<li>It can take 20 minutes for the URI to become available.
Kubeflow needs to provision a signed SSL certificate and register a DNS
name.</li>
<li>If you own or manage the domain or a subdomain with
<a href="https://cloud.google.com/dns/docs/">Cloud DNS</a>
then you can configure this process to be much faster.
Check <a href="https://github.com/kubeflow/kubeflow/issues/731">kubeflow/kubeflow#731</a>.</li>
</ul>
<h2 id="understanding-the-deployment-process">Understanding the deployment process</h2>
<p>This section gives you more details about the kubectl, kustomize, config connector configuration and
deployment process, so that you can customize your Kubeflow deployment if necessary.</p>
<h3 id="application-layout">Application layout</h3>
<p>Your Kubeflow application directory <code>kubeflow-distribution/kubeflow</code> contains the following files and
directories:</p>
<ul>
<li>
<p><strong>Makefile</strong> is a file that defines rules to automate deployment process. You can refer to <a href="https://www.gnu.org/software/make/manual/make.html#Introduction">GNU make documentation</a> for more introduction. The Makefile we provide is designed to be user maintainable. You are encouraged to read, edit and maintain it to suit your own deployment customization needs.</p>
</li>
<li>
<p><strong>apps</strong>, <strong>common</strong>, <strong>contrib</strong> are a series of independent components  directory containing kustomize packages for deploying Kubeflow components. The structure is to align with upstream <a href="https://github.com/kubeflow/manifests">kubeflow/manifests</a>.</p>
<ul>
<li>
<p><a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution">GoogleCloudPlatform/kubeflow-distribution</a> repository only stores <code>kustomization.yaml</code> and <code>patches</code> for Google Cloud specific resources.</p>
</li>
<li>
<p><code>./pull_upstream.sh</code> will pull <code>kubeflow/manifests</code> and store manifests in <code>upstream</code> folder of each component in this guide. <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution">GoogleCloudPlatform/kubeflow-distribution</a> repository doesn&rsquo;t store the copy of upstream manifests.</p>
</li>
</ul>
</li>
<li>
<p><strong>build</strong> is a directory that will contain the hydrated manifests outputted by
the <code>make</code> rules, each component will have its own <strong>build</strong> directory. You can customize the <strong>build</strong> path when calling <code>make</code> command.</p>
</li>
</ul>
<h3 id="source-control">Source Control</h3>
<p>It is recommended that you check in your entire local repository into source control.</p>
<p>Checking in <strong>build</strong> is recommended so you can easily see differences by <code>git diff</code> in manifests before applying them.</p>
<h2 id="google-cloud-service-accounts">Google Cloud service accounts</h2>
<p>The kfctl deployment process creates three service accounts in your
Google Cloud project. These service accounts follow the <a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">principle of least
privilege</a>.
The service accounts are:</p>
<ul>
<li><code>${KF_NAME}-admin</code> is used for some admin tasks like configuring the load
balancers. The principle is that this account is needed to deploy Kubeflow but
not needed to actually run jobs.</li>
<li><code>${KF_NAME}-user</code> is intended to be used by training jobs and models to access
Google Cloud resources (Cloud Storage, BigQuery, etc.). This account has a much smaller
set of privileges compared to <code>admin</code>.</li>
<li><code>${KF_NAME}-vm</code> is used only for the virtual machine (VM) service account. This
account has the minimal permissions needed to send metrics and logs to
<a href="https://cloud.google.com/stackdriver/">Stackdriver</a>.</li>
</ul>
<h2 id="upgrade-kubeflow">Upgrade Kubeflow</h2>
<p>Refer to <a href="/docs/distributions/gke/deploy/upgrade#upgrading-kubeflow-cluster">Upgrading Kubeflow cluster</a>.</p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>Run a full ML workflow on Kubeflow, using the
<a href="https://github.com/kubeflow/examples/blob/master/mnist/mnist_gcp.ipynb">end-to-end MNIST tutorial</a> or the
<a href="https://github.com/kubeflow/examples/tree/master/github_issue_summarization/pipelines">GitHub issue summarization Pipelines
example</a>.</li>
<li>Learn how to <a href="/docs/distributions/gke/deploy/delete-cli/">delete your Kubeflow deployment using the CLI</a>.</li>
<li>To add users to Kubeflow, go to <a href="/docs/distributions/gke/customizing-gke/#add-users-to-kubeflow">a dedicated section in Customizing Kubeflow on GKE</a>.</li>
<li>To taylor your Kubeflow deployment on GKE, go to <a href="/docs/distributions/gke/customizing-gke/">Customizing Kubeflow on GKE</a>.</li>
<li>For troubleshooting Kubeflow deployments on GKE, go to the <a href="/docs/distributions/gke/troubleshooting-gke/">Troubleshooting deployments</a> guide.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-8c408287dd856705815307c8c153e537">3.1.6 - Upgrade Kubeflow</h1>
    <div class="lead">Upgrading your Kubeflow installation on Google Cloud</div>
	<h2 id="before-you-start">Before you start</h2>
<p>To better understand upgrade process, you should read the following sections first:</p>
<ul>
<li><a href="/docs/distributions/gke/deploy/management-setup#understanding-the-deployment-process">Understanding the deployment process for management cluster</a></li>
<li><a href="/docs/distributions/gke/deploy/deploy-cli#understanding-the-deployment-process">Understanding the deployment process for Kubeflow cluster</a></li>
</ul>
<p>This guide assumes the following settings:</p>
<ul>
<li>The <code>${MGMT_DIR}</code> and <code>${MGMT_NAME}</code> environment variables
are the same as in <a href="/docs/distributions/gke/deploy/management-setup#configure-environment-variables">Management cluster setup</a>.</li>
<li>The <code>${KF_NAME}</code>, <code>${CLIENT_ID}</code> and <code>${CLIENT_SECRET}</code> environment variables
are the same as in <a href="/docs/distributions/gke/deploy/deploy-cli#environment-variables">Deploy using kubectl and kpt</a>.</li>
<li>The <code>${KF_DIR}</code> environment variable contains the path to
your Kubeflow application directory, which holds your Kubeflow configuration
files. For example, <code>/opt/kubeflow-distribution/kubeflow/</code>.</li>
</ul>
<h2 id="general-upgrade-instructions">General upgrade instructions</h2>
<p>Starting from Kubeflow v1.5, we have integrated with <a href="https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview">Config Controller</a>. You don&rsquo;t need to manually upgrade Management cluster any more, since it managed by <a href="https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-setup#upgrade">Upgrade Config Controller</a>.</p>
<p>Starting from Kubeflow v1.3, we have reworked on the structure of <code>GoogleCloudPlatform/kubeflow-distribution</code> repository. All resources are located in <code>kubeflow-distribution/management</code> directory. Upgrade to Management cluster v1.3 is not supported.</p>
<p>Before Kubeflow v1.3, both management cluster and Kubeflow cluster follow the same <code>instance</code> and <code>upstream</code> folder convention. To upgrade, you&rsquo;ll typically need to update packages in <code>upstream</code> to the new version and repeat the <code>make apply-&lt;subcommand&gt;</code> commands in their respective deployment process.</p>
<p>However, specific upgrades might need manual actions below.</p>
<h2 id="upgrading-management-cluster">Upgrading management cluster</h2>
<h3 id="upgrading-management-cluster-before-15">Upgrading management cluster before 1.5</h3>
<p>It is strongly recommended to use source control to keep a copy of your working repository for recording changes at each step.</p>
<p>Due to the refactoring of <code>kubeflow/manifests</code> repository, the way we depend on <code>GoogleCloudPlatform/kubeflow-distribution</code> has changed drastically. This section suits for upgrading from Kubeflow 1.3 to higher.</p>
<ol>
<li>
<p>The instructions below assume that your current working directory is</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMT_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Use your management cluster&rsquo;s kubectl context:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Look at all your contexts</span>
</span></span><span class="line"><span class="cl">kubectl config get-contexts
</span></span><span class="line"><span class="cl"><span class="c1"># Select your management cluster&#39;s context</span>
</span></span><span class="line"><span class="cl">kubectl config use-context <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Verify the context connects to the cluster properly</span>
</span></span><span class="line"><span class="cl">kubectl get namespace
</span></span></code></pre></div><p>If you are using a different environment, you can always
reconfigure the context by:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make create-context
</span></span></code></pre></div></li>
<li>
<p>Check your existing config connector version:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># For Kubeflow v1.3, it should be 1.46.0</span>
</span></span><span class="line"><span class="cl">$ kubectl get namespace cnrm-system -ojsonpath<span class="o">=</span><span class="s1">&#39;{.metadata.annotations.cnrm\.cloud\.google\.com\/version}&#39;</span>
</span></span><span class="line"><span class="cl">1.46.0
</span></span></code></pre></div></li>
<li>
<p>Merge the content from new Kubeflow version of <code>GoogleCloudPlatform/kubeflow-distribution</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">WORKING_BRANCH</span><span class="o">=</span>&lt;your-github-working-branch&gt;
</span></span><span class="line"><span class="cl"><span class="nv">VERSION_TAG</span><span class="o">=</span>&lt;targeted-kubeflow-version-tag-on-github&gt;
</span></span><span class="line"><span class="cl">git checkout -b <span class="s2">&#34;</span><span class="si">${</span><span class="nv">WORKING_BRANCH</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">git remote add upstream https://github.com/GoogleCloudPlatform/kubeflow-distribution.git <span class="c1"># This is one time only.</span>
</span></span><span class="line"><span class="cl">git fetch upstream 
</span></span><span class="line"><span class="cl">git merge <span class="s2">&#34;</span><span class="si">${</span><span class="nv">VERSION_TAG</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Make sure your build directory (<code>./build</code> by default) is checked in to source control (git).</p>
</li>
<li>
<p>Run the following command to hydrate Config Connector resources:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make hydrate-kcc
</span></span></code></pre></div></li>
<li>
<p>Compare the difference on your source control tracking after making hydration change. If they are addition or modification only, proceed to next step. If it includes deletion, you need to use <code>kubectl delete</code> to manually clean up the deleted resource for cleanup.</p>
</li>
<li>
<p>After confirmation, run the following command to apply new changes:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply-kcc
</span></span></code></pre></div></li>
<li>
<p>Check version has been upgraded after applying new Config Connector resource:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ kubectl get namespace cnrm-system -ojsonpath<span class="o">=</span><span class="s1">&#39;{.metadata.annotations.cnrm\.cloud\.google\.com\/version}&#39;</span>
</span></span></code></pre></div></li>
</ol>
<h3 id="upgrade-management-cluster-from-v11-to-v12">Upgrade management cluster from v1.1 to v1.2</h3>
<ol>
<li>
<p>The instructions below assume that your current working directory is</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMT_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Use your management cluster&rsquo;s kubectl context:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Look at all your contexts</span>
</span></span><span class="line"><span class="cl">kubectl config get-contexts
</span></span><span class="line"><span class="cl"><span class="c1"># Select your management cluster&#39;s context</span>
</span></span><span class="line"><span class="cl">kubectl config use-context <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Verify the context connects to the cluster properly</span>
</span></span><span class="line"><span class="cl">kubectl get namespace
</span></span></code></pre></div><p>If you are using a different environment, you can always
reconfigure the context by:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make create-context
</span></span></code></pre></div></li>
<li>
<p>Check your existing config connector version:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># For Kubeflow v1.1, it should be 1.15.1</span>
</span></span><span class="line"><span class="cl">$ kubectl get namespace cnrm-system -ojsonpath<span class="o">=</span><span class="s1">&#39;{.metadata.annotations.cnrm\.cloud\.google\.com\/version}&#39;</span>
</span></span><span class="line"><span class="cl">1.15.1
</span></span></code></pre></div></li>
<li>
<p>Uninstall the old config connector in the management cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl delete sts,deploy,po,svc,roles,clusterroles,clusterrolebindings --all-namespaces -l cnrm.cloud.google.com/system<span class="o">=</span><span class="nb">true</span> --wait<span class="o">=</span><span class="nb">true</span>
</span></span><span class="line"><span class="cl">kubectl delete validatingwebhookconfiguration abandon-on-uninstall.cnrm.cloud.google.com --ignore-not-found --wait<span class="o">=</span><span class="nb">true</span>
</span></span><span class="line"><span class="cl">kubectl delete validatingwebhookconfiguration validating-webhook.cnrm.cloud.google.com --ignore-not-found --wait<span class="o">=</span><span class="nb">true</span>
</span></span><span class="line"><span class="cl">kubectl delete mutatingwebhookconfiguration mutating-webhook.cnrm.cloud.google.com --ignore-not-found --wait<span class="o">=</span><span class="nb">true</span>
</span></span></code></pre></div><p>These commands uninstall the config connector without removing your resources.</p>
</li>
<li>
<p>Replace your <code>./Makefile</code> with the version in Kubeflow <code>v1.2.0</code>: <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/v1.2.0/management/Makefile">https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/v1.2.0/management/Makefile</a>.</p>
<p>If you made any customizations in <code>./Makefile</code>, you should merge your changes with the upstream version. We&rsquo;ve refactored the Makefile to move substantial commands into the upstream package, so hopefully future upgrades won&rsquo;t require a manual merge of the Makefile.</p>
</li>
<li>
<p>Update <code>./upstream/management</code> package:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make update
</span></span></code></pre></div></li>
<li>
<p>Use kpt to set user values:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kpt cfg <span class="nb">set</span> -R . name <span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">kpt cfg <span class="nb">set</span> -R . gcloud.core.project <span class="si">${</span><span class="nv">MGMT_PROJECT</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">kpt cfg <span class="nb">set</span> -R . location <span class="si">${</span><span class="nv">LOCATION</span><span class="si">}</span>
</span></span></code></pre></div><p>Note, you can find out which setters exist in a package and what there current values are by:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kpt cfg list-setters .
</span></span></code></pre></div></li>
<li>
<p>Apply upgraded config connector:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply-kcc
</span></span></code></pre></div><p>Note, you can optionally also run <code>make apply-cluster</code>, but it should be the same as your existing management cluster.</p>
</li>
<li>
<p>Check that your config connector upgrade is successful:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># For Kubeflow v1.2, it should be 1.29.0</span>
</span></span><span class="line"><span class="cl">$ kubectl get namespace cnrm-system -ojsonpath<span class="o">=</span><span class="s1">&#39;{.metadata.annotations.cnrm\.cloud\.google\.com\/version}&#39;</span>
</span></span><span class="line"><span class="cl">1.29.0
</span></span></code></pre></div></li>
</ol>
<h2 id="upgrading-kubeflow-cluster">Upgrading Kubeflow cluster</h2>


<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">DISCLAIMERS</h4>

    <div>The upgrade process depends on each Kubeflow application to handle the upgrade properly. There's no guarantee on data completeness unless the application provides such a guarantee.</div>
<div>You are recommended to back up your data before an upgrade.</div>
<div>Upgrading Kubeflow cluster can be a disruptive process, please schedule some downtime and communicate with your users.</div>


</div>

<p>To upgrade from specific versions of Kubeflow, you may need to take certain manual actions — refer to specific sections in the guidelines below.</p>
<h3 id="general-instructions-for-upgrading-kubeflow-cluster">General instructions for upgrading Kubeflow cluster</h3>
<ol>
<li>
<p>The instructions below assume that:</p>
<ul>
<li>
<p>Your current working directory is:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span>
</span></span></code></pre></div></li>
<li>
<p>Your kubectl uses a context that connects to your Kubeflow cluster</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># List your existing contexts</span>
</span></span><span class="line"><span class="cl">kubectl config get-contexts
</span></span><span class="line"><span class="cl"><span class="c1"># Use the context that connects to your Kubeflow cluster</span>
</span></span><span class="line"><span class="cl">kubectl config use-context <span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>Merge the new version of <code>GoogleCloudPlatform/kubeflow-distribution</code> (example: v1.3.1), you don&rsquo;t need to do it again if you have already done so during management cluster upgrade.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">WORKING_BRANCH</span><span class="o">=</span>&lt;your-github-working-branch&gt;
</span></span><span class="line"><span class="cl"><span class="nv">VERSION_TAG</span><span class="o">=</span>&lt;targeted-kubeflow-version-tag-on-github&gt;
</span></span><span class="line"><span class="cl">git checkout -b <span class="s2">&#34;</span><span class="si">${</span><span class="nv">WORKING_BRANCH</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">git remote add upstream https://github.com/GoogleCloudPlatform/kubeflow-distribution.git <span class="c1"># This is one time only.</span>
</span></span><span class="line"><span class="cl">git fetch upstream 
</span></span><span class="line"><span class="cl">git merge <span class="s2">&#34;</span><span class="si">${</span><span class="nv">VERSION_TAG</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Change the <code>KUBEFLOW_MANIFESTS_VERSION</code> in <code>./pull-upstream.sh</code> with the targeted kubeflow version same as <code>$VERSION_TAG</code>. Run the following commands to pull new changes from upstream <code>kubeflow/manifests</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">bash ./pull-upstream.sh
</span></span></code></pre></div></li>
<li>
<p>(Optional) If you only want to upgrade some of Kubeflow components, you can comment non-upgrade components in <code>kubeflow/config.yaml</code> file. Commands below will only apply the remaining components.</p>
</li>
<li>
<p>Make sure you have checked in <code>build</code> folders for each component. The following command will change them so you can compare for difference.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make hydrate
</span></span></code></pre></div></li>
<li>
<p>Once you confirm the changes are ready to apply, run the following command to upgrade Kubeflow cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply
</span></span></code></pre></div></li>
</ol>


<div class="alert alert-primary" role="alert">
<h4 class="alert-heading">Note</h4>

    Kubeflow on Google Cloud doesn&rsquo;t guarantee the upgrade for each Kubeflow component always works with the general upgrade guide here. Please refer to corresponding repository in <a href="https://github.com/kubeflow">Kubeflow org</a> for upgrade support.

</div>

<h3 id="upgrade-kubeflow-cluster-to-v16">Upgrade Kubeflow cluster to v1.6</h3>
<p>Starting from Kubeflow v1.6.0:</p>
<ul>
<li>Component with deprecated API versions were upgraded to support GKE v1.22. If you would like to upgrade your GKE cluster, follow <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster">GCP instructions</a>.</li>
<li>ASM was upgraded to v1.14. Follow the instructions on how to <a href="#upgrade-asm-anthos-service-mesh">upgrade ASM (Anthos Service Mesh)</a>. If you want to use ASM version prior to 1.11, refer to <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/common/asm/deprecated/README.md">the legacy instructions</a>.</li>
<li>Knative was upgraded to v1.2. Follow <a href="https://knative.dev/docs/install/upgrade/upgrade-installation/">Knative instructions</a> to check current version and see if the update includes any breaking changes.</li>
<li>Cert-manager was upgraded to v1.5. To check your current version and see if the update includes any breaking changes, follow the <a href="https://cert-manager.io/docs/installation/upgrading/">cert-manager instructions</a>.</li>
<li>Deprecated kfserving component was removed. To upgrade to KServe, follow the <a href="https://github.com/kserve/kserve/tree/master/hack/kserve_migration">KServe Migration guide</a>.</li>
</ul>
<h3 id="upgrade-kubeflow-cluster-to-v15">Upgrade Kubeflow cluster to v1.5</h3>
<p>In Kubeflow v1.5.1 we use ASM v1.13. See <a href="#upgrade-asm-anthos-service-mesh">how to upgrade ASM</a>. To use ASM versions prior to 1.11, follow <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/common/asm/deprecated/README.md">the legacy instructions</a>.</p>
<p>Starting from Kubeflow v1.5, Kubeflow manifests have included KServe as an independent component from kfserving, Google Cloud distribution has switched over from kfserving to KServe for default installed components. If you want to upgrade Kubeflow while keeping kfsering, you can comment KServe and uncomment kfserving in <code>kubeflow-distribution/kubeflow/config.yaml</code> file. If you want to upgrade to KServe, follow the <a href="https://github.com/kserve/kserve/tree/master/hack/kserve_migration">KServe Migration guide</a>.</p>
<h3 id="upgrade-kubeflow-cluster-to-v13">Upgrade Kubeflow cluster to v1.3</h3>
<p>Due to the refactoring of <code>kubeflow/manifests</code> repository, the way we depend on <code>GoogleCloudPlatform/kubeflow-distribution</code> has changed drastically. Upgrade to Kubeflow cluster v1.3 is not supported. And individual component upgrade has been deferred to its corresponding repository for support.</p>
<h3 id="upgrade-kubeflow-cluster-from-v11-to-v12">Upgrade Kubeflow cluster from v1.1 to v1.2</h3>
<ol>
<li>
<p>The instructions below assume</p>
<ul>
<li>
<p>Your current working directory is:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span>
</span></span></code></pre></div></li>
<li>
<p>Your kubectl uses a context that connects to your Kubeflow cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># List your existing contexts</span>
</span></span><span class="line"><span class="cl">kubectl config get-contexts
</span></span><span class="line"><span class="cl"><span class="c1"># Use the context that connects to your Kubeflow cluster</span>
</span></span><span class="line"><span class="cl">kubectl config use-context <span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>(Recommended) Replace your <code>./Makefile</code> with the version in Kubeflow <code>v1.2.0</code>: <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/v1.2.0/kubeflow/Makefile">https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/v1.2.0/kubeflow/Makefile</a>.</p>
<p>If you made any customizations in <code>./Makefile</code>, you should merge your changes with the upstream version.</p>
<p>This step is recommended, because we introduced usability improvements and fixed compatibility for newer Kustomize versions (while still being compatible with Kustomize v3.2.1) to the Makefile. However, the deployment process is backward-compatible, so this is recommended, but not required.</p>
</li>
<li>
<p>Update <code>./upstream/manifests</code> package:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make update
</span></span></code></pre></div></li>
<li>
<p>Before applying new resources, you need to delete some immutable resources that were updated in this release:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl delete statefulset kfserving-controller-manager -n kubeflow --wait
</span></span><span class="line"><span class="cl">kubectl delete crds experiments.kubeflow.org suggestions.kubeflow.org trials.kubeflow.org
</span></span></code></pre></div><p><strong>WARNING</strong>: This step <strong>deletes</strong> all Katib running resources.</p>
<p>Refer to <a href="https://github.com/kubeflow/kubeflow/issues/5371#issuecomment-731359384">a github comment in the v1.2 release issue</a> for more details.</p>
</li>
<li>
<p>Redeploy:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply
</span></span></code></pre></div><p>To evaluate the changes before deploying them you can:</p>
<ol>
<li>Run <code>make hydrate</code>.</li>
<li>Compare the contents
of <code>.build</code> with a historic version with tools like <code>git diff</code>.</li>
</ol>
</li>
</ol>
<h2 id="upgrade-asm-anthos-service-mesh">Upgrade ASM (Anthos Service Mesh)</h2>
<p>If you want to upgrade ASM instead of the Kubeflow components, refer to <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/common/asm/README.md">kubeflow/common/asm/README.md</a> for the latest instructions on upgrading ASM. Detailed explanation is listed below. Note: if you are going to upgrade major or minor version of ASM, it is recommended to read <a href="https://cloud.google.com/service-mesh/docs/upgrade-path-old-versions-gke">the official ASM upgrade documentation</a> before proceeding with the steps below.</p>
<h3 id="install-a-new-asm-workload">Install a new ASM workload</h3>
<p>In order to use the new ASM version, we need to download the corresponding ASM configuration package and <code>asmcli</code> script. Get a list of available ASM packages and the corresponding <code>asmcli</code> scripts by running the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl https://storage.googleapis.com/csm-artifacts/asm/ASMCLI_VERSIONS
</span></span></code></pre></div><p>It should return a list of ASM versions that can be installed with asmcli script. To install older versions, refer to <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/common/asm/deprecated/README.md">the legacy instructions</a>. The returned list will have a format of <code>${ASM_PACKAGE_VERSION}:${ASMCLI_SCRIPT_VERSION}</code>. For example, in the following output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">1.13.2-asm.5+config2:asmcli_1.13.2-asm.5-config2
</span></span><span class="line"><span class="cl">1.13.2-asm.5+config1:asmcli_1.13.2-asm.5-config1
</span></span><span class="line"><span class="cl">1.13.2-asm.2+config2:asmcli_1.13.2-asm.2-config2
</span></span><span class="line"><span class="cl">1.13.2-asm.2+config1:asmcli_1.13.2-asm.2-config1
</span></span><span class="line"><span class="cl">1.13.1-asm.1+config1:asmcli_1.13.1-asm.1-config1
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>record 1.13.2-asm.5+config2:asmcli_1.13.2-asm.5-config2 corresponds to:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">ASM_PACKAGE_VERSION</span><span class="o">=</span>1.13.2-asm.5+config2
</span></span><span class="line"><span class="cl"><span class="nv">ASMCLI_SCRIPT_VERSION</span><span class="o">=</span>asmcli_1.13.2-asm.5-config2
</span></span></code></pre></div><p>You need to set these two values in <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/asm/Makefile">kubeflow/asm/Makefile</a>. Then, run the following command in <code>kubeflow/asm</code> directory to install the new ASM. Note, the old ASM will not be uninstalled.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply
</span></span></code></pre></div><p>Once installed successfully, you can see istiod <code>Deployment</code> in your cluster with name in pattern <code>istiod-asm-VERSION-REVISION</code>. For example, <code>istiod-asm-1132-5</code> would correspond to ASM version 1.13.2-asm.5.</p>
<h3 id="upgrade-other-kubeflow-components-to-use-new-asm">Upgrade other Kubeflow components to use new ASM</h3>
<p>There are multiple Kubeflow components with ASM namespace label, including user created namespaces. To upgrade them at once, change the following line in <code>kubeflow/env.sh</code> with the new ASM version <code>asm-VERSION-REVISION</code>, like <code>asm-1132-5</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">ASM_LABEL</span><span class="o">=</span>asm-1132-5
</span></span></code></pre></div><p>Then run the following commands in <code>kubeflow/</code> directory to configure the environmental variables:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">source</span> env.sh
</span></span></code></pre></div><p>Run the following command to configure kpt setter:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">bash kpt-set.sh
</span></span></code></pre></div><p>Examine the change using source control after running the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make hydrate
</span></span></code></pre></div><p>Refer to <a href="https://cloud.google.com/service-mesh/docs/unified-install/upgrade#deploying_and_redeploying_workloads">Deploying and redeploying workloads</a> for the complete steps to adopt the new ASM version. As part of the instructions, you can run the following command to update namespaces&rsquo; labels across the cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">make apply
</span></span></code></pre></div><h3 id="optional-uninstall-the-old-asm-workload">(Optional) Uninstall the old ASM workload</h3>
<p>Once you validated that new ASM installation and sidecar-injection for Kubeflow components are working as expected. You can <strong>Complete the transition</strong> to the new ASM or <strong>Rollback</strong> to the old ASM as instructed in <a href="https://cloud.google.com/service-mesh/docs/unified-install/upgrade#deploying_and_redeploying_workloads">Deploy and Redeploy workloads</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-67037ec2f502c6d0b59e7689879bec78">3.1.7 - Monitor Cloud IAP Setup</h1>
    <div class="lead">Instructions for monitoring and troubleshooting Cloud IAP</div>
	<p><a href="https://cloud.google.com/iap/docs/">Cloud Identity-Aware Proxy (Cloud IAP)</a> is
the recommended solution for accessing your Kubeflow
deployment from outside the cluster, when running Kubeflow on Google Cloud.</p>
<p>This document is a step-by-step guide to ensuring that your IAP-secured endpoint
is available, and to debugging problems that may cause the endpoint to be
unavailable.</p>
<h2 id="introduction">Introduction</h2>
<p>When deploying Kubeflow using the <a href="/docs/distributions/gke/deploy/deploy-cli/">command-line interface</a>,
you choose the authentication method you want to use. One of the options is
Cloud IAP. This document assumes that you have already deployed Kubeflow.</p>
<p>Kubeflow uses the <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs">Google-managed certificate</a>
to provide an SSL certificate for the Kubeflow Ingress.</p>
<p>Cloud IAP gives you the following benefits:</p>
<ul>
<li>Users can log in in using their Google Cloud accounts.</li>
<li>You benefit from Google&rsquo;s security expertise to protect your sensitive
workloads.</li>
</ul>
<h2 id="monitoring-your-cloud-iap-setup">Monitoring your Cloud IAP setup</h2>
<p>Follow these instructions to monitor your Cloud IAP setup and troubleshoot any
problems:</p>
<ol>
<li>
<p>Examine the
<a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a>
and Google Cloud Build (GCB) load balancer to make sure it is available:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe ingress
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Name:             envoy-ingress
</span></span><span class="line"><span class="cl">Namespace:        kubeflow
</span></span><span class="line"><span class="cl">Address:          35.244.132.160
</span></span><span class="line"><span class="cl">Default backend:  default-http-backend:80 (10.20.0.10:8080)
</span></span><span class="line"><span class="cl">Annotations:
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Events:
</span></span><span class="line"><span class="cl">   Type     Reason     Age                 From                     Message
</span></span><span class="line"><span class="cl">   ----     ------     ----                ----                     -------
</span></span><span class="line"><span class="cl">   Normal   ADD        12m                 loadbalancer-controller  kubeflow/envoy-ingress
</span></span><span class="line"><span class="cl">   Warning  Translate  12m (x10 over 12m)  loadbalancer-controller  error while evaluating the ingress spec: could not find service &#34;kubeflow/envoy&#34;
</span></span><span class="line"><span class="cl">   Warning  Translate  12m (x2 over 12m)   loadbalancer-controller  error while evaluating the ingress spec: error getting BackendConfig for port &#34;8080&#34; on service &#34;kubeflow/envoy&#34;, err: no BackendConfig for service port exists.
</span></span><span class="line"><span class="cl">   Warning  Sync       12m                 loadbalancer-controller  Error during sync: Error running backend syncing routine: received errors when updating backend service: googleapi: Error 400: The resource &#39;projects/code-search-demo/global/backendServices/k8s-be-32230--bee2fc38fcd6383f&#39; is not ready, resourceNotReady
</span></span><span class="line"><span class="cl"> googleapi: Error 400: The resource &#39;projects/code-search-demo/global/backendServices/k8s-be-32230--bee2fc38fcd6383f&#39; is not ready, resourceNotReady
</span></span><span class="line"><span class="cl">   Normal  CREATE  11m  loadbalancer-controller  ip: 35.244.132.160
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>There should be an annotation indicating that we are using managed certificate:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">annotation:
</span></span><span class="line"><span class="cl">  networking.gke.io/managed-certificates: gke-certificate
</span></span></code></pre></div><p>Any problems with creating the load balancer are reported as Kubernetes
events in the results of the above <code>describe</code> command.</p>
<ul>
<li>
<p>If the address isn&rsquo;t set then there was a problem creating the load
balancer.</p>
</li>
<li>
<p>The <code>CREATE</code> event indicates the load balancer was successfully
created on the specified IP address.</p>
</li>
<li>
<p>The most common error is running out of Google Cloud resource quota. To fix this problem,
you must either increase the quota for the relevant resource on your Google Cloud
project or delete some existing resources.</p>
</li>
</ul>
</li>
<li>
<p>Verify that a managed certificate resource is generated:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl describe -n istio-system managedcertificate gke-certificate
</span></span></code></pre></div><p>The status field should have information about the current status of the Certificate.
Eventually, certificate status should be <code>Active</code>.</p>
</li>
<li>
<p>Wait for the load balancer to report the back ends as healthy:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl describe -n istio-system ingress envoy-ingress
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Annotations:
</span></span><span class="line"><span class="cl"> kubernetes.io/ingress.global-static-ip-name:  kubeflow-ip
</span></span><span class="line"><span class="cl"> kubernetes.io/tls-acme:                       true
</span></span><span class="line"><span class="cl"> certmanager.k8s.io/issuer:                    letsencrypt-prod
</span></span><span class="line"><span class="cl"> ingress.kubernetes.io/backends:               {&#34;k8s-be-31380--5e1566252944dfdb&#34;:&#34;HEALTHY&#34;,&#34;k8s-be-32133--5e1566252944dfdb&#34;:&#34;HEALTHY&#34;}
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>Both backends should be reported as healthy.
It can take several minutes for the load balancer to consider the back ends
healthy.</p>
<p>The service with port <code>31380</code> is the one that handles Kubeflow
traffic. (31380 is the default port of the service <code>istio-ingressgateway</code>.)</p>
<p>If the backend is unhealthy, check the pods in <code>istio-system</code>:</p>
<ul>
<li><code>kubectl get pods -n istio-system</code></li>
<li>The <code>istio-ingressgateway-XX</code> pods should be running</li>
<li>Check the logs of pod <code>backend-updater-0</code>, <code>iap-enabler-XX</code> to see if there is any error</li>
<li>Follow the steps <a href="https://www.kubeflow.org/docs/distributions/gke/troubleshooting-gke/#502-server-error">here</a> to check the load balancer and backend service on Google Cloud.</li>
</ul>
</li>
<li>
<p>Try accessing Cloud IAP at the fully qualified domain name in your web
browser:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://&lt;your-fully-qualified-domain-name&gt;     
</span></span></code></pre></div><p>If you get SSL errors when you log in, this typically means that your SSL
certificate is still propagating. Wait a few minutes and try again. SSL
propagation can take up to 10 minutes.</p>
<p>If you do not see a login prompt and you get a 404 error, the configuration
of Cloud IAP is not yet complete. Keep retrying for up to 10 minutes.</p>
</li>
<li>
<p>If you get an error <code>Error: redirect_uri_mismatch</code> after logging in, this means the list of OAuth authorized redirect URIs does not include your domain.</p>
<p>The full error message looks like the following example and includes the 
relevant links:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">The redirect URI in the request, https://mykubeflow.endpoints.myproject.cloud.goog/_gcp_gatekeeper/authenticate, does not match the ones authorized for the OAuth client. 	
</span></span><span class="line"><span class="cl">To update the authorized redirect URIs, visit: https://console.developers.google.com/apis/credentials/oauthclient/22222222222-7meeee7a9a76jvg54j0g2lv8lrsb4l8g.apps.googleusercontent.com?project=22222222222	
</span></span></code></pre></div><p>Follow the link in the error message to find the OAuth credential being used
and add the redirect URI listed in the error message to the list of 
authorized URIs. For more information, read the guide to 
<a href="/docs/distributions/gke/deploy/oauth-setup/">setting up OAuth for Cloud IAP</a>.</p>
</li>
</ol>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>The <a href="/docs/distributions/gke/troubleshooting-gke/">GKE troubleshooting guide</a> for Kubeflow.</li>
<li>Guide to <a href="/docs/components/multi-tenancy/getting-started">sharing cluster access</a>.</li>
<li>Google Cloud guide to <a href="https://cloud.google.com/iap/docs/">Cloud IAP</a>.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-214814226b05ed257a32f69d832f8eaf">3.1.8 - Delete Kubeflow</h1>
    <div class="lead">Deleting Kubeflow from Google Cloud using the command line interface (CLI)</div>
	<p>This page explains how to delete your Kubeflow cluster or management cluster on
Google Cloud.</p>
<h2 id="before-you-start">Before you start</h2>
<p>This guide assumes the following settings:</p>
<ul>
<li>
<p>For Management cluster: The <code>${MGMT_PROJECT}</code>, <code>${MGMT_DIR}</code> and <code>${MGMT_NAME}</code> environment variables
are the same as in <a href="/docs/distributions/gke/deploy/management-setup#configure-environment-variables">Deploy Management cluster</a>.</p>
</li>
<li>
<p>For Kubeflow cluster: The <code>${KF_PROJECT}</code>, <code>${KF_NAME}</code> and <code>${MGMTCTXT}</code> environment variables
are the same as in <a href="../deploy-cli#environment-variables">Deploy Kubeflow cluster</a>.</p>
</li>
<li>
<p>The <code>${KF_DIR}</code> environment variable contains the path to
your Kubeflow application directory, which holds your Kubeflow configuration
files. For example, <code>/opt/kubeflow-distribution/kubeflow/</code>.</p>
</li>
</ul>
<h2 id="deleting-your-kubeflow-cluster">Deleting your Kubeflow cluster</h2>
<ol>
<li>
<p>To delete the applications running in the Kubeflow namespace, remove that namespace:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl delete namespace kubeflow
</span></span></code></pre></div></li>
<li>
<p>To delete the cluster and all Google Cloud resources, run the following commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">make delete
</span></span></code></pre></div><p><strong>Warning</strong>: this will delete the persistent disks storing metadata. If you want to preserve the disks, don&rsquo;t run this command;
instead selectively delete only those resources you want to delete.</p>
</li>
</ol>
<h2 id="clean-up-your-management-cluster">Clean up your management cluster</h2>
<p>The following instructions introduce how to clean up all resources created when
installing management cluster in management project, and when using management cluster to manage Google Cloud resources in managed Kubeflow projects.</p>
<h3 id="delete-or-keep-managed-google-cloud-resources">Delete or keep managed Google Cloud resources</h3>
<p>There are Google Cloud resources managed by Config Connector in the
management cluster after you deploy Kubeflow clusters with this management
cluster.</p>
<p>To delete all the managed Google Cloud resources, delete the managed project namespace:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl config use-context <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMTCTXT</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">kubectl delete namespace --wait <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div><p>To keep all the managed Google Cloud resources, you can <a href="#delete-management-cluster">delete the management
cluster</a> directly.</p>
<p>If you need fine-grained control, refer to
<a href="https://cloud.google.com/config-connector/docs/how-to/managing-deleting-resources#keeping_resources_after_deletion">Config Connector: Keeping resources after deletion</a>
for more details.</p>
<p>After deleting Config Connector resources for a managed project, you can revoke IAM permission
that let the management cluster manage the project:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud projects remove-iam-policy-binding <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span><span class="s2">&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    <span class="s2">&#34;--member=serviceAccount:</span><span class="si">${</span><span class="nv">MGMT_NAME</span><span class="si">}</span><span class="s2">-cnrm-system@</span><span class="si">${</span><span class="nv">MGMT_PROJECT</span><span class="si">}</span><span class="s2">.iam.gserviceaccount.com&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --role<span class="o">=</span>roles/owner
</span></span></code></pre></div><h3 id="delete-management-cluster">Delete management cluster</h3>
<p>To delete the Google service account and the management cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMT_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">make delete-cluster
</span></span></code></pre></div><p>Starting from Kubeflow v1.5, Google Cloud distribution has switched to Config Controller for Google-managed Management cluster. You can learn more detail by reading <a href="https://cloud.google.com/anthos-config-management/docs/how-to/config-controller-setup#delete_your">Delete your Config Controller</a>.</p>
<p>Note, after deleting the management cluster, all the managed Google Cloud
resources will be kept. You will be responsible for managing them by yourself.
If you want to delete the managed Google Cloud resources, make sure to delete resources in the <code>${KF_PROJECT}</code> namespace in the management cluster first.
You can learn more about the <code>${KF_PROJECT}</code> namespace in <code>kubeflow-distribution/kubeflow/kcc</code> folder.</p>
<p>You can create a management cluster to manage them again if you apply the same
Config Connector resources. Refer to <a href="https://cloud.google.com/config-connector/docs/how-to/managing-deleting-resources#acquiring_an_existing_resource">Managing and deleting resources - Acquiring an existing resource</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-b1bfa0ba85c77f3778011197ed05b225">3.1.9 - Deploy using UI</h1>
    <div class="lead">Instructions for using the UI to deploy Kubeflow on Google Cloud Platform (GCP)</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">No longer supported</h4>

    Starting with Kubeflow v1.1.0 deploying Kubeflow via the click to deploy web application
is no longer supported. Please <a href="/docs/gke/deploy/deploy-cli/">use kustomize and kpt</a> to deploy Kubeflow.

</div>


</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-91e76f8533117a222af5493748b5922e">3.2 - Pipelines on Google Cloud</h1>
    <div class="lead">Instructions for customizing and using Kubeflow Pipelines on Google Cloud</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-bf0071210c5937ae0322be8c4a921718">3.2.1 - Connecting to Kubeflow Pipelines on Google Cloud using the SDK</h1>
    <div class="lead">How to connect to different Kubeflow Pipelines installations on Google Cloud using the Kubeflow Pipelines SDK</div>
	<p>This guide describes how to connect to your Kubeflow Pipelines cluster on Google
Cloud using <a href="/docs/components/pipelines/sdk/sdk-overview/">the Kubeflow Pipelines SDK</a>.</p>
<h2 id="before-you-begin">Before you begin</h2>
<ul>
<li>You need a Kubeflow Pipelines deployment on Google Cloud using one of the <a href="/docs/components/pipelines/installation/overview/">installation options</a>.</li>
<li><a href="/docs/components/pipelines/sdk/install-sdk/">Install the Kubeflow Pipelines SDK</a>.</li>
</ul>
<h2 id="how-sdk-connects-to-kubeflow-pipelines-api">How SDK connects to Kubeflow Pipelines API</h2>
<p>Kubeflow Pipelines includes an API service named <code>ml-pipeline-ui</code>. The
<code>ml-pipeline-ui</code> API service is deployed in the same Kubernetes namespace you
deployed Kubeflow Pipelines in.</p>
<p>The Kubeflow Pipelines SDK can send REST API requests to this API service, but
the SDK needs to know the hostname to connect to the API service.</p>
<p>If the hostname can be accessed without authentication, it&rsquo;s very simple to
connect to it. For example, you can use <code>kubectl port-forward</code> to access it via
localhost:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># The Kubeflow Pipelines API service and the UI is available at</span>
</span></span><span class="line"><span class="cl"><span class="c1"># http://localhost:3000 without authentication check.</span>
</span></span><span class="line"><span class="cl">$ kubectl port-forward svc/ml-pipeline-ui 3000:80 --namespace kubeflow
</span></span><span class="line"><span class="cl"><span class="c1"># Change the namespace if you deployed Kubeflow Pipelines in a different</span>
</span></span><span class="line"><span class="cl"><span class="c1"># namespace.</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp</span>
</span></span><span class="line"><span class="cl"><span class="n">client</span> <span class="o">=</span> <span class="n">kfp</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s1">&#39;http://localhost:3000&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>When deploying Kubeflow Pipelines on Google Cloud, a public endpoint for this
API service is auto-configured for you, but this public endpoint has security
checks to protect your cluster from unauthorized access.</p>
<p>The following sections introduce how to authenticate your SDK requests to connect
to Kubeflow Pipelines via the public endpoint.</p>
<h2 id="connecting-to-kubeflow-pipelines-standalone-or-ai-platform-pipelines">Connecting to Kubeflow Pipelines standalone or AI Platform Pipelines</h2>
<p>Refer to <a href="https://cloud.google.com/ai-platform/pipelines/docs/connecting-with-sdk">Connecting to AI Platform Pipelines using the Kubeflow Pipelines SDK</a> for
both Kubeflow Pipelines standalone and AI Platform Pipelines.</p>
<p>Kubeflow Pipelines standalone deployments also show up in <a href="https://console.cloud.google.com/ai-platform/pipelines/clusters">AI Platform Pipelines</a>. They have the
name &ldquo;pipeline&rdquo; by default, but you can customize the name by overriding
<a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/sample/params.env#L1">the <code>appName</code> parameter in <code>params.env</code></a> when <a href="/docs/components/pipelines/installation/standalone-deployment/">deploying Kubeflow Pipelines standalone</a>.</p>
<h2 id="connecting-to-kubeflow-pipelines-in-a-full-kubeflow-deployment">Connecting to Kubeflow Pipelines in a full Kubeflow deployment</h2>
<p>A full Kubeflow deployment on Google Cloud uses an <a href="https://cloud.google.com/iap/docs">Identity-Aware Proxy (IAP)</a> to manage access to the public Kubeflow endpoint. The steps
below let you connect to Kubeflow Pipelines in a full Kubeflow deployment with
authentication through IAP.</p>
<ol>
<li>
<p>Find out your IAP OAuth 2.0 client ID.</p>
<p>You or your cluster admin followed <a href="https://www.kubeflow.org/docs/gke/deploy/oauth-setup/">Set up OAuth for Cloud IAP</a>
to deploy your full Kubeflow deployment on Google Cloud. You need the OAuth client
ID created in that step.</p>
<p>You can browse all of your existing OAuth client IDs <a href="https://console.cloud.google.com/apis/credentials">in the Credentials page of Google Cloud Console</a>.</p>
</li>
<li>
<p>Create another SDK OAuth Client ID for authenticating Kubeflow Pipelines SDK users.
Follow <a href="https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app">the steps to set up a client ID to authenticate from a desktop app</a>. Take
a note of the <strong>client ID</strong> and <strong>client secret</strong>. This client ID and secret can
be shared among all SDK users, because a separate login step is still needed below.</p>
</li>
<li>
<p>To connect to Kubeflow Pipelines public endpoint, initiate SDK client like the following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp</span>
</span></span><span class="line"><span class="cl"><span class="n">client</span> <span class="o">=</span> <span class="n">kfp</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s1">&#39;https://&lt;KF_NAME&gt;.endpoints.&lt;PROJECT&gt;.cloud.goog/pipeline&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">client_id</span><span class="o">=</span><span class="s1">&#39;&lt;AAAAAAAAAAAAAAAAAAAAAA&gt;.apps.googleusercontent.com&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">other_client_id</span><span class="o">=</span><span class="s1">&#39;&lt;BBBBBBBBBBBBBBBBBBB&gt;.apps.googleusercontent.com&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">other_client_secret</span><span class="o">=</span><span class="s1">&#39;&lt;CCCCCCCCCCCCCCCCCCCC&gt;&#39;</span><span class="p">)</span>
</span></span></code></pre></div><ul>
<li>Pass your <strong>IAP</strong> OAuth client ID found in <strong>step 1</strong> to <code>client_id</code> argument.</li>
<li>Pass your <strong>SDK</strong> OAuth client ID and secret created in <strong>step 2</strong> to <code>other_client_id</code>
and <code>other_client_secret</code> arguments.</li>
</ul>
</li>
<li>
<p>When you init the SDK client for the first time, you will be asked to log in.
The Kubeflow Pipelines SDK stores obtained credentials in <code>$HOME/.config/kfp/credentials.json</code>. You do not need to log in again unless you manually delete the credentials file.</p>
<pre><code>To use the SDK from cron tasks where you cannot log in manually, you can copy the credentials file in `$HOME/.config/kfp/credentials.json` to another machine.
However, you should keep the credentials safe and never expose it to
third parties.
</code></pre>
</li>
<li>
<p>After login, you can use the client.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">list_pipelines</span><span class="p">())</span>
</span></span></code></pre></div></li>
</ol>
<h2 id="troubleshooting">Troubleshooting</h2>
<ul>
<li>
<p>Error &ldquo;Failed to authorize with API resource references: there is no user identity header&rdquo; when using SDK methods.</p>
<p>Direct access to the API service without authentication works for Kubeflow
Pipelines standalone, AI Platform Pipelines, and Kubeflow 1.0 or earlier.</p>
<p>However, it fails authorization checks for Kubeflow Pipelines with multi-user
isolation in the full Kubeflow deployment starting from Kubeflow 1.1.
Multi-user isolation requires all API access to authenticate as a user. Refer to <a href="/docs/components/pipelines/overview/multi-user/#in-cluster-request-authentication">Kubeflow Pipelines Multi-user isolation documentation</a>
for more details.</p>
</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1d8bca1af0a199f092910444014260e4">3.2.2 - Authenticating Pipelines to Google Cloud</h1>
    <div class="lead">Authentication and authorization to Google Cloud in Pipelines</div>
	<p>This page describes authentication for Kubeflow Pipelines to Google Cloud.
Available options listed below have different tradeoffs. You should choose the one that fits your use-case.</p>
<ul>
<li>Configuring your cluster to access Google Cloud using <a href="#compute-engine-default-service-account">Compute Engine default service account</a> with the &ldquo;cloud-platform&rdquo; scope is easier to set up than the other options. However, this approach grants excessive permissions. Therefore, it is not suitable if you need workload permission separation.</li>
<li><a href="#workload-identity">Workload Identity</a> takes more efforts to set up, but allows fine-grained permission control. It is recommended for production use-cases.</li>
<li><a href="#google-service-account-keys-stored-as-kubernetes-secrets">Google service account keys stored as Kubernetes secrets</a> is the legacy approach and no longer recommended in GKE. However, it&rsquo;s the only option to use Google Cloud APIs when your cluster is an <a href="https://cloud.google.com/anthos">anthos</a> or on-prem cluster.</li>
</ul>
<h2 id="before-you-begin">Before you begin</h2>
<p>There are various options on how to install Kubeflow Pipelines in the <a href="/docs/components/pipelines/installation/overview/">Installation Options for Kubeflow Pipelines</a> guide.
Be aware that authentication support and cluster setup instructions will vary depending on the method you used to install Kubeflow Pipelines.</p>
<ul>
<li>For Kubeflow Pipelines standalone, you can compare and choose from all 3 options.</li>
<li>For full Kubeflow starting from Kubeflow 1.1, <a href="#workload-identity">Workload Identity</a> is the recommended and default option.</li>
<li>For AI Platform Pipelines, <a href="#compute-engine-default-service-account">Compute Engine default service account</a> is the only supported option.</li>
</ul>
<h2 id="compute-engine-default-service-account">Compute Engine default service account</h2>
<p>This is good for trying out Kubeflow Pipelines, because it is easy to set up.</p>
<p>However, it does not support permission separation for workloads in the cluster. <strong>Any workload</strong> in the cluster will be able to call <strong>any Google Cloud APIs</strong> in the chosen scope.</p>


<div class="alert alert-warning" role="alert">


    NOTE: Using pipelines with Compute Engine default service account is not supported in Full Kubeflow deployment.

</div>

<h3 id="cluster-setup-to-use-compute-engine-default-service-account">Cluster setup to use Compute Engine default service account</h3>
<p>By default, your GKE nodes use <a href="https://cloud.google.com/compute/docs/access/service-accounts#default_service_account">Compute Engine default service account</a>. If you allowed <code>cloud-platform</code> scope when creating the cluster,
Kubeflow Pipelines can authenticate to Google Cloud and manage resources in your project without further configuration.</p>
<p>Use one of the following options to create a GKE cluster that uses the Compute Engine default service account:</p>
<ul>
<li>If you followed instructions in <a href="https://cloud.google.com/ai-platform/pipelines/docs/setting-up">Setting up AI Platform Pipelines</a> and checked <code>Allow access to the following Cloud APIs</code>, your cluster is already using Compute Engine default service account.</li>
<li>In Google Cloud Console UI, you can enable it in <code>Create a Kubernetes cluster -&gt; default-pool -&gt; Security -&gt; Access Scopes -&gt; Allow full access to all Cloud APIs</code> like the following:
<img src="/docs/images/pipelines/v1/gke-allow-full-access.png"></li>
<li>Using <code>gcloud</code> CLI, you can enable it with <code>--scopes cloud-platform</code> like the following:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud container clusters create &lt;cluster-name&gt; <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --scopes cloud-platform
</span></span></code></pre></div><p>Please refer to <a href="https://cloud.google.com/sdk/gcloud/reference/container/clusters/create#--scopes">gcloud container clusters create command documentation</a> for other available options.</p>
<h3 id="authoring-pipelines-to-use-default-service-account">Authoring pipelines to use default service account</h3>
<p>Pipelines don&rsquo;t need any specific changes to authenticate to Google Cloud, it will use the default service account transparently.</p>
<p>However, you must update existing pipelines that use the <a href="https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.extensions.html#kfp.gcp.use_gcp_secret">use_gcp_secret kfp sdk operator</a>. Remove the <code>use_gcp_secret</code> usage to let your pipeline authenticate to Google Cloud using the default service account.</p>
<h2 id="securing-the-cluster-with-fine-grained-google-cloud-permission-control">Securing the cluster with fine-grained Google Cloud permission control</h2>
<h3 id="workload-identity">Workload Identity</h3>
<blockquote>
<p>Workload Identity is the recommended way for your GKE applications to consume services provided by Google APIs. You accomplish this by configuring a <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Kubernetes service account</a> to act as a <a href="https://cloud.google.com/iam/docs/service-accounts">Google service account</a>. Any Pods running as the Kubernetes service account then use the Google service account to authenticate to cloud services.</p>
</blockquote>
<p>Referenced from <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity Documentation</a>. Please read this doc for:</p>
<ul>
<li>A detailed introduction to Workload Identity.</li>
<li>Instructions to enable it on your cluster.</li>
<li>Whether its limitations affect your adoption.</li>
</ul>
<h4 id="terminology">Terminology</h4>
<p>This document distinguishes between <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Kubernetes service accounts</a> (KSAs) and <a href="https://cloud.google.com/iam/docs/service-accounts">Google service accounts</a> (GSAs). KSAs are Kubernetes resources, while GSAs are specific to Google Cloud. Other documentation usually refers to both of them as just &ldquo;service accounts&rdquo;.</p>
<h4 id="authoring-pipelines-to-use-workload-identity">Authoring pipelines to use Workload Identity</h4>
<p>Pipelines don&rsquo;t need any specific changes to authenticate to Google Cloud. With Workload Identity, pipelines run as the Google service account that is bound to the KSA.</p>
<p>However, existing pipelines that use <a href="https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.extensions.html#kfp.gcp.use_gcp_secret">use_gcp_secret kfp sdk operator</a> need to remove the <code>use_gcp_secret</code> usage to use the bound GSA.
You can also continue to use <code>use_gcp_secret</code> in a cluster with Workload Identity enabled and <code>use_gcp_secret</code> will take precedence for those workloads.</p>
<h4 id="cluster-setup-to-use-workload-identity-for-full-kubeflow">Cluster setup to use Workload Identity for Full Kubeflow</h4>
<p>Starting from Kubeflow 1.1, Kubeflow Pipelines <a href="/docs/components/pipelines/overview/multi-user/">supports multi-user isolation</a>. Therefore, pipeline runs are executed in user namespaces using the <code>default-editor</code> KSA. The <code>default-editor</code> KSA is auto-bound to the GSA specified in the user profile, which defaults to a shared GSA <code>${KFNAME}-user@${PROJECT}.iam.gserviceaccount.com</code>.</p>
<p>If you want to bind the <code>default-editor</code> KSA with a different GSA for a specific namespace, refer to the <a href="/docs/gke/authentication/#in-cluster-authentication">In-cluster authentication to Google Cloud</a> guide.</p>
<p>Additionally, the Kubeflow Pipelines UI, visualization, and TensorBoard server instances are deployed in your user namespace using the <code>default-editor</code> KSA. Therefore, to <a href="/docs/components/pipelines/sdk/output-viewer/">visualize results in the Pipelines UI</a>, they can fetch artifacts in Google Cloud Storage using permissions of the same GSA you configured for this namespace.</p>
<h4 id="cluster-setup-to-use-workload-identity-for-pipelines-standalone">Cluster setup to use Workload Identity for Pipelines Standalone</h4>
<h5 id="1-create-your-cluster-with-workload-identity-enabled">1. Create your cluster with Workload Identity enabled</h5>
<ul>
<li>
<p>In Google Cloud Console UI, you can enable Workload Identity in <code>Create a Kubernetes cluster -&gt; Security -&gt; Enable Workload Identity</code> like the following:
<img src="/docs/images/pipelines/v1/gke-enable-workload-identity.png"></p>
</li>
<li>
<p>Using <code>gcloud</code> CLI, you can enable it with:</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud beta container clusters create &lt;cluster-name&gt; <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --release-channel regular <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --workload-pool<span class="o">=</span>project-id.svc.id.goog
</span></span></code></pre></div><p>References:</p>
<ul>
<li>
<p><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#enable_workload_identity_on_a_new_cluster">Enable Workload Identity on a new cluster</a></p>
</li>
<li>
<p><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#enable_workload_identity_on_an_existing_cluster">Enable Workload Identity on an existing cluster</a></p>
</li>
</ul>
<h5 id="2-deploy-kubeflow-pipelines">2. Deploy Kubeflow Pipelines</h5>
<p>Deploy via <a href="/docs/components/pipelines/installation/overview/#kubeflow-pipelines-standalone">Pipelines Standalone</a> as usual.</p>
<h5 id="3-bind-workload-identities-for-ksas-used-by-kubeflow-pipelines">3. Bind Workload Identities for KSAs used by Kubeflow Pipelines</h5>
<p>The following helper bash scripts bind Workload Identities for KSAs used by Kubeflow Pipelines:</p>
<ul>
<li><a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/gcp-workload-identity-setup.sh">gcp-workload-identity-setup.sh</a> helps you create GSAs and bind them to KSAs used by pipelines workloads. This script provides an interactive command line dialog with explanation messages.</li>
<li><a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/manifests/kustomize/wi-utils.sh">wi-utils.sh</a> alternatively provides minimal utility bash functions that let you customize your setup. The minimal utilities make it easy to read and use programmatically.</li>
</ul>
<p>For example, to get a default setup using <code>gcp-workload-identity-setup.sh</code>, you can</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ curl -O https://raw.githubusercontent.com/kubeflow/pipelines/master/manifests/kustomize/gcp-workload-identity-setup.sh
</span></span><span class="line"><span class="cl">$ chmod +x ./gcp-workload-identity-setup.sh
</span></span><span class="line"><span class="cl">$ ./gcp-workload-identity-setup.sh
</span></span><span class="line"><span class="cl"># This prints the command&#39;s usage example and introduction.
</span></span><span class="line"><span class="cl"># Then you can run the command with required parameters.
</span></span><span class="line"><span class="cl"># Command output will tell you which GSAs and Workload Identity bindings have been
</span></span><span class="line"><span class="cl"># created.
</span></span></code></pre></div><h5 id="4-configure-iam-permissions-of-used-gsas">4. Configure IAM permissions of used GSAs</h5>
<p>If you used <code>gcp-workload-identity-setup.sh</code> to bind Workload Identities for your cluster, you can simply add the following IAM bindings:</p>
<ul>
<li>Give GSA <code>&lt;cluster-name&gt;-kfp-system@&lt;project-id&gt;.iam.gserviceaccount.com</code> <code>Storage Object Viewer</code> role to let UI load data in GCS in the same project.</li>
<li>Give GSA <code>&lt;cluster-name&gt;-kfp-user@&lt;project-id&gt;.iam.gserviceaccount.com</code> any permissions your pipelines need. For quick tryouts, you can give it <code>Project Editor</code> role for all permissions.</li>
</ul>
<p>If you configured bindings by yourself, here are Google Cloud permission requirements for KFP KSAs:</p>
<ul>
<li>Pipelines use <code>pipeline-runner</code> KSA. Configure IAM permissions of the GSA bound to this KSA to allow pipelines use Google Cloud APIs.</li>
<li>Pipelines UI uses <code>ml-pipeline-ui</code> KSA. Pipelines Visualization Server uses <code>ml-pipeline-visualizationserver</code> KSA. If you need to view artifacts and visualizations stored in Google Cloud Storage (GCS) from pipelines UI, you should add Storage Object Viewer permission (or the minimal required permission) to their bound GSAs.</li>
</ul>
<h3 id="google-service-account-keys-stored-as-kubernetes-secrets">Google service account keys stored as Kubernetes secrets</h3>
<p>It is recommended to use Workload Identity for easier and secure management, but you can also choose to use GSA keys.</p>
<h4 id="authoring-pipelines-to-use-gsa-keys">Authoring pipelines to use GSA keys</h4>
<p>Each pipeline step describes a
container that is run independently. If you want to grant access for a single step to use
one of your service accounts, you can use
<a href="https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.extensions.html#kfp.gcp.use_gcp_secret"><code>kfp.gcp.use_gcp_secret()</code></a>.
Examples for how to use this function can be found in the
<a href="https://github.com/kubeflow/examples/blob/871895c54402f68685c8e227c954d86a81c0575f/pipelines/mnist-pipelines/mnist_pipeline.py#L97">Kubeflow examples repo</a>.</p>
<h4 id="cluster-setup-to-use-use_gcp_secret-for-full-kubeflow">Cluster setup to use use_gcp_secret for Full Kubeflow</h4>
<p>From Kubeflow 1.1, there&rsquo;s no longer a <code>user-gcp-sa</code> secrets deployed for you. Recommend using Workload Identity instead.</p>
<p>For Kubeflow 1.0 or earlier, you don&rsquo;t need to do anything. Full Kubeflow deployment has already deployed the <code>user-gcp-sa</code> secret for you.</p>
<h4 id="cluster-setup-to-use-use_gcp_secret-for-pipelines-standalone">Cluster setup to use use_gcp_secret for Pipelines Standalone</h4>
<p>Pipelines Standalone require your manual setup for the <code>user-gcp-sa</code> secret used by <code>use_gcp_secret</code>.</p>
<p>Instructions to set up the secret:</p>
<ol>
<li>
<p>First download the GCE VM service account token (refer to <a href="https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys">Google Cloud documentation</a> for more information):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud iam service-accounts keys create application_default_credentials.json \
</span></span><span class="line"><span class="cl">  --iam-account [SA-NAME]@[PROJECT-ID].iam.gserviceaccount.com
</span></span></code></pre></div></li>
<li>
<p>Run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl create secret -n [your-namespace] generic user-gcp-sa \
</span></span><span class="line"><span class="cl">  --from-file=user-gcp-sa.json=application_default_credentials.json
</span></span></code></pre></div></li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-29022b460eea866cfda0585aa6967335">3.2.3 - Upgrading</h1>
    <div class="lead">How to upgrade your Kubeflow Pipelines deployment on Google Cloud</div>
	<h2 id="before-you-begin">Before you begin</h2>
<p>There are various options on how to install Kubeflow Pipelines in the <a href="/docs/components/pipelines/installation/overview/">Installation Options for Kubeflow Pipelines</a> guide. Be aware that upgrade support and instructions will vary depending on the method you used to install Kubeflow Pipelines.</p>
<h3 id="upgrade-related-feature-matrix">Upgrade-related feature matrix</h3>
<table>
<thead>
<tr>
<th>Installation \ Features</th>
<th>In-place upgrade</th>
<th>Reinstallation on the same cluster</th>
<th>Reinstallation on a different cluster</th>
<th>User customizations across upgrades (via <a href="https://kustomize.io/">Kustomize</a>)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Standalone</td>
<td>✅</td>
<td>⚠️ Data is deleted by default.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><a href="https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/manifests/kustomize/env/gcp">Standalone (managed storage)</a></td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
</tr>
<tr>
<td>full Kubeflow (&gt;= v1.1)</td>
<td>✅</td>
<td>✅</td>
<td>Needs documentation</td>
<td>✅</td>
</tr>
<tr>
<td>full Kubeflow (&lt; v1.1)</td>
<td></td>
<td>✅</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td>AI Platform Pipelines</td>
<td></td>
<td>✅</td>
<td></td>
<td></td>
</tr>
<tr>
<td>AI Platform Pipelines (managed storage)</td>
<td></td>
<td>✅</td>
<td>✅</td>
<td></td>
</tr>
</tbody>
</table>
<p>Notes:</p>
<ul>
<li>When you deploy Kubeflow Pipelines with managed storage on Google Cloud, you pipeline&rsquo;s metadata and artifacts are stored in <a href="https://cloud.google.com/storage/docs">Cloud Storage</a> and <a href="https://cloud.google.com/sql/docs">Cloud SQL</a>. Using managed storage makes it easier to manage, back up, and restore Kubeflow Pipelines data.</li>
</ul>
<h2 id="kubeflow-pipelines-standalone">Kubeflow Pipelines Standalone</h2>
<p>Upgrade Support for Kubeflow Pipelines Standalone is in <strong>Beta</strong>.</p>
<p><a href="/docs/components/pipelines/installation/standalone-deployment/#upgrading-kubeflow-pipelines">Upgrading Kubeflow Pipelines Standalone</a> introduces how to upgrade in-place.</p>
<h2 id="full-kubeflow">Full Kubeflow</h2>
<p>On Google Cloud, the full Kubeflow deployment follows <a href="https://googlecontainertools.github.io/kpt/guides/producer/packages/">the package pattern</a> starting from Kubeflow 1.1.</p>
<p>The package pattern enables you to upgrade the full Kubeflow in-place while keeping user customizations — refer to the <a href="/docs/gke/deploy/upgrade">Upgrade Kubeflow on Google Cloud</a> documentation for instructions.</p>
<p>However, there&rsquo;s no current support to upgrade from Kubeflow 1.0 or earlier to Kubeflow 1.1 while keeping Kubeflow Pipelines data. This may change in the future, so provide your feedback in <a href="https://github.com/kubeflow/pipelines/issues/4346">kubeflow/pipelines#4346</a> on GitHub.</p>
<h2 id="ai-platform-pipelines">AI Platform Pipelines</h2>
<p>Upgrade Support for AI Platform Pipelines is in <strong>Alpha</strong>.</p>


<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Warning</h4>

    Kubeflow Pipelines Standalone deployments also show up in the AI Platform Pipelines dashboard, DO NOT follow instructions below if you deployed Kubeflow Pipelines using standalone deployment.
Because data is deleted by default when a Kubeflow Pipelines Standalone deployment is deleted.

</div>

<p>Below are the steps that describe how to upgrade your AI Platform Pipelines instance while keeping existing data:</p>
<h3 id="for-instances-_without_-managed-storage">For instances <em>without</em> managed storage:</h3>
<ol>
<li><a href="https://cloud.google.com/ai-platform/pipelines/docs/getting-started#clean_up">Delete your AI Platform Pipelines instance</a> <strong>WITHOUT</strong> selecting <strong>Delete cluster</strong>. The persisted artifacts and database data are stored in persistent volumes in the cluster. They are kept by default when you do not delete the cluster.</li>
<li><a href="https://console.cloud.google.com/marketplace/details/google-cloud-ai-platform/kubeflow-pipelines">Reinstall Kubeflow Pipelines from the Google Cloud Marketplace</a> using the same <strong>Google Kubernetes Engine cluster</strong>, <strong>namespace</strong>, and <strong>application name</strong>. Persisted data will be automatically picked up during reinstallation.</li>
</ol>
<h3 id="for-instances-_with_-managed-storage">For instances <em>with</em> managed storage:</h3>
<ol>
<li><a href="https://cloud.google.com/ai-platform/pipelines/docs/getting-started#clean_up">Delete your AI Platform Pipelines instance</a>.</li>
<li>If you are upgrading from Kubeflow Pipelines 0.5.1, note that the Cloud Storage bucket is a required starting from 1.0.0. Previously deployed instances should be using a bucket named like &ldquo;<cloudsql instance connection name>-<database prefix or instance name>&rdquo;. Browse <a href="https://console.cloud.google.com/storage/browser">your Cloud Storage buckets</a> to find your existing bucket name and provide it in the next step.</li>
<li><a href="https://console.cloud.google.com/marketplace/details/google-cloud-ai-platform/kubeflow-pipelines">Reinstall Kubeflow Pipelines from the Google Cloud Marketplace</a> using the same application name and managed storage options as before. You can freely install it in any cluster and namespace (not necessarily the same as before), because persisted artifacts and database data are stored in managed storages (Cloud Storage and Cloud SQL), and will be automatically picked up during reinstallation.</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-0b1f7130f0e8cf9fc127b06695b4a7e5">3.2.4 - Enabling GPU and TPU</h1>
    <div class="lead">Enable GPU and TPU for Kubeflow Pipelines on Google Kubernetes Engine (GKE)</div>
	<p>This page describes how to enable GPU or TPU for a pipeline on GKE by using the Pipelines
DSL language.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>To enable GPU and TPU on your Kubeflow cluster, follow the instructions on how to
<a href="/docs/gke/customizing-gke#common-customizations">customize</a> the GKE cluster for Kubeflow before
setting up the cluster.</p>
<h2 id="configure-containerop-to-consume-gpus">Configure ContainerOp to consume GPUs</h2>
<p>After enabling the GPU, the Kubeflow setup script installs a default GPU pool with type nvidia-tesla-k80 with auto-scaling enabled.
The following code consumes 2 GPUs in a ContainerOp.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp.dsl</span> <span class="k">as</span> <span class="nn">dsl</span>
</span></span><span class="line"><span class="cl"><span class="n">gpu_op</span> <span class="o">=</span> <span class="n">dsl</span><span class="o">.</span><span class="n">ContainerOp</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;gpu-op&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">set_gpu_limit</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></div><p>The code above will be compiled into Kubernetes Pod spec:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">container</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="l">...</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;2&#34;</span><span class="w">
</span></span></span></code></pre></div><p>If the cluster has multiple node pools with different GPU types, you can specify the GPU type by the following code.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp.dsl</span> <span class="k">as</span> <span class="nn">dsl</span>
</span></span><span class="line"><span class="cl"><span class="n">gpu_op</span> <span class="o">=</span> <span class="n">dsl</span><span class="o">.</span><span class="n">ContainerOp</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;gpu-op&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">set_gpu_limit</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">gpu_op</span><span class="o">.</span><span class="n">add_node_selector_constraint</span><span class="p">(</span><span class="s1">&#39;cloud.google.com/gke-accelerator&#39;</span><span class="p">,</span> <span class="s1">&#39;nvidia-tesla-p4&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>The code above will be compiled into Kubernetes Pod spec:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">container</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="l">...</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;2&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">cloud.google.com/gke-accelerator</span><span class="p">:</span><span class="w"> </span><span class="l">nvidia-tesla-p4</span><span class="w">
</span></span></span></code></pre></div><p>See <a href="https://github.com/kubeflow/pipelines/tree/sdk/release-1.8/samples/tutorials/gpu">GPU tutorial</a> for a complete example to build a Kubeflow pipeline that uses GPUs.</p>
<p>Check the <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/gpus">GKE GPU guide</a> to learn more about GPU settings.</p>
<h2 id="configure-containerop-to-consume-tpus">Configure ContainerOp to consume TPUs</h2>
<p>Use the following code to configure ContainerOp to consume TPUs on GKE:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp.dsl</span> <span class="k">as</span> <span class="nn">dsl</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">kfp.gcp</span> <span class="k">as</span> <span class="nn">gcp</span>
</span></span><span class="line"><span class="cl"><span class="n">tpu_op</span> <span class="o">=</span> <span class="n">dsl</span><span class="o">.</span><span class="n">ContainerOp</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;tpu-op&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">gcp</span><span class="o">.</span><span class="n">use_tpu</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">tpu_cores</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">tpu_resource</span> <span class="o">=</span> <span class="s1">&#39;v2&#39;</span><span class="p">,</span> <span class="n">tf_version</span> <span class="o">=</span> <span class="s1">&#39;1.12&#39;</span><span class="p">))</span>
</span></span></code></pre></div><p>The above code uses 8 v2 TPUs with TF version to be 1.12. The code above will be compiled into Kubernetes Pod spec:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">container</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="l">...</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">cloud-tpus.google.com/v2</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;8&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">tf-version.cloud-tpus.google.com</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;1.12&#34;</span><span class="w">
</span></span></span></code></pre></div><p>To learn more, see an <a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/core/preemptible_tpu_gpu/preemptible_tpu_gpu.py">example pipeline that uses a preemptible node pool with TPU or GPU.</a>.</p>
<p>See the <a href="https://cloud.google.com/tpu/docs/kubernetes-engine-setup">GKE TPU Guide</a> to learn more about TPU settings.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-74493451fbef92c5f85e353ba719ee9d">3.2.5 - Using Preemptible VMs and GPUs on Google Cloud</h1>
    <div class="lead">Configuring preemptible VMs and GPUs for Kubeflow Pipelines on Google Cloud</div>
	<p>This document describes how to configure preemptible virtual machines
(<a href="https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms">preemptible VMs</a>)
and GPUs on preemptible VM instances
(<a href="https://cloud.google.com/compute/docs/instances/preemptible#preemptible_with_gpu">preemptible GPUs</a>)
for your workflows running on Kubeflow Pipelines on Google Cloud.</p>
<h2 id="introduction">Introduction</h2>
<p>Preemptible VMs are <a href="https://cloud.google.com/compute/docs/instances/">Compute Engine VM
instances</a> that last a maximum
of 24 hours and provide no availability guarantees. The
<a href="https://cloud.google.com/compute/pricing">pricing</a> of preemptible VMs is
lower than that of standard Compute Engine VMs.</p>
<p>GPUs attached to preemptible instances
(<a href="https://cloud.google.com/compute/docs/instances/preemptible#preemptible_with_gpu">preemptible GPUs</a>)
work like normal GPUs but persist only for the life of the instance.</p>
<p>Using preemptible VMs and GPUs can reduce costs on Google Cloud.
In addition to using preemptible VMs, your Google Kubernetes Engine (GKE)
cluster can autoscale based on current workloads.</p>
<p>This guide assumes that you have already deployed Kubeflow Pipelines. If not,
follow the guide to <a href="/docs/gke/deploy/">deploying Kubeflow on Google Cloud</a>.</p>
<h2 id="before-you-start">Before you start</h2>
<p>The variables defined in this page can be found in <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/env.sh">kubeflow-distribution/kubeflow/env.sh</a>. They are the same value as you set based on your <a href="/docs/distributions/gke/deploy/deploy-cli/#environment-variables">Kubeflow deployment</a>.</p>
<h2 id="using-preemptible-vms-with-kubeflow-pipelines">Using preemptible VMs with Kubeflow Pipelines</h2>
<p>In summary, the steps to schedule a pipeline to run on <a href="https://cloud.google.com/compute/docs/instances/preemptible">preemptible
VMs</a> are as
follows:</p>
<ol>
<li>Create a
<a href="https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools">node pool</a>
in your cluster that contains preemptible VMs.</li>
<li>Configure your pipelines to run on the preemptible VMs.</li>
</ol>
<p>The following sections contain more detail about the above steps.</p>
<h3 id="1-create-a-node-pool-with-preemptible-vms">1. Create a node pool with preemptible VMs</h3>
<p>Create a <code>preemptible-nodepool.yaml</code> as below and fulfill all placerholder content <code>KF_NAME</code>, <code>KF_PROJECT</code>, <code>LOCATION</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: container.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ContainerNodePool
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  labels:
</span></span><span class="line"><span class="cl">    kf-name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">  name: PREEMPTIBLE_CPU_POOL
</span></span><span class="line"><span class="cl">  namespace: KF_PROJECT # kpt-set: ${gcloud.core.project}
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  location: LOCATION # kpt-set: ${location}
</span></span><span class="line"><span class="cl">  initialNodeCount: 1
</span></span><span class="line"><span class="cl">  autoscaling:
</span></span><span class="line"><span class="cl">    minNodeCount: 0
</span></span><span class="line"><span class="cl">    maxNodeCount: 5
</span></span><span class="line"><span class="cl">  nodeConfig:
</span></span><span class="line"><span class="cl">    machineType: n1-standard-4
</span></span><span class="line"><span class="cl">    diskSizeGb: 100
</span></span><span class="line"><span class="cl">    diskType: pd-standard
</span></span><span class="line"><span class="cl">    preemptible: true
</span></span><span class="line"><span class="cl">    taint:
</span></span><span class="line"><span class="cl">    - effect: NO_SCHEDULE
</span></span><span class="line"><span class="cl">      key: preemptible
</span></span><span class="line"><span class="cl">      value: &#34;true&#34;
</span></span><span class="line"><span class="cl">    oauthScopes:
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/logging.write&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/monitoring&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/devstorage.read_only&#34;
</span></span><span class="line"><span class="cl">    serviceAccountRef:
</span></span><span class="line"><span class="cl">      external: KF_NAME-vm@KF_PROJECT.iam.gserviceaccount.com # kpt-set: ${name}-vm@${gcloud.core.project}.iam.gserviceaccount.com
</span></span><span class="line"><span class="cl">    metadata:
</span></span><span class="line"><span class="cl">      disable-legacy-endpoints: &#34;true&#34;
</span></span><span class="line"><span class="cl">  management:
</span></span><span class="line"><span class="cl">    autoRepair: true
</span></span><span class="line"><span class="cl">    autoUpgrade: true
</span></span><span class="line"><span class="cl">  clusterRef:
</span></span><span class="line"><span class="cl">    name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">    namespace: KF_PROJECT # kpt-set: ${name}
</span></span></code></pre></div><p>Where:</p>
<ul>
<li><code>PREEMPTIBLE_CPU_POOL</code> is the name of the node pool.</li>
<li><code>KF_NAME</code> is the name of the Kubeflow GKE cluster.</li>
<li><code>KF_PROJECT</code> is the name of your Kubeflow Google Cloud project.</li>
<li><code>LOCATION</code> is the region of this nodepool, for example: us-west1-b.</li>
<li><code>KF_NAME-vm@KF_PROJECT.iam.gserviceaccount.com</code> is your service account, replace the <code>KF_NAME</code> and <code>KF_PROJECT</code> using the value above  in this pattern, you can get vm service account you have already created in Kubeflow cluster deployment</li>
</ul>
<p>Apply the nodepool patch file above by running:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="si">${</span><span class="nv">MGMTCTXT</span><span class="si">}</span> --namespace<span class="o">=</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span> apply -f &lt;path-to-nodepool-file&gt;/preemptible-nodepool.yaml
</span></span></code></pre></div><h4 id="for-kubeflow-pipelines-standalone-only">For Kubeflow Pipelines standalone only</h4>
<p>Alternatively, if you are on Kubeflow Pipelines standalone, or AI Platform Pipelines, you can run this command to create node pool:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud container node-pools create PREEMPTIBLE_CPU_POOL \
</span></span><span class="line"><span class="cl">    --cluster=CLUSTER_NAME \
</span></span><span class="line"><span class="cl">      --enable-autoscaling --max-nodes=MAX_NODES --min-nodes=MIN_NODES \
</span></span><span class="line"><span class="cl">      --preemptible \
</span></span><span class="line"><span class="cl">      --node-taints=preemptible=true:NoSchedule \
</span></span><span class="line"><span class="cl">      --service-account=DEPLOYMENT_NAME-vm@PROJECT_NAME.iam.gserviceaccount.com
</span></span></code></pre></div><p>Below is an example of command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud container node-pools create preemptible-cpu-pool \
</span></span><span class="line"><span class="cl">  --cluster=user-4-18 \
</span></span><span class="line"><span class="cl">    --enable-autoscaling --max-nodes=4 --min-nodes=0 \
</span></span><span class="line"><span class="cl">    --preemptible \
</span></span><span class="line"><span class="cl">    --node-taints=preemptible=true:NoSchedule \
</span></span><span class="line"><span class="cl">    --service-account=user-4-18-vm@ml-pipeline-project.iam.gserviceaccount.com
</span></span></code></pre></div><h3 id="2-schedule-your-pipeline-to-run-on-the-preemptible-vms">2. Schedule your pipeline to run on the preemptible VMs</h3>
<p>After configuring a node pool with preemptible VMs, you must configure your
pipelines to run on the preemptible VMs.</p>
<p>In the <a href="/docs/components/pipelines/sdk/sdk-overview/">DSL code</a> for
your pipeline, add the following to the <code>ContainerOp</code> instance:</p>
<pre><code>.apply(gcp.use_preemptible_nodepool())
</code></pre>
<p>The above function works for both methods of generating the <code>ContainerOp</code>:</p>
<ul>
<li>The <code>ContainerOp</code> generated from
<a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/components/_python_op.py"><code>kfp.components.func_to_container_op</code></a>.</li>
<li>The <code>ContainerOp</code> generated from the task factory function, which is
loaded by <a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/components/_components.py"><code>components.load_component_from_url</code></a>.</li>
</ul>
<p><strong>Note</strong>:</p>
<ul>
<li>Call <code>.set_retry(#NUM_RETRY)</code> on your <code>ContainerOp</code> to retry
the task after the task is preempted.</li>
<li>If you modified the
<a href="https://cloud.google.com/kubernetes-engine/docs/how-to/node-taints">node taint</a>
when creating the node pool, pass the same node toleration to the
<code>use_preemptible_nodepool()</code> function.</li>
<li><code>use_preemptible_nodepool()</code> also accepts a parameter <code>hard_constraint</code>. When the <code>hard_constraint</code> is
<code>True</code>, the system will strictly schedule the task in preemptible VMs. When the <code>hard_constraint</code> is
<code>False</code>, the system will try to schedule the task in preemptible VMs. If it cannot find the preemptible VMs,
or the preemptible VMs are busy, the system will schedule the task in normal VMs.</li>
</ul>
<p>For example:</p>
<pre><code>import kfp.dsl as dsl
import kfp.gcp as gcp

class FlipCoinOp(dsl.ContainerOp):
  &quot;&quot;&quot;Flip a coin and output heads or tails randomly.&quot;&quot;&quot;

  def __init__(self):
    super(FlipCoinOp, self).__init__(
      name='Flip',
      image='python:alpine3.6',
      command=['sh', '-c'],
      arguments=['python -c &quot;import random; result = \'heads\' if random.randint(0,1) == 0 '
                 'else \'tails\'; print(result)&quot; | tee /tmp/output'],
      file_outputs={'output': '/tmp/output'})

@dsl.pipeline(
  name='pipeline flip coin',
  description='shows how to use dsl.Condition.'
)

def flipcoin():
  flip = FlipCoinOp().apply(gcp.use_preemptible_nodepool())

if __name__ == '__main__':
  import kfp.compiler as compiler
  compiler.Compiler().compile(flipcoin, __file__ + '.zip')
</code></pre>
<h2 id="using-preemptible-gpus-with-kubeflow-pipelines">Using preemptible GPUs with Kubeflow Pipelines</h2>
<p>This guide assumes that you have already deployed Kubeflow Pipelines. In
summary, the steps to schedule a pipeline to run with
<a href="https://cloud.google.com/compute/docs/instances/preemptible#preemptible_with_gpu">preemptible GPUs</a>
are as follows:</p>
<ol>
<li>Make sure you have enough GPU quota.</li>
<li>Create a node pool in your GKE cluster that contains preemptible VMs with
preemptible GPUs.</li>
<li>Configure your pipelines to run on the preemptible VMs with preemptible
GPUs.</li>
</ol>
<p>The following sections contain more detail about the above steps.</p>
<h3 id="1-make-sure-you-have-enough-gpu-quota">1. Make sure you have enough GPU quota</h3>
<p>Add GPU quota to your Google Cloud project. The <a href="https://cloud.google.com/compute/docs/gpus/#introduction">Google Cloud
documentation</a> lists
the availability of GPUs across regions. To check the available quota for
resources in your project, go to the
<a href="https://console.cloud.google.com/iam-admin/quotas">Quotas</a> page in the Google Cloud
Console.</p>
<h3 id="2-create-a-node-pool-of-preemptible-vms-with-preemptible-gpus">2. Create a node pool of preemptible VMs with preemptible GPUs</h3>
<p>Create a <code>preemptible-gpu-nodepool.yaml</code> as below and fulfill all placerholder content:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: container.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ContainerNodePool
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  labels:
</span></span><span class="line"><span class="cl">    kf-name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">  name: KF_NAME-containernodepool-gpu
</span></span><span class="line"><span class="cl">  namespace: KF_PROJECT # kpt-set: ${gcloud.core.project}
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  location: LOCATION # kpt-set: ${location}
</span></span><span class="line"><span class="cl">  initialNodeCount: 1
</span></span><span class="line"><span class="cl">  autoscaling:
</span></span><span class="line"><span class="cl">    minNodeCount: 0
</span></span><span class="line"><span class="cl">    maxNodeCount: 5
</span></span><span class="line"><span class="cl">  nodeConfig:
</span></span><span class="line"><span class="cl">    machineType: n1-standard-4
</span></span><span class="line"><span class="cl">    diskSizeGb: 100
</span></span><span class="line"><span class="cl">    diskType: pd-standard
</span></span><span class="line"><span class="cl">    preemptible: true
</span></span><span class="line"><span class="cl">    oauthScopes:
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/logging.write&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/monitoring&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/devstorage.read_only&#34;
</span></span><span class="line"><span class="cl">    serviceAccountRef:
</span></span><span class="line"><span class="cl">      external: KF_NAME-vm@KF_PROJECT.iam.gserviceaccount.com # kpt-set: ${name}-vm@${gcloud.core.project}.iam.gserviceaccount.com
</span></span><span class="line"><span class="cl">    guestAccelerator:
</span></span><span class="line"><span class="cl">    - type: &#34;nvidia-tesla-k80&#34;
</span></span><span class="line"><span class="cl">      count: 1
</span></span><span class="line"><span class="cl">    metadata:
</span></span><span class="line"><span class="cl">      disable-legacy-endpoints: &#34;true&#34;
</span></span><span class="line"><span class="cl">  management:
</span></span><span class="line"><span class="cl">    autoRepair: true
</span></span><span class="line"><span class="cl">    autoUpgrade: true
</span></span><span class="line"><span class="cl">  clusterRef:
</span></span><span class="line"><span class="cl">    name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">    namespace: KF_PROJECT # kpt-set: ${gcloud.core.project}
</span></span></code></pre></div><p>Where:</p>
<ul>
<li><code>PREEMPTIBLE_CPU_POOL</code> is the name of the node pool.</li>
<li><code>KF_NAME</code> is the name of the Kubeflow GKE cluster.</li>
<li><code>KF_PROJECT</code> is the name of your Kubeflow Google Cloud project.</li>
<li><code>LOCATION</code> is the region of this nodepool, for example: us-west1-b.</li>
<li><code>KF_NAME-vm@KF_PROJECT.iam.gserviceaccount.com</code> is your service account, replace the <code>KF_NAME</code> and <code>KF_PROJECT</code> using the value above  in this pattern, you can get vm service account you have already created in Kubeflow cluster deployment.</li>
</ul>
<h4 id="for-kubeflow-pipelines-standalone-only-1">For Kubeflow Pipelines standalone only</h4>
<p>Alternatively, if you are on Kubeflow Pipelines standalone, or AI Platform Pipelines, you can run this command to create node pool:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud container node-pools create PREEMPTIBLE_GPU_POOL \
</span></span><span class="line"><span class="cl">    --cluster=CLUSTER_NAME \
</span></span><span class="line"><span class="cl">    --enable-autoscaling --max-nodes=MAX_NODES --min-nodes=MIN_NODES \
</span></span><span class="line"><span class="cl">    --preemptible \
</span></span><span class="line"><span class="cl">    --node-taints=preemptible=true:NoSchedule \
</span></span><span class="line"><span class="cl">    --service-account=DEPLOYMENT_NAME-vm@PROJECT_NAME.iam.gserviceaccount.com \
</span></span><span class="line"><span class="cl">    --accelerator=type=GPU_TYPE,count=GPU_COUNT
</span></span></code></pre></div><p>Below is an example of command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud container node-pools create preemptible-gpu-pool \
</span></span><span class="line"><span class="cl">    --cluster=user-4-18 \
</span></span><span class="line"><span class="cl">    --enable-autoscaling --max-nodes=4 --min-nodes=0 \
</span></span><span class="line"><span class="cl">    --preemptible \
</span></span><span class="line"><span class="cl">    --node-taints=preemptible=true:NoSchedule \
</span></span><span class="line"><span class="cl">    --service-account=user-4-18-vm@ml-pipeline-project.iam.gserviceaccount.com \
</span></span><span class="line"><span class="cl">    --accelerator=type=nvidia-tesla-t4,count=2
</span></span></code></pre></div><h3 id="3-schedule-your-pipeline-to-run-on-the-preemptible-vms-with-preemptible-gpus">3. Schedule your pipeline to run on the preemptible VMs with preemptible GPUs</h3>
<p>In the <a href="/docs/components/pipelines/sdk/sdk-overview/">DSL code</a> for
your pipeline, add the following to the <code>ContainerOp</code> instance:</p>
<pre><code>.apply(gcp.use_preemptible_nodepool()
</code></pre>
<p>The above function works for both methods of generating the <code>ContainerOp</code>:</p>
<ul>
<li>The <code>ContainerOp</code> generated from
<a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/components/_python_op.py"><code>kfp.components.func_to_container_op</code></a>.</li>
<li>The <code>ContainerOp</code> generated from the task factory function, which is
loaded by <a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/sdk/python/kfp/components/_components.py"><code>components.load_component_from_url</code></a>.</li>
</ul>
<p><strong>Note</strong>:</p>
<ul>
<li>Call <code>.set_gpu_limit(#NUM_GPUs, GPU_VENDOR)</code> on your
<code>ContainerOp</code> to specify the GPU limit (for example, <code>1</code>) and vendor (for
example, <code>'nvidia'</code>).</li>
<li>Call <code>.set_retry(#NUM_RETRY)</code> on your <code>ContainerOp</code> to retry
the task after the task is preempted.</li>
<li>If you modified the
<a href="https://cloud.google.com/kubernetes-engine/docs/how-to/node-taints">node taint</a>
when creating the node pool, pass the same node toleration to the
<code>use_preemptible_nodepool()</code> function.</li>
<li><code>use_preemptible_nodepool()</code> also accepts a parameter <code>hard_constraint</code>. When the <code>hard_constraint</code> is
<code>True</code>, the system will strictly schedule the task in preemptible VMs. When the <code>hard_constraint</code> is
<code>False</code>, the system will try to schedule the task in preemptible VMs. If it cannot find the preemptible VMs,
or the preemptible VMs are busy, the system will schedule the task in normal VMs.</li>
</ul>
<p>For example:</p>
<pre><code>import kfp.dsl as dsl
import kfp.gcp as gcp

class FlipCoinOp(dsl.ContainerOp):
  &quot;&quot;&quot;Flip a coin and output heads or tails randomly.&quot;&quot;&quot;

  def __init__(self):
    super(FlipCoinOp, self).__init__(
      name='Flip',
      image='python:alpine3.6',
      command=['sh', '-c'],
      arguments=['python -c &quot;import random; result = \'heads\' if random.randint(0,1) == 0 '
                 'else \'tails\'; print(result)&quot; | tee /tmp/output'],
      file_outputs={'output': '/tmp/output'})

@dsl.pipeline(
  name='pipeline flip coin',
  description='shows how to use dsl.Condition.'
)

def flipcoin():
  flip = FlipCoinOp().set_gpu_limit(1, 'nvidia').apply(gcp.use_preemptible_nodepool())
if __name__ == '__main__':
  import kfp.compiler as compiler
  compiler.Compiler().compile(flipcoin, __file__ + '.zip')
</code></pre>
<h2 id="debugging">Debugging</h2>
<p>Run the following command if your nodepool didn&rsquo;t show up or has error during provisioning:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="si">${</span><span class="nv">MGMTCTXT</span><span class="si">}</span> --namespace<span class="o">=</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span> describe containernodepool -l kf-name<span class="o">=</span><span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span>
</span></span></code></pre></div><h2 id="next-steps">Next steps</h2>
<ul>
<li>Explore further options for <a href="/docs/gke/">customizing Kubeflow on Google Cloud</a>.</li>
<li>See how to <a href="/docs/components/pipelines/sdk/">build pipelines with the SDK</a>.</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5df0e440bd50642775398ffe0c30eff1">3.3 - Customize Kubeflow on GKE</h1>
    <div class="lead">Tailoring a GKE deployment of Kubeflow</div>
	<p>This guide describes how to customize your deployment of Kubeflow on Google
Kubernetes Engine (GKE) on Google Cloud.</p>
<h2 id="before-you-start">Before you start</h2>
<p>The variables defined in this page can be found in <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/master/kubeflow/env.sh">kubeflow-distribution/kubeflow/env.sh</a>. They are the same value as you set based on your <a href="/docs/distributions/gke/deploy/deploy-cli/#environment-variables">Kubeflow deployment</a>.</p>
<h2 id="customizing-kubeflow-before-deployment">Customizing Kubeflow before deployment</h2>
<p>The Kubeflow deployment process is divided into two steps, <strong>hydrate</strong> and
<strong>apply</strong>, so that you can modify your configuration before deploying your
Kubeflow cluster.</p>
<p>Follow the guide to <a href="/docs/gke/deploy/deploy-cli/">deploying Kubeflow on Google Cloud</a>. You can add your patches in corresponding component folder, and include those patches in <code>kustomization.yaml</code> file. Learn more about the usage of <a href="https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/">kustomize</a>. You can also find the existing kustomization in <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution">GoogleCloudPlatform/kubeflow-distribution</a> as example. After adding the patches, you can run <code>make hydrate</code> to validate the resulting resources. Finally, you can run <code>make apply</code> to deploy the customized Kubeflow.</p>
<h2 id="customizing-an-existing-deployment">Customizing an existing deployment</h2>
<p>You can also customize an existing Kubeflow deployment. In that case, this
guide assumes that you have already followed the guide to
<a href="/docs/gke/deploy/deploy-cli/">deploying Kubeflow on Google Cloud</a> and have deployed
Kubeflow to a GKE cluster.</p>
<h2 id="before-you-start-1">Before you start</h2>
<p>This guide assumes the following settings:</p>
<ul>
<li>
<p>The <code>${KF_DIR}</code> environment variable contains the path to
your Kubeflow application directory, which holds your Kubeflow configuration
files. For example, <code>/opt/kubeflow-distribution/kubeflow/</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export KF_DIR=&lt;path to your Kubeflow application directory&gt;
</span></span><span class="line"><span class="cl">cd &#34;${KF_DIR}&#34;
</span></span></code></pre></div></li>
<li>
<p>Make sure your environment variables are set up for the Kubeflow cluster you want to customize. For further background about the settings, see the guide to
<a href="/docs/gke/deploy/deploy-cli">deploying Kubeflow with the CLI</a>.</p>
</li>
</ul>
<h2 id="customizing-google-cloud-resources">Customizing Google Cloud resources</h2>
<p>To customize Google Cloud resources, such as your Kubernetes Engine cluster, you can
modify the Deployment settings starting in <code>${KF_DIR}/common/cnrm</code>.</p>
<p>This folder contains multiple dependencies on sibling directories for Google Cloud resources. So you can start from here by reviewing <code>kustomization.yaml</code>. Depends on the type of Google Cloud resources you want to customize, you can add patches in corresponding directory.</p>
<ol>
<li>
<p>Make sure you checkin the existing resources in <code>/build</code> folder to source control.</p>
</li>
<li>
<p>Add the patches in corresponding directory, and update <code>kustomization.yaml</code> to include patches.</p>
</li>
<li>
<p>Run <code>make hydrate</code> to build new resources in <code>/build</code> folder.</p>
</li>
<li>
<p>Carefully examine the result resources in <code>/build</code> folder. If the customization is addition only, you can run <code>make apply</code> to directly patch the resources.</p>
</li>
<li>
<p>It is possible that you are modifying immutable resources. In this case, you will need to delete existing resource and applying new resources. Note that this might mean lost of your service and data, please execute carefully. General approach to delete and deploy Google Cloud resources:</p>
<ol>
<li>
<p>Revert to old resources in <code>/build</code> using source control.</p>
</li>
<li>
<p>Carefully delete the resource you need to delete by using <code>kubectl delete</code>.</p>
</li>
<li>
<p>Rebuild and apply new Google Cloud resources</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> common/cnrm
</span></span><span class="line"><span class="cl"><span class="nv">NAME</span><span class="o">=</span><span class="k">$(</span>NAME<span class="k">)</span> <span class="nv">KFCTXT</span><span class="o">=</span><span class="k">$(</span>KFCTXT<span class="k">)</span> <span class="nv">LOCATION</span><span class="o">=</span><span class="k">$(</span>LOCATION<span class="k">)</span> <span class="nv">PROJECT</span><span class="o">=</span><span class="k">$(</span>PROJECT<span class="k">)</span> make apply
</span></span></code></pre></div></li>
</ol>
<h2 id="customizing-kubeflow-resources">Customizing Kubeflow resources</h2>
<p>You can use <a href="https://kustomize.io/">kustomize</a> to customize Kubeflow.
Make sure that you have the minimum required version of kustomize:
<b>2.0.3</b> or later. For more information about
kustomize in Kubeflow, see
<a href="/docs/methods/kfctl/kustomize/">how Kubeflow uses kustomize</a>.</p>
<p>To customize the Kubernetes resources running within the cluster, you can modify
the kustomize manifests in corresponding component under <code>${KF_DIR}</code>.</p>
<p>For example, to modify settings for the Jupyter web app:</p>
<ol>
<li>
<p>Open <code>${KF_DIR}/apps/jupyter/jupyter-web-app/kustomization.yaml</code> in a text editor.</p>
</li>
<li>
<p>Review the file&rsquo;s inclusion of <code>deployment-patch.yaml</code>, and add your modification to <code>deployment-patch.yaml</code> based on the original content in <code>${KF_DIR}/apps/jupyter/jupyter-web-app/upstream/base/deployment.yaml</code>. For example: change <code>volumeMounts</code>&rsquo;s <code>mountPath</code> if you need to customize it.</p>
</li>
<li>
<p>Verify the output resources in <code>/build</code> folder using <code>Makefile</code>&quot;</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">make hydrate
</span></span></code></pre></div></li>
<li>
<p>Redeploy Kubeflow using <code>Makefile</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">make apply
</span></span></code></pre></div></li>
</ol>
<h2 id="common-customizations">Common customizations</h2>
<h3 id="add-users-to-kubeflow">Add users to Kubeflow</h3>
<p>You must grant each user the minimal permission scope that allows them to
connect to the Kubernetes cluster.</p>
<p>For Google Cloud, you should grant the following Cloud Identity and Access Management (IAM) roles.</p>
<p>In the following commands, replace <code>[PROJECT]</code> with your Google Cloud project and replace <code>[EMAIL]</code> with the user&rsquo;s email address:</p>
<ul>
<li>
<p>To access the Kubernetes cluster, the user needs the <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/iam">Kubernetes Engine
Cluster Viewer</a>
role:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding [PROJECT] --member=user:[EMAIL] --role=roles/container.clusterViewer
</span></span></code></pre></div></li>
<li>
<p>To access the Kubeflow UI through IAP, the user needs the
<a href="https://cloud.google.com/iap/docs/managing-access">IAP-secured Web App User</a>
role:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding [PROJECT] --member=user:[EMAIL] --role=roles/iap.httpsResourceAccessor
</span></span></code></pre></div><p>Note, you need to grant the user <code>IAP-secured Web App User</code> role even if the user is already an owner or editor of the project. <code>IAP-secured Web App User</code> role is not implied by the <code>Project Owner</code> or <code>Project Editor</code> roles.</p>
</li>
<li>
<p>To be able to run <code>gcloud container clusters get-credentials</code> and see logs in Cloud Logging
(formerly Stackdriver), the user needs viewer access on the project:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding [PROJECT] --member=user:[EMAIL] --role=roles/viewer
</span></span></code></pre></div></li>
</ul>
<p>Alternatively, you can also grant these roles on the <a href="https://console.cloud.google.com/iam-admin/iam">IAM page in the Cloud Console</a>. Make sure you are in the same project as your Kubeflow deployment.</p>
<p><a id="gpu-config"></a></p>
<h3 id="add-gpu-nodes-to-your-cluster">Add GPU nodes to your cluster</h3>
<p>To add GPU accelerators to your Kubeflow cluster, you have the following
options:</p>
<ul>
<li>Pick a Google Cloud zone that provides NVIDIA Tesla K80 Accelerators
(<code>nvidia-tesla-k80</code>).</li>
<li>Or disable node-autoprovisioning in your Kubeflow cluster.</li>
<li>Or change your node-autoprovisioning configuration.</li>
</ul>
<p>To see which accelerators are available in each zone, run the following
command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud compute accelerator-types list
</span></span></code></pre></div><p>Create the <a href="https://cloud.google.com/config-connector/docs/reference/resource-docs/container/containernodepool">ContainerNodePool</a> resource adopting GPU, for exmaple, create a new file <code>containernodepool-gpu.yaml</code> file and fulfill the value <code>KUBEFLOW-NAME</code>, <code>KF-PROJECT</code>, <code>LOCATION</code> based on your <a href="/docs/distributions/gke/deploy/deploy-cli/#environment-variables">Kubeflow deployment</a>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: container.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ContainerNodePool
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  labels:
</span></span><span class="line"><span class="cl">    kf-name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">  name: containernodepool-gpu
</span></span><span class="line"><span class="cl">  namespace: KF_PROJECT # kpt-set: ${gcloud.core.project}
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  location: LOCATION # kpt-set: ${location}
</span></span><span class="line"><span class="cl">  initialNodeCount: 1
</span></span><span class="line"><span class="cl">  autoscaling:
</span></span><span class="line"><span class="cl">    minNodeCount: 0
</span></span><span class="line"><span class="cl">    maxNodeCount: 5
</span></span><span class="line"><span class="cl">  nodeConfig:
</span></span><span class="line"><span class="cl">    machineType: n1-standard-4
</span></span><span class="line"><span class="cl">    diskSizeGb: 100
</span></span><span class="line"><span class="cl">    diskType: pd-standard
</span></span><span class="line"><span class="cl">    preemptible: true
</span></span><span class="line"><span class="cl">    oauthScopes:
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/logging.write&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/monitoring&#34;
</span></span><span class="line"><span class="cl">    - &#34;https://www.googleapis.com/auth/devstorage.read_only&#34;
</span></span><span class="line"><span class="cl">    guestAccelerator:
</span></span><span class="line"><span class="cl">    - type: &#34;nvidia-tesla-k80&#34;
</span></span><span class="line"><span class="cl">      count: 1
</span></span><span class="line"><span class="cl">    metadata:
</span></span><span class="line"><span class="cl">      disable-legacy-endpoints: &#34;true&#34;
</span></span><span class="line"><span class="cl">  management:
</span></span><span class="line"><span class="cl">    autoRepair: true
</span></span><span class="line"><span class="cl">    autoUpgrade: true
</span></span><span class="line"><span class="cl">  clusterRef:
</span></span><span class="line"><span class="cl">    name: KF_NAME # kpt-set: ${name}
</span></span><span class="line"><span class="cl">    namespace: KF_PROJECT # kpt-set: ${gcloud.core.project}
</span></span></code></pre></div><p>Note that the <code>metadata:name</code> must be unique in your Kubeflow project. Because the management cluster uses this as ID and your Google Cloud project as a namespace to identify a node pool.</p>
<p>Apply the node pool patch file above by running:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">MGMTCTXT</span><span class="si">}</span><span class="s2">&#34;</span> --namespace<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_PROJECT</span><span class="si">}</span><span class="s2">&#34;</span> apply -f &lt;path-to-gpu-nodepool-file&gt;
</span></span></code></pre></div><p>After adding GPU nodes to your cluster, you need to install NVIDIA&rsquo;s device drivers to the nodes. Google provides a DaemonSet that automatically installs the drivers for you.
To deploy the installation DaemonSet, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl --context<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span><span class="s2">&#34;</span> apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
</span></span></code></pre></div><p>To disable node-autoprovisioning, edit <code>${KF_DIR}/common/cluster/upstream/cluster.yaml</code> to set
<a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/blob/v1.3.0/kubeflow/common/cluster/upstream/cluster.yaml#L30"><code>enabled</code></a>
to <code>false</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">    ...
</span></span><span class="line"><span class="cl">    clusterAutoscaling:
</span></span><span class="line"><span class="cl">      enabled: false
</span></span><span class="line"><span class="cl">      autoProvisioningDefaults:
</span></span><span class="line"><span class="cl">    ...
</span></span></code></pre></div><h3 id="add-cloud-tpus-to-your-cluster">Add Cloud TPUs to your cluster</h3>
<p>Note: The following instruction should be used when creating GKE cluster, because the TPU enablement flag <code>enableTpu</code> is immutable once cluster is created. You need to create new cluster if existing cluster doesn&rsquo;t have TPU enabled.</p>
<p>Set <a href="https://cloud.google.com/config-connector/docs/reference/resource-docs/container/containercluster"><code>enableTpu:true</code></a>
in <code>${KF_DIR}/common/cluster/upstream/cluster.yaml</code> and enable alias IP (VPC-native traffic routing):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: container.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ContainerCluster
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  ...
</span></span><span class="line"><span class="cl">  enableTpu: true
</span></span><span class="line"><span class="cl">  networkingMode: VPC_NATIVE
</span></span><span class="line"><span class="cl">  networkRef:
</span></span><span class="line"><span class="cl">    name: containercluster-dep-vpcnative
</span></span><span class="line"><span class="cl">  subnetworkRef:
</span></span><span class="line"><span class="cl">    name: containercluster-dep-vpcnative
</span></span><span class="line"><span class="cl">  ipAllocationPolicy:
</span></span><span class="line"><span class="cl">    servicesSecondaryRangeName: servicesrange
</span></span><span class="line"><span class="cl">    clusterSecondaryRangeName: clusterrange
</span></span><span class="line"><span class="cl">  ...
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">apiVersion: compute.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ComputeNetwork
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: containercluster-dep-vpcnative
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  routingMode: REGIONAL
</span></span><span class="line"><span class="cl">  autoCreateSubnetworks: false
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">apiVersion: compute.cnrm.cloud.google.com/v1beta1
</span></span><span class="line"><span class="cl">kind: ComputeSubnetwork
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: containercluster-dep-vpcnative
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  ipCidrRange: 10.2.0.0/16
</span></span><span class="line"><span class="cl">  region: us-west1
</span></span><span class="line"><span class="cl">  networkRef:
</span></span><span class="line"><span class="cl">    name: containercluster-dep-vpcnative
</span></span><span class="line"><span class="cl">  secondaryIpRange:
</span></span><span class="line"><span class="cl">  - rangeName: servicesrange
</span></span><span class="line"><span class="cl">    ipCidrRange: 10.3.0.0/16
</span></span><span class="line"><span class="cl">  - rangeName: clusterrange
</span></span><span class="line"><span class="cl">    ipCidrRange: 10.4.0.0/16
</span></span></code></pre></div><p>You can learn more at <a href="https://cloud.google.com/tpu/docs/kubernetes-engine-setup#new-cluster">Creating a new cluster with Cloud TPU support</a>, and view an example <a href="https://cloud.google.com/config-connector/docs/reference/resource-docs/container/containercluster">Vpc Native Container Cluster</a> config connector yaml file.</p>
<h2 id="more-customizations">More customizations</h2>
<p>Refer to the navigation panel on the left of these docs for more customizations,
including <a href="/docs/distributions/gke/custom-domain">using your own domain</a>,
<a href="/docs/distributions/gke/cloud-filestore">setting up Cloud Filestore</a>, and more.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7f39074f07a3befd5b88014d70e65b71">3.4 - Using Your Own Domain</h1>
    <div class="lead">Using a custom domain with Kubeflow on GKE</div>
	<p>This guide assumes you have already set up Kubeflow on Google Cloud. If you haven&rsquo;t done
so, follow the guide to
<a href="/docs/gke/deploy/">getting started with Kubeflow on Google Cloud</a>.</p>
<h2 id="using-your-own-domain">Using your own domain</h2>
<p>If you want to use your own domain instead of <strong>${KF_NAME}.endpoints.${PROJECT}.cloud.goog</strong>, follow these instructions after building your cluster:</p>
<ol>
<li>
<p>Remove the substitution <code>hostname</code> in the Kptfile.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kpt cfg delete-subst instance hostname
</span></span></code></pre></div></li>
<li>
<p>Create a new setter <code>hostname</code> in the Kptfile.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kpt cfg create-setter instance/ hostname --field &#34;data.hostname&#34; --value &#34;&#34;
</span></span></code></pre></div></li>
<li>
<p>Configure new setter with your own domain.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kpt cfg set ./instance hostname &lt;enter your domain here&gt;
</span></span></code></pre></div></li>
<li>
<p>Apply the changes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">make apply-kubeflow
</span></span></code></pre></div></li>
<li>
<p>Check Ingress to verify that your domain was properly configured.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe ingresses
</span></span></code></pre></div></li>
<li>
<p>Get the address of the static IP address created.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">IPNAME=${KF_NAME}-ip
</span></span><span class="line"><span class="cl">gcloud compute addresses describe ${IPNAME} --global
</span></span></code></pre></div></li>
<li>
<p>Use your DNS provider to map the fully qualified domain specified in the third step to the above IP address.</p>
</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-58c78f426cfb6021c769a20fdcc5669d">3.5 - Authenticating Kubeflow to Google Cloud</h1>
    <div class="lead">Authentication and authorization to Google Cloud</div>
	<p>This page describes in-cluster and local authentication for Kubeflow Google Cloud deployments.</p>
<h2 id="in-cluster-authentication">In-cluster authentication</h2>
<p>Starting from Kubeflow v0.6, you consume Kubeflow from custom namespaces (that is, namespaces other than <code>kubeflow</code>).
The <code>kubeflow</code> namespace is only for running Kubeflow system components. Individual jobs and model deployments
run in separate namespaces.</p>
<h3 id="google-kubernetes-engine-gke-workload-identity">Google Kubernetes Engine (GKE) workload identity</h3>
<p>Starting in v0.7, Kubeflow uses the new GKE feature: <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">workload identity</a>.
This is the recommended way to access Google Cloud APIs from your GKE cluster.
You can configure a Kubernetes service account (KSA) to act as a Google Cloud service account (GSA).</p>
<p>If you deployed Kubeflow following the Google Cloud instructions, then the profiler controller automatically binds the &ldquo;default-editor&rdquo; service account for every profile namespace to a default Google Cloud service account created during kubeflow deployment.
The Kubeflow deployment process also creates a default profile for the cluster admin.</p>
<p>For more info about profiles see the <a href="/docs/components/multi-tenancy/">Multi-user isolation</a> page.</p>
<p>Here is an example profile spec:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: kubeflow.org/v1beta1
</span></span><span class="line"><span class="cl">kind: Profile
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  plugins:
</span></span><span class="line"><span class="cl">  - kind: WorkloadIdentity
</span></span><span class="line"><span class="cl">    spec:
</span></span><span class="line"><span class="cl">      gcpServiceAccount: ${SANAME}@${PROJECT}.iam.gserviceaccount.com
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>You can verify that there is a KSA called default-editor and that it has an annotation of the corresponding GSA:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n ${PROFILE_NAME} describe serviceaccount default-editor
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">Name:        default-editor
</span></span><span class="line"><span class="cl">Annotations: iam.gke.io/gcp-service-account: ${KFNAME}-user@${PROJECT}.iam.gserviceaccount.com
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>You can double check that GSA is also properly set up:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud --project=${PROJECT} iam service-accounts get-iam-policy ${KFNAME}-user@${PROJECT}.iam.gserviceaccount.com
</span></span></code></pre></div><p>When a pod uses KSA default-editor, it can access Google Cloud APIs with the role granted to the GSA.</p>
<p><strong>Provisioning custom Google service accounts in namespaces</strong>:
When creating a profile, you can specify a custom Google Cloud service account for the namespace to control which Google Cloud resources are accessible.</p>
<p>Prerequisite: you must have permission to edit your Google Cloud project&rsquo;s IAM policy and to create a profile custom resource (CR) in your Kubeflow cluster.</p>
<ol>
<li>if you don&rsquo;t already have a Google Cloud service account you want to use, create a new one. For example: <code>user1-gcp@&lt;project-id&gt;.iam.gserviceaccount.com</code>:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud iam service-accounts create user1-gcp@&lt;project-id&gt;.iam.gserviceaccount.com
</span></span></code></pre></div><ol start="2">
<li>You can bind roles to the Google Cloud service account to allow access to the desired Google Cloud resources. For example to run BigQuery job, you can grant access like so:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding &lt;project-id&gt; \
</span></span><span class="line"><span class="cl">      --member=&#39;serviceAccount:user1-gcp@&lt;project-id&gt;.iam.gserviceaccount.com&#39; \
</span></span><span class="line"><span class="cl">      --role=&#39;roles/bigquery.jobUser&#39;
</span></span></code></pre></div><ol start="3">
<li><a href="https://cloud.google.com/sdk/gcloud/reference/iam/service-accounts/add-iam-policy-binding">Grant <code>owner</code> permission</a> of service account <code>user1-gcp@&lt;project-id&gt;.iam.gserviceaccount.com</code> to cluster account <code>&lt;cluster-name&gt;-admin@&lt;project-id&gt;.iam.gserviceaccount.com</code>:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud iam service-accounts add-iam-policy-binding \
</span></span><span class="line"><span class="cl">      user1-gcp@&lt;project-id&gt;.iam.gserviceaccount.com \
</span></span><span class="line"><span class="cl">      --member=&#39;serviceAccount:&lt;cluster-name&gt;-admin@&lt;project-id&gt;.iam.gserviceaccount.com&#39; --role=&#39;roles/owner&#39;
</span></span></code></pre></div><ol start="4">
<li>Manually create a profile for user1 and specify the Google Cloud service account to bind in <code>plugins</code> field:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow.org/v1beta1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Profile</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">profileName  </span><span class="w"> </span><span class="c"># replace with the name of the profile (the user&#39;s namespace name)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">owner</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">User</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">user1@email.com  </span><span class="w"> </span><span class="c"># replace with the email of the user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">plugins</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">WorkloadIdentity</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">gcpServiceAccount</span><span class="p">:</span><span class="w"> </span><span class="l">user1-gcp@project-id.iam.gserviceaccount.com</span><span class="w">
</span></span></span></code></pre></div><p><strong>Note:</strong>
The profile controller currently doesn&rsquo;t perform any access control checks to see whether the user creating the profile should be able to use the Google Cloud service account.
As a result, any user who can create a profile can get access to any service account for which the admin controller has owner permissions. We will improve this in subsequent releases.</p>
<p>You can find more details on workload identity in the <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">GKE documentation</a>.</p>
<h3 id="authentication-from-kubeflow-pipelines">Authentication from Kubeflow Pipelines</h3>
<p>Starting from Kubeflow v1.1, Kubeflow Pipelines <a href="/docs/components/pipelines/overview/multi-user/">supports multi-user isolation</a>. Therefore, pipeline runs are executed in user namespaces also using the <code>default-editor</code> KSA.</p>
<p>Additionally, the Kubeflow Pipelines UI, visualization, and TensorBoard server instances are deployed in your user namespace using the <code>default-editor</code> KSA. Therefore, to <a href="/docs/components/pipelines/sdk/output-viewer/">visualize results in the Pipelines UI</a>, they can fetch artifacts in Google Cloud Storage using permissions of the same GSA you configured for this namespace.</p>
<p>For more details, refer to <a href="/docs/gke/pipelines/authentication-pipelines/">Authenticating Pipelines to Google Cloud</a>.</p>
<hr>
<h2 id="local-authentication">Local authentication</h2>
<h3 id="gcloud">gcloud</h3>
<p>Use the <a href="https://cloud.google.com/sdk/gcloud/"><code>gcloud</code> tool</a> to interact with Google Cloud on the command line.
You can use the <code>gcloud</code> command to <a href="https://cloud.google.com/sdk/gcloud/reference/container/clusters/create">set up Google Kubernetes Engine (GKE) clusters</a>,
and interact with other Google services.</p>
<h5 id="logging-in">Logging in</h5>
<p>You have two options for authenticating the <code>gcloud</code> command:</p>
<ul>
<li>
<p>You can use a <strong>user account</strong> to authenticate using a Google account (typically Gmail).
You can register a user account using <a href="https://cloud.google.com/sdk/gcloud/reference/auth/login"><code>gcloud auth login</code></a>,
which brings up a browser window to start the familiar Google authentication flow.</p>
</li>
<li>
<p>You can create a <strong>service account</strong> within your Google Cloud project. You can then
<a href="https://cloud.google.com/iam/docs/creating-managing-service-account-keys">download a <code>.json</code> key file</a>
associated with the account, and run the
<a href="https://cloud.google.com/sdk/gcloud/reference/auth/activate-service-account"><code>gcloud auth activate-service-account</code></a>
command to authenticate your <code>gcloud</code> session.</p>
</li>
</ul>
<p>You can find more information in the <a href="https://cloud.google.com/sdk/docs/authorizing">Google Cloud docs</a>.</p>
<h5 id="listing-active-accounts">Listing active accounts</h5>
<p>You can run the following command to verify you are authenticating with the expected account:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud auth list
</span></span></code></pre></div><p>In the output of the command, an asterisk denotes your active account.</p>
<h5 id="viewing-iam-roles">Viewing IAM roles</h5>
<p>Permissions are handled in Google Cloud using <a href="https://cloud.google.com/iam/docs/understanding-roles">IAM Roles</a>.
These roles define which resources your account can read or write to. Provided you have the
<a href="https://cloud.google.com/iam/docs/understanding-custom-roles#required_permissions_and_roles_">necessary permissions</a>,
you can check which roles were assigned to your account using the following gcloud command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">PROJECT_ID=your-gcp-project-id-here
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">gcloud projects get-iam-policy $PROJECT_ID --flatten=&#34;bindings[].members&#34; \
</span></span><span class="line"><span class="cl">    --format=&#39;table(bindings.role)&#39; \
</span></span><span class="line"><span class="cl">    --filter=&#34;bindings.members:$(gcloud config list account --format &#39;value(core.account)&#39;)&#34;
</span></span></code></pre></div><p>You can view and modify roles through the
<a href="https://console.cloud.google.com/iam-admin/">Google Cloud IAM console</a>.</p>
<p>You can find more information about IAM in the
<a href="https://cloud.google.com/iam/docs/granting-changing-revoking-access">Google Cloud docs</a>.</p>
<hr>
<h3 id="kubectl">kubectl</h3>
<p>The <a href="https://kubernetes.io/docs/reference/kubectl/overview/"><code>kubectl</code> tool</a> is used for interacting with a Kubernetes cluster through the command line.</p>
<h5 id="connecting-to-a-cluster-using-a-google-cloud-account">Connecting to a cluster using a Google Cloud account</h5>
<p>If you set up your Kubernetes cluster using GKE, you can authenticate with the cluster using a Google Cloud account.
The following commands fetch the credentials for your cluster and save them to your local
<a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/"><code>kubeconfig</code> file</a>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">CLUSTER_NAME=your-gke-cluster
</span></span><span class="line"><span class="cl">ZONE=your-gcp-zone
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">gcloud container clusters get-credentials $CLUSTER_NAME --zone $ZONE
</span></span></code></pre></div><p>You can find more information in the
<a href="https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl">Google Cloud docs</a>.</p>
<h5 id="changing-active-clusters">Changing active clusters</h5>
<p>If you work with multiple Kubernetes clusters, you may have multiple contexts saved in your local
<a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/"><code>kubeconfig</code> file</a>.
You can view the clusters you have saved by run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl config get-contexts
</span></span></code></pre></div><p>You can view which cluster is currently being controlled by <code>kubectl</code> with the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">CONTEXT_NAME=your-new-context
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl config set-context $CONTEXT_NAME
</span></span></code></pre></div><p>You can find more information in the
<a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Kubernetes docs</a>.</p>
<h5 id="checking-rbac-permissions">Checking RBAC permissions</h5>
<p>Like GKE IAM, Kubernetes permissions are typically handled with a &ldquo;role-based authorization control&rdquo; (RBAC) system.
Each Kubernetes service account has a set of authorized roles associated with it. If your account doesn&rsquo;t have the
right roles assigned to it, certain tasks fail.</p>
<p>You can check if an account has the proper permissions to run a command by building a query structured as
<code>kubectl auth can-i [VERB] [RESOURCE] --namespace [NAMESPACE]</code>. For example, the following command verifies
that your account has permissions to create deployments in the <code>kubeflow</code> namespace:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl auth can-i create deployments --namespace kubeflow
</span></span></code></pre></div><p>You can find more information in the
<a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/">Kubernetes docs</a>.</p>
<h5 id="adding-rbac-permissions">Adding RBAC permissions</h5>
<p>If you find you are missing a permission you need, you can grant the missing roles to your service account using
Kubernetes resources.</p>
<ul>
<li><strong>Roles</strong> describe the permissions you want to assign. For example, <code>verbs: [&quot;create&quot;], resources:[&quot;deployments&quot;]</code></li>
<li><strong>RoleBindings</strong> define a mapping between the <code>Role</code>, and a specific service account</li>
</ul>
<p>By default, <code>Roles</code> and <code>RoleBindings</code> apply only to resources in a specific namespace, but there are also
<code>ClusterRoles</code> and <code>ClusterRoleBindings</code> that can grant access to resources cluster-wide</p>
<p>You can find more information in the
<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole">Kubernetes docs</a>.</p>
<h2 id="next-steps">Next steps</h2>
<p>See the <a href="/docs/gke/troubleshooting-gke/">troubleshooting guide</a> for help with diagnosing and fixing issues you may encounter with Kubeflow on Google Cloud</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-555bf215221d6c0d84a913c030f14a50">3.6 - Securing Your Clusters</h1>
    <div class="lead">How to secure Kubeflow clusters using private GKE</div>
	<p>Currently we are collecting interest for supporting private Kubeflow cluster deployment. Please upvote to <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/issues/267">Support private GKE cluster on Google Cloud</a> feature request if it fits your use case.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3f09966772a7188fe111e48915b76580">3.7 - Troubleshooting Deployments on GKE</h1>
    <div class="lead">Help fixing problems on GKE and Google Cloud</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<p>This guide helps diagnose and fix issues you may encounter with Kubeflow on
Google Kubernetes Engine (GKE) and Google Cloud.</p>
<h2 id="before-you-start">Before you start</h2>
<p>This guide covers troubleshooting specifically for
<a href="/docs/gke/deploy/">Kubeflow deployments on Google Cloud</a>.</p>
<p>For more help, try the
<a href="/docs/other-guides/troubleshooting">general Kubeflow troubleshooting guide</a>.</p>
<p>This guide assumes the following settings:</p>
<ul>
<li>
<p>The <code>${KF_DIR}</code> environment variable contains the path to
your Kubeflow application directory, which holds your Kubeflow configuration
files. For example, <code>/opt/kubeflow-distribution/kubeflow/</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export KF_DIR=&lt;path to your Kubeflow application directory&gt;
</span></span></code></pre></div></li>
<li>
<p>The <code>${CONFIG_FILE}</code> environment variable contains the path to your
Kubeflow configuration file.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export CONFIG_FILE=${KF_DIR}/kfctl_gcp_iap.v1.0.2.yaml
</span></span></code></pre></div><p>Or:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export CONFIG_FILE=${KF_DIR}/kfctl_gcp_basic_auth.v1.0.2.yaml
</span></span></code></pre></div></li>
<li>
<p>The <code>${KF_NAME}</code> environment variable contains the name of your Kubeflow
deployment. You can find the name in your <code>${CONFIG_FILE}</code>
configuration file, as the value for the <code>metadata.name</code> key.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export KF_NAME=&lt;the name of your Kubeflow deployment&gt;
</span></span></code></pre></div></li>
<li>
<p>The <code>${PROJECT}</code> environment variable contains the ID of your Google Cloud project.
You can find the project ID in
your <code>${CONFIG_FILE}</code> configuration file, as the value for the <code>project</code> key.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export PROJECT=&lt;your Google Cloud project ID&gt;
</span></span></code></pre></div></li>
<li>
<p>The <code>${ZONE}</code> environment variable contains the Google Cloud zone where your
Kubeflow resources are deployed.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export ZONE=&lt;your Google Cloud zone&gt;
</span></span></code></pre></div></li>
<li>
<p>For further background about the above settings, see the guide to
<a href="/docs/gke/deploy/deploy-cli">deploying Kubeflow with the CLI</a>.</p>
</li>
</ul>
<h2 id="troubleshooting-kubeflow-deployment-on-google-cloud">Troubleshooting Kubeflow deployment on Google Cloud</h2>
<p>Here are some tips for troubleshooting Google Cloud.</p>
<ul>
<li>Make sure you are a Google Cloud project owner.</li>
<li>Make sure you are using HTTPS.</li>
<li>Check project <a href="https://console.cloud.google.com/iam-admin/quotas">quota page</a> to see if any service&rsquo;s current usage reached quota limit, increase them as needed.</li>
<li>Check <a href="https://console.cloud.google.com/deployments">deployment manager page</a> and see if there’s a failed deployment.</li>
<li>Check if endpoint is up: do <a href="https://mxtoolbox.com/DNSLookup.aspx">DNS lookup</a> against your Cloud Identity-Aware Proxy (Cloud IAP) URL and see if it resolves to the correct IP address.</li>
<li>Check if certificate succeeded: <code>kubectl describe certificates -n istio-system</code> should give you certificate status.</li>
<li>Check ingress status: <code>kubectl describe ingress -n istio-system</code></li>
<li>Check if <a href="https://console.cloud.google.com/endpoints">endpoint entry</a> is created. There should be one entry with name <code>&lt;deployment&gt;.endpoints.&lt;project&gt;.cloud.goog</code>
<ul>
<li>If endpoint entry doesn&rsquo;t exist, check <code>kubectl describe cloudendpoint -n istio-system</code></li>
</ul>
</li>
<li>If using IAP: make sure you <a href="/docs/gke/deploy/oauth-setup/">added</a> <code>https://&lt;deployment&gt;.endpoints.&lt;project&gt;.cloud.goog/_gcp_gatekeeper/authenticate</code>
as an authorized redirect URI for the OAUTH credentials used to create the deployment.</li>
<li>If using IAP: see the guide to
<a href="/docs/gke/deploy/monitor-iap-setup/">monitoring your Cloud IAP setup</a>.</li>
<li>See the sections below for troubleshooting specific problems.</li>
<li>Please <a href="https://github.com/kubeflow/kubeflow/issues/new?template=bug_report.md">report a bug</a> if you can&rsquo;t resolve the problem by following the above steps.</li>
</ul>
<h3 id="dns-name-not-registered">DNS name not registered</h3>
<p>This section provides troubleshooting information for problems creating a DNS entry for your <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">ingress</a>. The ingress is a K8s resource
that creates a Google Cloud loadbalancer to enable http(s) access to Kubeflow web services from outside
the cluster. This section assumes
you are using <a href="https://cloud.google.com/endpoints/">Cloud Endpoints</a> and a DNS name of the following pattern</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://${KF_NAME}.endpoints.${PROJECT}.cloud.goog
</span></span></code></pre></div><p>Symptoms:</p>
<ul>
<li>
<p>When you access the URL in Chrome you get the error: <strong>server IP address could not be found</strong></p>
</li>
<li>
<p>nslookup for the domain name doesn&rsquo;t return the IP address associated with the ingress</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">nslookup ${KF_NAME}.endpoints.${PROJECT}.cloud.goog
</span></span><span class="line"><span class="cl">Server:   127.0.0.1
</span></span><span class="line"><span class="cl">Address:  127.0.0.1#53
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">** server can&#39;t find ${KF_NAME}.endpoints.${PROJECT}.cloud.goog: NXDOMAIN
</span></span></code></pre></div></li>
</ul>
<p>Troubleshooting</p>
<ol>
<li>
<p>Check the <code>cloudendpoints</code> resource</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get cloudendpoints -o yaml ${KF_NAME}
</span></span><span class="line"><span class="cl">kubectl describe cloudendpoints ${KF_NAME}
</span></span></code></pre></div><ul>
<li>Check if there are errors indicating problems creating the endpoint</li>
</ul>
</li>
<li>
<p>The status of the <code>cloudendpoints</code> object will contain the cloud operation used to register the operation</p>
<ul>
<li>
<p>For example</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"> status:
</span></span><span class="line"><span class="cl">   config: &#34;&#34;
</span></span><span class="line"><span class="cl">   configMapHash: &#34;&#34;
</span></span><span class="line"><span class="cl">   configSubmit: operations/serviceConfigs.jlewi-1218-001.endpoints.cloud-ml-dev.cloud.goog:43fe6c6f-eb9c-41d0-ac85-b547fc3e6e38
</span></span><span class="line"><span class="cl">   endpoint: jlewi-1218-001.endpoints.cloud-ml-dev.cloud.goog
</span></span><span class="line"><span class="cl">   ingressIP: 35.227.243.83
</span></span><span class="line"><span class="cl">   jwtAudiences: null
</span></span><span class="line"><span class="cl">   lastAppliedSig: 4f3b903a06a683b380bf1aac1deca72792472429
</span></span><span class="line"><span class="cl">   observedGeneration: 1
</span></span><span class="line"><span class="cl">   stateCurrent: ENDPOINT_SUBMIT_PENDING
</span></span></code></pre></div></li>
</ul>
</li>
</ol>
<ul>
<li>
<p>You can check the status of the operation by running:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud --project=${PROJECT} endpoints operations describe ${OPERATION}
</span></span></code></pre></div><ul>
<li>Operation is everything after <code>operations/</code> in the <code>configSubmit</code> field</li>
</ul>
</li>
</ul>
<h3 id="404-page-not-found-when-accessing-central-dashboard">404 Page Not Found When Accessing Central Dashboard</h3>
<p>This section provides troubleshooting information for 404s, page not found, being return by the central dashboard which is served at</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://${KUBEFLOW_FQDN}/
</span></span></code></pre></div><ul>
<li><em><strong>KUBEFLOW_FQDN</strong></em> is your project&rsquo;s OAuth web app URI domain name <code>&lt;name&gt;.endpoints.&lt;project&gt;.cloud.goog</code></li>
<li>Since we were able to sign in this indicates the Ambassador reverse proxy is up and healthy we can confirm this is the case by running the following command</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n ${NAMESPACE} get pods -l service=envoy
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">NAME                     READY     STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">envoy-76774f8d5c-lx9bd   2/2       Running   2          4m
</span></span><span class="line"><span class="cl">envoy-76774f8d5c-ngjnr   2/2       Running   2          4m
</span></span><span class="line"><span class="cl">envoy-76774f8d5c-sg555   2/2       Running   2          4m
</span></span></code></pre></div><ul>
<li>
<p>Try other services to see if they&rsquo;re accessible for example</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://${KUBEFLOW_FQDN}/whoami
</span></span><span class="line"><span class="cl">https://${KUBEFLOW_FQDN}/tfjobs/ui
</span></span><span class="line"><span class="cl">https://${KUBEFLOW_FQDN}/hub
</span></span></code></pre></div></li>
<li>
<p>If other services are accessible then we know its a problem specific to the central dashboard and not ingress</p>
</li>
<li>
<p>Check that the centraldashboard is running</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get pods -l app=centraldashboard
</span></span><span class="line"><span class="cl">NAME                                READY     STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">centraldashboard-6665fc46cb-592br   1/1       Running   0          7h
</span></span></code></pre></div></li>
<li>
<p>Check a service for the central dashboard exists</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get service -o yaml centraldashboard
</span></span></code></pre></div></li>
<li>
<p>Check that an Ambassador route is properly defined</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get service centraldashboard -o jsonpath=&#39;{.metadata.annotations.getambassador\.io/config}&#39;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">apiVersion: ambassador/v0
</span></span><span class="line"><span class="cl">  kind:  Mapping
</span></span><span class="line"><span class="cl">  name: centralui-mapping
</span></span><span class="line"><span class="cl">  prefix: /
</span></span><span class="line"><span class="cl">  rewrite: /
</span></span><span class="line"><span class="cl">  service: centraldashboard.kubeflow,
</span></span></code></pre></div></li>
<li>
<p>Check the logs of Ambassador for errors. See if there are errors like the following indicating
an error parsing the route.If you are using the new Stackdriver Kubernetes monitoring you can use the following filter in the <a href="https://console.cloud.google.com/logs/viewer">stackdriver console</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"> resource.type=&#34;k8s_container&#34;
</span></span><span class="line"><span class="cl"> resource.labels.location=${ZONE}
</span></span><span class="line"><span class="cl"> resource.labels.cluster_name=${CLUSTER}
</span></span><span class="line"><span class="cl"> metadata.userLabels.service=&#34;ambassador&#34;
</span></span><span class="line"><span class="cl">&#34;could not parse YAML&#34;
</span></span></code></pre></div></li>
</ul>
<h3 id="502-server-error">502 Server Error</h3>
<p>A 502 usually means traffic isn&rsquo;t even making it to the envoy reverse proxy. And it
usually indicates the loadbalancer doesn&rsquo;t think any backends are healthy.</p>
<ul>
<li>In Cloud Console select Network Services -&gt; Load Balancing
<ul>
<li>
<p>Click on the load balancer (the name should contain the name of the ingress)</p>
</li>
<li>
<p>The exact name can be found by looking at the <code>ingress.kubernetes.io/url-map</code> annotation on your ingress object</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">URLMAP=$(kubectl --namespace=${NAMESPACE} get ingress envoy-ingress -o jsonpath=&#39;{.metadata.annotations.ingress\.kubernetes\.io/url-map}&#39;)
</span></span><span class="line"><span class="cl">echo ${URLMAP}
</span></span></code></pre></div></li>
<li>
<p>Click on your loadbalancer</p>
</li>
<li>
<p>This will show you the backend services associated with the load balancer</p>
<ul>
<li>
<p>There is 1 backend service for each K8s service the ingress rule routes traffic too</p>
</li>
<li>
<p>The named port will correspond to the NodePort a service is using</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">NODE_PORT=$(kubectl --namespace=${NAMESPACE} get svc envoy -o jsonpath=&#39;{.spec.ports[0].nodePort}&#39;)
</span></span><span class="line"><span class="cl">BACKEND_NAME=$(gcloud compute --project=${PROJECT} backend-services list --filter=name~k8s-be-${NODE_PORT}- --format=&#39;value(name)&#39;)
</span></span><span class="line"><span class="cl">gcloud compute --project=${PROJECT} backend-services get-health --global ${BACKEND_NAME}
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>Make sure the load balancer reports the backends as healthy</p>
<ul>
<li>
<p>If the backends aren&rsquo;t reported as healthy check that the pods associated with the K8s service are up and running</p>
</li>
<li>
<p>Check that health checks are properly configured</p>
<ul>
<li>Click on the health check associated with the backend service for envoy</li>
<li>Check that the path is /healthz and corresponds to the path of the readiness probe on the envoy pods</li>
<li>See <a href="https://github.com/kubernetes-retired/contrib/tree/master/ingress/controllers/gce/examples/health_checks">K8s docs</a> for important information about how health checks are determined from readiness probes.</li>
</ul>
</li>
<li>
<p>Check firewall rules to ensure traffic isn&rsquo;t blocked from the Google Cloud loadbalancer</p>
<ul>
<li>
<p>The firewall rule should be added automatically by the ingress but its possible it got deleted if you have some automatic firewall policy enforcement. You can recreate the firewall rule if needed with a rule like this</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud compute firewall-rules create $NAME \
</span></span><span class="line"><span class="cl">--project $PROJECT \
</span></span><span class="line"><span class="cl">--allow tcp:$PORT \
</span></span><span class="line"><span class="cl">--target-tags $NODE_TAG \
</span></span><span class="line"><span class="cl">--source-ranges 130.211.0.0/22,35.191.0.0/16
</span></span></code></pre></div></li>
<li>
<p>To get the node tag</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># From the Kubernetes Engine cluster get the name of the managed instance group
</span></span><span class="line"><span class="cl">gcloud --project=$PROJECT container clusters --zone=$ZONE describe $CLUSTER
</span></span><span class="line"><span class="cl"># Get the template associated with the MIG
</span></span><span class="line"><span class="cl">gcloud --project=kubeflow-rl compute instance-groups managed describe --zone=${ZONE} ${MIG_NAME}
</span></span><span class="line"><span class="cl"># Get the instance tags from the template
</span></span><span class="line"><span class="cl">gcloud --project=kubeflow-rl compute instance-templates describe ${TEMPLATE_NAME}
</span></span></code></pre></div><p>For more info <a href="https://cloud.google.com/compute/docs/load-balancing/health-checks">see Google Cloud HTTP health check docs</a></p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>In Stackdriver Logging look at the Cloud Http Load Balancer logs</p>
<ul>
<li>Logs are labeled with the forwarding rule</li>
<li>The forwarding rules are available via the annotations on the ingress
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ingress.kubernetes.io/forwarding-rule
</span></span><span class="line"><span class="cl">ingress.kubernetes.io/https-forwarding-rule
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>Verify that requests are being properly routed within the cluster</p>
</li>
<li>
<p>Connect to one of the envoy proxies</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl exec -ti `kubectl get pods --selector=service=envoy -o jsonpath=&#39;{.items[0].metadata.name}&#39;` /bin/bash
</span></span></code></pre></div></li>
<li>
<p>Install curl in the pod</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apt-get update &amp;&amp; apt-get install -y curl
</span></span></code></pre></div></li>
<li>
<p>Verify access to the whoami app</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">curl -L -s -i http://envoy:8080/noiap/whoami
</span></span></code></pre></div></li>
<li>
<p>If this doesn&rsquo;t return a 200 OK response; then there is a problem with the K8s resources</p>
<ul>
<li>Check the pods are running</li>
<li>Check services are pointing at the points (look at the endpoints for the various services)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="gke-certificate-fails-to-be-provisioned">GKE Certificate Fails To Be Provisioned</h3>
<p>A common symptom of your certificate failing to be provisioned is SSL errors like <code>ERR_SSL_VERSION_OR_CIPHER_MISMATCH</code> when
you try to access the Kubeflow https endpoint.</p>
<p>To troubleshoot check the status of your GKE managed certificate</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe managedcertificate
</span></span></code></pre></div><p>If the certificate is in status <code>FailedNotVisible</code> then it means Google Cloud failed to provision the certificate
because it could not verify that you owned the domain by doing an ACME challenge. In order for Google Cloud to provision your certificate</p>
<ol>
<li>Your ingress must be created in order to associated a Google Cloud Load Balancer(GCLB) with the IP address for your endpoint</li>
<li>There must be a DNS entry mapping your domain name to the IP.</li>
</ol>
<p>If there is a problem preventing either of the above then Google Cloud will be unable to provision your certificate
and eventually enter the permanent failure state <code>FailedNotVisible</code> indicating your endpoint isn&rsquo;t accessible. The most common
cause is the ingress can&rsquo;t be created because the K8s secret containing OAuth credentials doesn&rsquo;t
exist.</p>
<p>To fix this you must first resolve the underlying problems preventing your ingress or DNS entry from being created.
Once the underlying problem has been fixed you can follow the steps below to force a new certificate to be
generated.</p>
<p>You can fix the certificate by performing the following steps to delete the existing certificate and create a new one.</p>
<ol>
<li>
<p>Get the name of the Google Cloud certificate</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe managedcertificate gke-certificate
</span></span></code></pre></div><ul>
<li>The status will contain <code>Certificate Name</code> which will start with <code>mcrt</code> make a note of this.</li>
</ul>
</li>
<li>
<p>Delete the ingress</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system delete ingress envoy-ingress
</span></span></code></pre></div></li>
<li>
<p>Ensure the certificate was deleted</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud --project=${PROJECT} compute ssl-certificates list
</span></span></code></pre></div><ul>
<li>Make sure the certificate obtained in the first step no longer exists</li>
</ul>
</li>
<li>
<p>Reapply kubeflow in order to recreate the ingress and certificate</p>
<ul>
<li>If you deployed with <code>kfctl</code> rerun <code>kfctl apply</code></li>
<li>If you deployed using the Google Cloud blueprint rerun <code>make apply-kubeflow</code></li>
</ul>
</li>
<li>
<p>Monitor the certificate to make sure it can be provisioned</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl --context=gcp-private-0527 -n istio-system describe managedcertificate gke-certificate
</span></span></code></pre></div></li>
<li>
<p>Since the ingress has been recreated we need to restart the pods that configure it</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system delete pods -l service=backend-updater
</span></span><span class="line"><span class="cl">kubectl -n istio-system delete pods -l service=iap-enabler
</span></span></code></pre></div></li>
</ol>
<h3 id="problems-with-ssl-certificate-from-lets-encrypt">Problems with SSL certificate from Let&rsquo;s Encrypt</h3>
<p>As of Kubeflow 1.0, Kubeflow should be using GKE Managed Certificates and no longer using Let&rsquo;s Encrypt.</p>
<p>See the guide to
<a href="/docs/gke/deploy/monitor-iap-setup/">monitoring your Cloud IAP setup</a>.</p>
<h2 id="envoy-pods-crash-looping-root-cause-is-backend-quota-exceeded">Envoy pods crash-looping: root cause is backend quota exceeded</h2>
<p>If your logs show the
<a href="https://istio.io/docs/concepts/what-is-istio/#envoy">Envoy</a> pods crash-looping,
the root cause may be that you have exceeded your quota for some
backend services such as loadbalancers.
This is particularly likely if you have multiple, differently named deployments
in the same Google Cloud project using <a href="https://cloud.google.com/iap/">Cloud IAP</a>.</p>
<h3 id="the-error">The error</h3>
<p>The error looks like this for the pod&rsquo;s Envoy container:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl logs -n kubeflow envoy-79ff8d86b-z2snp envoy
</span></span><span class="line"><span class="cl">[2019-01-22 00:19:44.400][1][info][main] external/envoy/source/server/server.cc:184] initializing epoch 0 (hot restart version=9.200.16384.127.options=capacity=16384, num_slots=8209 hash=228984379728933363)
</span></span><span class="line"><span class="cl">[2019-01-22 00:19:44.400][1][critical][main] external/envoy/source/server/server.cc:71] error initializing configuration &#39;/etc/envoy/envoy-config.json&#39;: unable to read file: /etc/envoy/envoy-config.json
</span></span></code></pre></div><p>And the Cloud IAP container shows a message like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Waiting for backend id PROJECT=&lt;your-project&gt; NAMESPACE=kubeflow SERVICE=envoy filter=name~k8s-be-30352-...
</span></span></code></pre></div><h3 id="diagnosing-the-cause">Diagnosing the cause</h3>
<p>You can verify the cause of the problem by entering the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe ingress
</span></span></code></pre></div><p>Look for something like this in the output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Events:
</span></span><span class="line"><span class="cl">  Type     Reason  Age                  From                     Message
</span></span><span class="line"><span class="cl">  ----     ------  ----                 ----                     -------
</span></span><span class="line"><span class="cl">  Warning  Sync    14m (x193 over 19h)  loadbalancer-controller  Error during sync: googleapi: Error 403: Quota &#39;BACKEND_SERVICES&#39; exceeded. Limit: 5.0 globally., quotaExceeded
</span></span></code></pre></div><h3 id="fixing-the-problem">Fixing the problem</h3>
<p>If you have any redundant Kubeflow deployments, you can delete them using
the <a href="https://cloud.google.com/deployment-manager/docs/">Deployment Manager</a>.</p>
<p>Alternatively, you can request more backend services quota on the Google Cloud Console.</p>
<ol>
<li>Go to the <a href="https://console.cloud.google.com/iam-admin/quotas?metric=Backend%20services">quota settings for backend services on the Google Cloud
Console</a>.</li>
<li>Click <strong>EDIT QUOTAS</strong>. A quota editing form opens on the right of the
screen.</li>
<li>Follow the form instructions to apply for more quota.</li>
</ol>
<h2 id="legacy-networks-are-not-supported">Legacy networks are not supported</h2>
<p>Cloud Filestore and GKE try to use the network named <code>default</code> by default. For older projects,
this will be a legacy network which is incompatible with Cloud Filestore and newer GKE features
like private clusters. This will
manifest as the error <strong>&ldquo;default is invalid; legacy networks are not supported&rdquo;</strong> when
deploying Kubeflow.</p>
<p>Here&rsquo;s an example error when deploying Cloud Filestore:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ERROR: (gcloud.deployment-manager.deployments.update) Error in Operation [operation-1533189457517-5726d7cfd19c9-e1b0b0b5-58ca11b8]: errors:
</span></span><span class="line"><span class="cl">- code: RESOURCE_ERROR
</span></span><span class="line"><span class="cl">  location: /deployments/jl-0801-b-gcfs/resources/filestore
</span></span><span class="line"><span class="cl">  message: &#39;{&#34;ResourceType&#34;:&#34;gcp-types/file-v1beta1:projects.locations.instances&#34;,&#34;ResourceErrorCode&#34;:&#34;400&#34;,&#34;ResourceErrorMessage&#34;:{&#34;code&#34;:400,&#34;message&#34;:&#34;network
</span></span><span class="line"><span class="cl">    default is invalid; legacy networks are not supported.&#34;,&#34;status&#34;:&#34;INVALID_ARGUMENT&#34;,&#34;statusMessage&#34;:&#34;Bad
</span></span><span class="line"><span class="cl">    Request&#34;,&#34;requestPath&#34;:&#34;https://file.googleapis.com/v1beta1/projects/cloud-ml-dev/locations/us-central1-a/instances&#34;,&#34;httpMethod&#34;:&#34;POST&#34;}}&#39;
</span></span></code></pre></div><p>To fix this we can create a new network:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cd ${KF_DIR}
</span></span><span class="line"><span class="cl">cp .cache/master/deployment/gke/deployment_manager_configs/network.* \
</span></span><span class="line"><span class="cl">   ./gcp_config/
</span></span></code></pre></div><p>Edit <code>network.yaml </code>to set the name for the network.</p>
<p>Edit <code>gcfs.yaml</code> to use the name of the newly created network.</p>
<p>Apply the changes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cd ${KF_DIR}
</span></span><span class="line"><span class="cl">kfctl apply -V -f ${CONFIG}
</span></span></code></pre></div><h2 id="cpu-platform-unavailable-in-requested-zone">CPU platform unavailable in requested zone</h2>
<p>By default, we set minCpuPlatform to <code>Intel Haswell</code> to make sure AVX2 is supported.
See <a href="/docs/other-guides/troubleshooting/">troubleshooting</a> for more details.</p>
<p>If you encounter this <code>CPU platform unavailable</code> error (might manifest as
<code>Cluster is currently being created, deleted, updated or repaired and cannot be updated.</code>),
you can change the <a href="https://github.com/kubeflow/manifests/blob/master/gcp/deployment_manager_configs/cluster-kubeflow.yaml#L31">zone</a>
or change the <a href="https://github.com/kubeflow/manifests/blob/master/gcp/deployment_manager_configs/cluster.jinja#L131">minCpuPlatform</a>.
See <a href="https://cloud.google.com/compute/docs/regions-zones/#available">here</a>
for available zones and cpu platforms.</p>
<h2 id="changing-the-oauth-client-used-by-iap">Changing the OAuth client used by IAP</h2>
<p>If you need to change the OAuth client used by IAP, you can run the following commands
to replace the Kubernetes secret containing the ID and secret.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n kubeflow delete secret kubeflow-oauth
</span></span><span class="line"><span class="cl">kubectl -n kubeflow create secret generic kubeflow-oauth \
</span></span><span class="line"><span class="cl">       --from-literal=client_id=${CLIENT_ID} \
</span></span><span class="line"><span class="cl">       --from-literal=client_secret=${CLIENT_SECRET}
</span></span></code></pre></div><h2 id="troubleshooting-ssl-certificate-errors">Troubleshooting SSL certificate errors</h2>
<p>This section describes how to enable service management API to avoid managed certificates failure.</p>
<p>To check your certificate:</p>
<ol>
<li>
<p>Run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system describe managedcertificate gke-certificate
</span></span></code></pre></div><p>Make sure the certificate status is either <code>Active</code> or <code>Provisioning</code> which means it is not ready. For more details on certificate status, refer to the <a href="https://cloud.google.com/load-balancing/docs/ssl-certificates?hl=en_US&amp;_ga=2.164380342.-821786221.1568995229#certificate-resource-status">certificate statuses descriptions</a> section. Also, make sure the domain name is correct.</p>
</li>
<li>
<p>Run the following command to look for the errors using the certificate name from the previous step:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud beta --project=${PROJECT} compute ssl-certificates describe --global ${CERTIFICATE_NAME}
</span></span></code></pre></div></li>
<li>
<p>Run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system get ingress envoy-ingress -o yaml
</span></span></code></pre></div><p>Make sure of the following:</p>
<ul>
<li>
<p><code>networking.gke.io/managed-certificates</code> annotation value points to the name of the Kubernetes managed certificate resource and is <code>gke-certificate</code>;</p>
</li>
<li>
<p>public IP address that is displayed in the status is assigned. See the example of IP address below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">status:
</span></span><span class="line"><span class="cl">  loadBalancer:
</span></span><span class="line"><span class="cl">    ingress:
</span></span><span class="line"><span class="cl">     - ip: 35.186.212.202
</span></span></code></pre></div></li>
<li>
<p>DNS entry for the domain has propagated. To verify this, use the following <code>nslookup</code> command example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">`nslookup ${DOMAIN}`
</span></span></code></pre></div></li>
<li>
<p>domain name is the fully qualified domain name which be the host value in the <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">ingress</a>. See the example below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">${KF_APP_NAME}.endpoints.${PROJECT}.cloud.goog
</span></span></code></pre></div></li>
</ul>
<p>Note that managed certificates cannot provision the certificate if the DNS lookup does not work properly.</p>
</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e2cfa50f92f61f2b363d831f5c905d72">3.8 - Kubeflow On-premises on Anthos</h1>
    <div class="lead">Running Kubeflow across on-premises and cloud environments with Anthos</div>
	<h2 id="introduction">Introduction</h2>
<p><a href="https://cloud.google.com/anthos">Anthos</a> is a hybrid and multi-cloud
application platform developed and supported by Google. Anthos is built on
open source technologies, including Kubernetes, Istio, and Knative.</p>
<p>Using Anthos, you can create a consistent setup across your on-premises and
cloud environments, helping you to automate policy and security at scale.</p>
<p>We are collecting interest for Kubeflow on GKE On Prem. You can subscribe
to the GitHub issue <a href="https://github.com/GoogleCloudPlatform/kubeflow-distribution/issues/138">GoogleCloudPlatform/kubeflow-distribution#138</a>.</p>
<h2 id="next-steps">Next steps</h2>
<p>While waiting for a response from the support team, you may like to <a href="/docs/gke/deploy/">deploy
Kubeflow on GKE</a>.</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-50e7e176be038e50da2a8ffef395d701">4 - Kubeflow on IBM Cloud</h1>
    <div class="lead">Running Kubeflow on IBM Cloud Kubernetes Service (IKS)</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-9da47466a15cd177f0b055b42c21f06a">4.1 - Create or access an IBM Cloud Kubernetes cluster</h1>
    <div class="lead">Instructions for creating or connecting to a Kubernetes cluster on IBM Cloud</div>
	<p>This guide describes how to create a Kubernetes cluster with IBM Cloud Kubernetes Service.</p>
<p><a href="https://www.ibm.com/cloud/kubernetes-service">IBM Cloud Kubernetes Service</a> provides powerful tools and services to help deploy highly available containerized apps in Kubernetes clusters and to automate, isolate, secure, manage, and monitor your workloads across zones or regions.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ol>
<li>
<p><code>IBMid</code></p>
<p>To get started, first go to <a href="https://ibm.biz/Bdqgck">IBM Cloud</a> to create your <code>IBMid</code> if you do not have one.</p>
</li>
<li>
<p>Installing the IBM Cloud CLI</p>
<p>Follow the instructions in this <a href="https://cloud.ibm.com/docs/cli?topic=cli-getting-started#overview">Getting started with the IBM Cloud CLI</a> guide to install the IBM Cloud CLI.</p>
</li>
<li>
<p>Installing the IBM Cloud Kubernetes Service plug-in with the command</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud plugin install container-service
</span></span></code></pre></div><p>Refer to this <a href="https://cloud.ibm.com/docs/cli?topic=containers-kubernetes-service-cli">link</a> for more info on IBM Cloud Kubernetes Service CLI.</p>
</li>
<li>
<p>Authenticating with IBM Cloud</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud login
</span></span></code></pre></div><p>Use your registered email and password for your <code>IBMid</code> to log in to IBM Cloud.</p>
</li>
</ol>
<h2 id="connecting-to-an-existing-cluster">Connecting to an existing cluster</h2>
<p>If you have an existing cluster, use it to install Kubeflow as far as it meets the minimum system requirement.</p>
<p>Get the Kubeconfig file:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks cluster config --cluster <span class="nv">$CLUSTER_NAME</span>
</span></span></code></pre></div><p>From here on, go to <a href="/docs/ibm/deploy/install-kubeflow-on-iks">Install Kubeflow on IKS</a> for more information.</p>
<h2 id="create-and-setup-a-new-cluster">Create and setup a new cluster</h2>
<ul>
<li>Use a <code>classic</code> provider if you want to try out Kubeflow.</li>
<li>Use a <code>vpc-gen2</code> provider if you are familiar with Cloud networking and want to deploy Kubeflow on a secure environment.</li>
</ul>
<p>A <code>classic</code> provider exposes each cluster node to the public internet and therefore has
a relatively simpler networking setup. Services exposed using Kubernetes <code>NodePort</code> need to be secured using
authentication mechanism.</p>
<p>To create a cluster with <code>vpc-gen2</code> provider, follow the
<a href="/docs/ibm/create-cluster-vpc">Create a cluster on IKS with a <code>vpc-gen2</code> provider</a>
guide.</p>
<p>The next section will explain how to create and set up a new IBM Cloud Kubernetes Service (IKS)</p>
<h3 id="setting-environment-variables">Setting environment variables</h3>
<p>Choose the region and the worker node provider for your cluster, and set the environment variables.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">KUBERNETES_VERSION</span><span class="o">=</span>1.21
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLUSTER_ZONE</span><span class="o">=</span>dal13
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">WORKER_NODE_PROVIDER</span><span class="o">=</span>classic
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLUSTER_NAME</span><span class="o">=</span>kubeflow
</span></span></code></pre></div><p>where:</p>
<ul>
<li><code>KUBERNETES_VERSION</code> specifies the Kubernetes version for the cluster. Run <code>ibmcloud ks versions</code> to see the supported
Kubernetes versions. If this environment variable is not set, the cluster will be created with the default version set
by IBM Cloud Kubernetes Service. Refer to
<a href="https://www.kubeflow.org/docs/started/k8s/overview/#minimum-system-requirements">Minimum system requirements</a>
and choose a Kubernetes version compatible with the Kubeflow release to be deployed.</li>
<li><code>CLUSTER_ZONE</code> identifies the regions or location where cluster will be created. Run <code>ibmcloud ks locations</code> to
list supported IBM Cloud Kubernetes Service locations. For example, choose <code>dal13</code> to create your cluster in the
Dallas (US) data center.</li>
<li><code>WORKER_NODE_PROVIDER</code> specifies the kind of IBM Cloud infrastructure on which the Kubernetes worker nodes will be
created. The <code>classic</code> type supports worker nodes with GPUs. There are other worker nodes providers including
<code>vpc-classic</code> and <code>vpc-gen2</code> where zone names and worker flavors will be different. Run
<code>ibmcloud ks zones --provider classic</code> to list zone names for <code>classic</code> provider and set the <code>CLUSTER_ZONE</code>
accordingly.</li>
<li><code>CLUSTER_NAME</code> must be lowercase and unique among any other Kubernetes
clusters in the specified <code>${CLUSTER_ZONE}</code>.</li>
</ul>
<p><strong>Notice</strong>: Refer to <a href="https://cloud.ibm.com/docs/containers?topic=containers-clusters">Creating clusters</a> in the IBM
Cloud documentation for additional information on how to set up other providers and zones in your cluster.</p>
<h3 id="choosing-a-worker-node-flavor">Choosing a worker node flavor</h3>
<p>The worker node flavor name varies from zones and providers. Run
<code>ibmcloud ks flavors --zone ${CLUSTER_ZONE} --provider ${WORKER_NODE_PROVIDER}</code> to list available flavors.</p>
<p>For example, the following are some worker node flavors supported in the <code>dal13</code> zone with a <code>classic</code> node provider.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks flavors --zone dal13 --provider classic
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">OK
</span></span><span class="line"><span class="cl">For more information about these flavors, see &#39;https://ibm.biz/flavors&#39;
</span></span><span class="line"><span class="cl">Name                      Cores   Memory   Network Speed   OS             Server Type   Storage      Secondary Storage   Provider
</span></span><span class="line"><span class="cl">b2c.16x64                 16      64GB     1000Mbps        UBUNTU_16_64   virtual       25GB         100GB               classic
</span></span><span class="line"><span class="cl">b2c.32x128                32      128GB    1000Mbps        UBUNTU_16_64   virtual       25GB         100GB               classic
</span></span><span class="line"><span class="cl">b2c.4x16                  4       16GB     1000Mbps        UBUNTU_16_64   virtual       25GB         100GB               classic
</span></span><span class="line"><span class="cl">b2c.56x242                56      242GB    1000Mbps        UBUNTU_16_64   virtual       25GB         100GB               classic
</span></span><span class="line"><span class="cl">b2c.8x32                  8       32GB     1000Mbps        UBUNTU_16_64   virtual       25GB         100GB               classic
</span></span><span class="line"><span class="cl">b3c.16x64                 16      64GB     1000Mbps        UBUNTU_18_64   virtual       25GB         100GB               classic
</span></span><span class="line"><span class="cl">b3c.32x128                32      128GB    1000Mbps        UBUNTU_18_64   virtual       25GB         100GB               classic
</span></span><span class="line"><span class="cl">b3c.4x16                  4       16GB     1000Mbps        UBUNTU_18_64   virtual       25GB         100GB               classic
</span></span><span class="line"><span class="cl">b3c.56x242                56      242GB    1000Mbps        UBUNTU_18_64   virtual       25GB         100GB               classic
</span></span><span class="line"><span class="cl">b3c.8x32                  8       32GB     1000Mbps        UBUNTU_18_64   virtual       25GB         100GB               classic
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>Choose a flavor that will work for your applications. For the purpose of the Kubeflow deployment, the recommended
configuration for a cluster is at least 8 vCPU cores with 16GB memory. Hence you can either choose the <code>b3c.8x32</code> flavor
to create a one-worker-node cluster or choose the <code>b3c.4x16</code> flavor to create a two-worker-node cluster. Keep in mind
that you can always scale the cluster by adding more worker nodes should your application scales up.</p>
<p>Now, set the environment variable with the worker node flavor of your choice:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">WORKER_NODE_FLAVOR</span><span class="o">=</span>b3c.4x16
</span></span></code></pre></div><h3 id="creating-an-ibm-cloud-kubernetes-cluster">Creating an IBM Cloud Kubernetes cluster</h3>
<p>Run with the following command to create a cluster:</p>
<p>Replace the <code>workers</code> parameter above with the desired number of worker nodes.</p>
<p>If you&rsquo;re starting in a fresh account with no public and private VLANs, they are created automatically for you
when creating a Kubernetes cluster with worker nodes provider <code>classic</code> for the first time. If you already have VLANs
configured in your account, retrieve them via <code>ibmcloud ks vlans --zone ${CLUSTER_ZONE}</code> and include the public and
private VLAN ids (set in the <code>PUBLIC_VLAN_ID</code> and <code>PRIVATE_VLAN_ID</code> environment variables) in the command.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks cluster create <span class="si">${</span><span class="nv">WORKER_NODE_PROVIDER</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --name<span class="o">=</span><span class="nv">$CLUSTER_NAME</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --zone<span class="o">=</span><span class="nv">$CLUSTER_ZONE</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --version<span class="o">=</span><span class="si">${</span><span class="nv">KUBERNETES_VERSION</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --flavor <span class="si">${</span><span class="nv">WORKER_NODE_FLAVOR</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --workers<span class="o">=</span><span class="m">2</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --private-vlan <span class="si">${</span><span class="nv">PRIVATE_VLAN_ID</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --public-vlan <span class="si">${</span><span class="nv">PUBLIC_VLAN_ID</span><span class="si">}</span> 
</span></span></code></pre></div><p>Wait until the cluster is deployed and configured. It can take a while for the cluster to be ready. Run with following
command to periodically check the state of your cluster. Your cluster is ready when the state is <code>normal</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks clusters --provider <span class="si">${</span><span class="nv">WORKER_NODE_PROVIDER</span><span class="si">}</span> <span class="p">|</span>grep <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> <span class="p">|</span>awk <span class="s1">&#39;{print &#34;Name:&#34;$1&#34;\tState:&#34;$3}&#39;</span>
</span></span></code></pre></div><h3 id="verifying-the-cluster">Verifying the cluster</h3>
<p>To use the created cluster, switch the Kubernetes context to point to the cluster with the command</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks cluster config --cluster <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span>
</span></span></code></pre></div><p>Make sure all worker nodes are up with the command below</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get nodes
</span></span></code></pre></div><p>and make sure all the nodes are in <code>Ready</code> state.</p>
<h3 id="delete-the-cluster">Delete the cluster</h3>
<p>Delete the cluster including it&rsquo;s storage:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks cluster rm --force-delete-storage -c <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span>
</span></span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a047fc4918a90f69c5ffaf6fbbf06a68">4.2 - Create or access an IBM Cloud Kubernetes cluster on a VPC</h1>
    <div class="lead">Instructions for creating or connecting to a Kubernetes cluster on IBM Cloud vpc-gen2</div>
	<h2 id="create-and-setup-a-new-cluster">Create and setup a new cluster</h2>
<p>Follow these steps to create and setup a new IBM Cloud Kubernetes Service(IKS) cluster on <code>vpc-gen2</code> provider.</p>
<p>A <code>vpc-gen2</code> cluster does not expose each node to the public internet directly and thus has more secure
and more complex network setup. It is recommended setup for secured production use cases of Kubeflow.</p>
<h3 id="setting-environment-variables">Setting environment variables</h3>
<p>Choose the region and the worker node provider for your cluster, and set the environment variables.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">KUBERNERTES_VERSION</span><span class="o">=</span>1.18
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLUSTER_ZONE</span><span class="o">=</span>us-south-3
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLUSTER_NAME</span><span class="o">=</span>kubeflow-vpc
</span></span></code></pre></div><p>where:</p>
<ul>
<li><code>KUBERNETES_VERSION</code>: Run <code>ibmcloud ks versions</code> to see the supported Kubernetes versions. Refer to
<a href="https://www.kubeflow.org/docs/started/k8s/overview/#minimum-system-requirements">Supported version matrix</a>.</li>
<li><code>CLUSTER_ZONE</code>: Run <code>ibmcloud ks locations</code> to list supported zones. For example, choose <code>us-south-3</code> to create your
cluster in the Dallas (US) data center.</li>
<li><code>CLUSTER_NAME</code> must be lowercase and unique among any other Kubernetes
clusters in the specified <code>${CLUSTER_ZONE}</code>.</li>
</ul>
<p><strong>Notice</strong>: Refer to <a href="https://cloud.ibm.com/docs/containers?topic=containers-clusters">Creating clusters</a> in the IBM
Cloud documentation for additional information on how to set up other providers and zones in your cluster.</p>
<h3 id="choosing-a-worker-node-flavor">Choosing a worker node flavor</h3>
<p>The worker nodes flavor name varies from zones and providers. Run
<code>ibmcloud ks flavors --zone ${CLUSTER_ZONE} --provider vpc-gen2</code> to list available flavors.</p>
<p>Below are some examples of flavors supported in the <code>us-south-3</code> zone with <code>vpc-gen2</code> node provider:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks flavors --zone us-south-3 --provider vpc-gen2
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">For more information about these flavors, see &#39;https://ibm.biz/flavors&#39;
</span></span><span class="line"><span class="cl">Name         Cores   Memory   Network Speed   OS             Server Type   Storage   Secondary Storage   Provider   
</span></span><span class="line"><span class="cl">bx2.16x64    16      64GB     16Gbps          UBUNTU_18_64   virtual       100GB     0B                  vpc-gen2   
</span></span><span class="line"><span class="cl">bx2.2x8†     2       8GB      4Gbps           UBUNTU_18_64   virtual       100GB     0B                  vpc-gen2   
</span></span><span class="line"><span class="cl">bx2.32x128   32      128GB    16Gbps          UBUNTU_18_64   virtual       100GB     0B                  vpc-gen2   
</span></span><span class="line"><span class="cl">bx2.48x192   48      192GB    16Gbps          UBUNTU_18_64   virtual       100GB     0B                  vpc-gen2   
</span></span><span class="line"><span class="cl">bx2.4x16     4       16GB     8Gbps           UBUNTU_18_64   virtual       100GB     0B                  vpc-gen2   
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div><p>The recommended configuration for a cluster is at least 8 vCPU cores with 16GB memory. Hence, we recommend
<code>bx2.4x16</code> flavor to create a two-worker-node cluster. Keep in mind that you can always scale the cluster
by adding more worker nodes should your application scales up.</p>
<p>Now set the environment variable with the flavor you choose.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">WORKER_NODE_FLAVOR</span><span class="o">=</span>bx2.4x16
</span></span></code></pre></div><h2 id="create-an-ibm-cloud-kubernetes-cluster-for-vpc-gen2-infrastructure">Create an IBM Cloud Kubernetes cluster for <code>vpc-gen2</code> infrastructure</h2>
<p>Creating a <code>vpc-gen2</code> based cluster needs a VPC, a subnet and a public gateway attached to it. Fortunately, this is a one
time setup. Future <code>vpc-gen2</code> clusters can reuse the same VPC/subnet(with attached public-gateway).</p>
<ol>
<li>
<p>Begin with installing a <code>vpc-infrastructure</code> plugin:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud plugin install vpc-infrastructure
</span></span></code></pre></div><p>Refer to this <a href="https://cloud.ibm.com/docs/containers?topic=containers-vpc_ks_tutorial">link</a>, for more information.</p>
</li>
<li>
<p>Target <code>vpc-gen 2</code> to access gen 2 VPC resources:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud is target --gen <span class="m">2</span>
</span></span></code></pre></div><p>Verify that the target is correctly set up:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud is target
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Target Generation: 2
</span></span></code></pre></div></li>
<li>
<p>Create or use an existing VPC:</p>
<p>a) Use an existing VPC:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud is vpcs
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Listing vpcs for generation 2 compute in all resource groups and region ...
</span></span><span class="line"><span class="cl">ID                                          Name                Status      Classic access   Default network ACL                                    Default security group                                 Resource group   
</span></span><span class="line"><span class="cl">r006-hidden-68cc-4d40-xxxx-4319fa3gxxxx   my-vpc1              available   false            husker-sloping-bee-resize                              blimp-hasty-unaware-overflow                           kubeflow   
</span></span></code></pre></div><p>If the above list contains the VPC that can be used to deploy your cluster - make a note of its ID.</p>
<p>b) To create a new VPC, proceed as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud is vpc-create my-vpc
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Creating vpc my-vpc in resource group kubeflow under account IBM as ...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ID                                             r006-hidden-68cc-4d40-xxxx-4319fa3fxxxx   
</span></span><span class="line"><span class="cl">Name                                           my-vpc   
</span></span><span class="line"><span class="cl">...  
</span></span></code></pre></div><p><strong>Save the ID in a variable <code>VPC_ID</code> as follows, so that we can use it later.</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">VPC_ID</span><span class="o">=</span>r006-hidden-68cc-4d40-xxxx-4319fa3fxxxx
</span></span></code></pre></div></li>
<li>
<p>Create or use an existing subnet:</p>
<p>a) To use an existing subnet:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud is subnets
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Listing subnets for generation 2 compute in all resource groups and region ...
</span></span><span class="line"><span class="cl">ID                                          Name                      Status      Subnet CIDR       Addresses     ACL                                                    Public Gateway                             VPC                 Zone         Resource group   
</span></span><span class="line"><span class="cl">0737-27299d09-1d95-4a9d-a491-a6949axxxxxx   my-subnet                 available   10.240.128.0/18   16373/16384   husker-sloping-bee-resize                              my-gateway                                 my-vpc              us-south-3   kubeflow   
</span></span></code></pre></div><p>If the above list contains the subnet corresponding to your VPC, that can be used to deploy your cluster - make sure
you note it&rsquo;s ID.</p>
<p>b) To create a new subnet:</p>
<ul>
<li>List address prefixes and note the CIDR block corresponding to a Zone;
in the below example, for Zone: <code>us-south-3</code> the CIDR block is : <code>10.240.128.0/18</code>.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud is vpc-address-prefixes <span class="nv">$VPC_ID</span>
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Listing address prefixes of vpc r006-hidden-68cc-4d40-xxxx-4319fa3fxxxx under account IBM as user new@user-email.com...
</span></span><span class="line"><span class="cl">ID                                          Name                                CIDR block        Zone         Has subnets   Is default   Created   
</span></span><span class="line"><span class="cl">r006-xxxxxxxx-4002-46d2-8a4f-f69e7ba3xxxx   rising-rectified-much-brew          10.240.0.0/18     us-south-1   false         true         2021-03-05T14:58:39+05:30   
</span></span><span class="line"><span class="cl">r006-xxxxxxxx-dca9-4321-bb6c-960c4424xxxx   retrial-reversal-pelican-cavalier   10.240.64.0/18    us-south-2   false         true         2021-03-05T14:58:39+05:30   
</span></span><span class="line"><span class="cl">r006-xxxxxxxx-7352-4a46-bfb1-fcbac6cbxxxx   subfloor-certainly-herbal-ajar      10.240.128.0/18   us-south-3   false         true         2021-03-05T14:58:39+05:30  
</span></span></code></pre></div><ul>
<li>Now create a subnet as follows:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud is subnet-create my-subnet <span class="nv">$VPC_ID</span> <span class="nv">$CLUSTER_ZONE</span> --ipv4-cidr-block <span class="s2">&#34;10.240.128.0/18&#34;</span>
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Creating subnet my-subnet in resource group kubeflow under account IBM as user new@user-email.com...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ID                  0737-27299d09-1d95-4a9d-a491-a6949axxxxxx   
</span></span><span class="line"><span class="cl">Name                my-subnet
</span></span></code></pre></div><ul>
<li>Make sure you export the subnet IDs follows:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">SUBNET_ID</span><span class="o">=</span>0737-27299d09-1d95-4a9d-a491-a6949axxxxxx
</span></span></code></pre></div></li>
<li>
<p>Create a <code>vpc-gen2</code> based Kubernetes cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks cluster create vpc-gen2 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>--name <span class="nv">$CLUSTER_NAME</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>--zone <span class="nv">$CLUSTER_ZONE</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>--version <span class="si">${</span><span class="nv">KUBERNETES_VERSION</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>--flavor <span class="si">${</span><span class="nv">WORKER_NODE_FLAVOR</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>--vpc-id <span class="si">${</span><span class="nv">VPC_ID</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>--subnet-id <span class="si">${</span><span class="nv">SUBNET_ID</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>--workers <span class="m">2</span>
</span></span></code></pre></div></li>
<li>
<p>Attach a public gateway</p>
<p>This step is mandatory for Kubeflow deployment to succeed, because pods need public internet access to download images.</p>
<ul>
<li>First, check if your cluster is already assigned a public gateway:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud is pubgws
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Listing public gateways for generation 2 compute in all resource groups and region ...
</span></span><span class="line"><span class="cl">ID                                          Name                                       Status      Floating IP      VPC                 Zone         Resource group   
</span></span><span class="line"><span class="cl">r006-xxxxxxxx-5731-4ffe-bc51-1d9e5fxxxxxx   my-gateway                                 available   xxx.xxx.xxx.xxx       my-vpc              us-south-3   default   
</span></span></code></pre></div><p>In the above run, the gateway is already attached for the vpc: <code>my-vpc</code>. In case no gateway is attached, proceed with
the rest of the setup.</p>
<ul>
<li>Next, attach a public gateway by running the following command:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud is public-gateway-create my-gateway <span class="nv">$VPC_ID</span> <span class="nv">$CLUSTER_ZONE</span>
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ID: r006-xxxxxxxx-5731-4ffe-bc51-1d9e5fxxxxxx
</span></span></code></pre></div><p>Save the above generated gateway ID as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">GATEWAY_ID</span><span class="o">=</span><span class="s2">&#34;r006-xxxxxxxx-5731-4ffe-bc51-1d9e5fxxxxxx&#34;</span>
</span></span></code></pre></div><ul>
<li>Finally, attach the public gateway to the subnet:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud is subnet-update <span class="nv">$SUBNET_ID</span> --public-gateway-id <span class="nv">$GATEWAY_ID</span>
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Updating subnet 0737-27299d09-1d95-4a9d-a491-a6949axxxxxx under account IBM as user new@user-email.com...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ID                  0737-27299d09-1d95-4a9d-a491-a6949axxxxxx   
</span></span><span class="line"><span class="cl">Name                my-subnet   
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div></li>
</ol>
<h3 id="verifying-the-cluster">Verifying the cluster</h3>
<p>To use the created cluster, switch the Kubernetes context to point to the cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks cluster config --cluster <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span>
</span></span></code></pre></div><p>Make sure all worker nodes are up with the command below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get nodes
</span></span></code></pre></div><p>and verify that all the nodes are in <code>Ready</code> state.</p>
<h3 id="delete-the-cluster">Delete the cluster</h3>
<p>Delete the cluster including it&rsquo;s storage:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks cluster rm --force-delete-storage -c <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span>
</span></span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-66865d260b69636513b82f1bb3490e7e">4.3 - Kubeflow Deployment on IBM Cloud</h1>
    <div class="lead">Instructions for Kubeflow deployment on IBM Cloud</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-eaa47a4d7381236a0b84e713fc392ffa">4.3.1 - Kubeflow Deployment Process</h1>
    <div class="lead">How kubeflow installation works</div>
	<h2 id="understanding-the-kubeflow-deployment-process">Understanding the Kubeflow deployment process</h2>
<p>The deployment process is controlled by the following commands:</p>
<ul>
<li><strong>kustomize build</strong> - Use kustomize to generate configuration files defining
the various resources for your deployment. .</li>
<li><strong>kubectl apply</strong> - Apply the resources created by <code>kustomize build</code> to the
kubenetes cluster</li>
</ul>
<h3 id="repository-layout">Repository layout</h3>
<p>IBM manifests repository contains the following files and directories:</p>
<ul>
<li>
<p><strong>iks-single</strong> directory: A kustomize file for single-user deployment</p>
</li>
<li>
<p><strong>iks-multi</strong> directory: A kustomize file for multi-user deployment</p>
</li>
<li>
<p><strong>others</strong> Other files are used to compose Kubeflow resources</p>
</li>
</ul>
<h2 id="kubeflow-installation">Kubeflow installation</h2>
<p>Starting from Kubeflow 1.3, the official installation documentation uses a combination of <code>kustomize</code> and <code>kubectl</code> to install Kubeflow.</p>
<h3 id="install-kubectl-and-kustomize">Install kubectl and kustomize</h3>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/tools/#kubectl">Install kubectl</a></li>
<li><a href="https://github.com/kubernetes-sigs/kustomize/releases/tag/v3.2.0">Download kustomize 3.2.0</a></li>
</ul>
<p>To use the <code>kustomize</code> binary, you need to make it executable and move it to your path.</p>
<p>To add <code>kustomize</code> to your global path, run the following commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">wget https://github.com/kubernetes-sigs/kustomize/releases/download/v3.2.0/&lt;distribution&gt;
</span></span><span class="line"><span class="cl">chmod +x &lt;distribution&gt;
</span></span><span class="line"><span class="cl">mv &lt;distribution&gt; /usr/local/bin/kustomize
</span></span></code></pre></div><p>Your machine might already have <code>kustomize</code> installed. If you want to temporarily add this version of <code>kustomize</code> to your path, run the following commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">wget https://github.com/kubernetes-sigs/kustomize/releases/download/v3.2.0/&lt;distribution&gt;
</span></span><span class="line"><span class="cl">chmod +x &lt;distribution&gt;
</span></span><span class="line"><span class="cl">mv &lt;distribution&gt; /some/path/kustomize
</span></span><span class="line"><span class="cl"><span class="c1"># /some/path should not already be in path. </span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span>/some/path:<span class="nv">$PATH</span>
</span></span><span class="line"><span class="cl"><span class="c1"># order is important here. $PATH needs to be the last thing. We are trying to put our kustomize before the kustomize installtion in system.</span>
</span></span></code></pre></div><h2 id="next-steps">Next Steps</h2>
<ol>
<li>Check <a href="/docs/distributions/ibm/deploy/iks-compatibility">Kubeflow Compatibility</a></li>
<li>Go here for installing <a href="/docs/distributions/ibm/deploy/install-kubeflow-on-iks">Kubeflow on IKS</a></li>
<li>Go here for installing <a href="/docs/distributions/ibm/deploy/install-kubeflow-on-ibm-openshift">Kubeflow on IBM OpenShift</a></li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bb5daad82a3658275d7a80ec50f67eae">4.3.2 - IBM Cloud Kubernetes and Kubeflow Compatibility</h1>
    <div class="lead">Compatibility matrix for Kubeflow on IBM Cloud by Kubernetes version</div>
	<h2 id="compatibility">Compatibility</h2>
<p>The following table relates compatibility between Kubernetes versions 1.22+ of IBM Cloud Kubernetes service and Kubeflow version 1.6.</p>
<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>IBM Cloud Kubernetes Versions</th>
        <th>Kubeflow 1.6.0</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1.22</td>
        <td><b>Compatible</b></td>
      </tr>
      <tr>
        <td>1.23</td>
        <td><b>Compatible</b></td>
      </tr>
      <tr>
        <td>1.24</td>
        <td><b>Compatible</b></td>
      </tr>
    </tbody>
  </table>
</div>
<ul>
<li><strong>Incompatible</strong>: the combination is not known to work together</li>
<li><strong>Compatible</strong>: all Kubeflow features have been tested and verified for the IKS Kubernetes version</li>
<li><strong>No known issues</strong>: the combination has not been fully tested but there are no reported issues</li>
</ul>
<h2 id="next-steps">Next Steps</h2>
<ol>
<li>
<ol start="2">
<li>Go here for installing <a href="/docs/distributions/ibm/deploy/install-kubeflow-on-iks">Kubeflow on IKS</a></li>
</ol>
</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-73f567b57211c57a9dfb28f8048e2037">4.3.3 - Install Kubeflow on IKS</h1>
    <div class="lead">Instructions for deploying Kubeflow on IBM Cloud Kubernetes Service</div>
	<p>This guide describes how to use the kustomize + kubectl to deploy Kubeflow on IBM Cloud Kubernetes Service (IKS).</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>
<p>Authenticate with IBM Cloud</p>
<p>Log into IBM Cloud using the <a href="https://www.ibm.com/cloud/cli">IBM Cloud Command Line Interface (CLI)</a> as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud login
</span></span></code></pre></div><p>Or, if you have federated credentials, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud login --sso  
</span></span></code></pre></div></li>
<li>
<p>Create and access a Kubernetes cluster on IKS</p>
<p>To deploy Kubeflow on IBM Cloud, you need a cluster running on IKS. If you don&rsquo;t have a cluster running, follow the <a href="/docs/ibm/create-cluster">Create an IBM Cloud cluster</a> guide.</p>
<p>Run the following command to switch the Kubernetes context and access the cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks cluster config --cluster &lt;cluster_name&gt;
</span></span></code></pre></div><p>Replace <code>&lt;cluster_name&gt;</code> with your cluster name.</p>
</li>
<li>
<p>kustomize (version 3.2.0) (<a href="https://github.com/kubernetes-sigs/kustomize/releases/tag/v3.2.0">download link</a>)</p>
</li>
<li>
<p><a href="https://www.python.org/downloads/">Python 3</a> with <a href="https://pypi.org/project/passlib/">passlib</a>
and <a href="https://pypi.org/project/bcrypt/">bcrypt</a> packages installed</p>
</li>
</ul>
<h3 id="storage-setup-for-a-classic-ibm-cloud-kubernetes-cluster">Storage setup for a <strong>Classic</strong> IBM Cloud Kubernetes cluster</h3>
<p><strong>Note</strong>: This section is only required when the worker nodes provider <code>WORKER_NODE_PROVIDER</code> is set to <code>classic</code>. For other infrastructures, IBM Cloud Storage with Group ID support is already set up as the cluster&rsquo;s default storage class.</p>
<p>When you use the <code>classic</code> worker node provider of an IBM Cloud Kubernetes cluster, it uses the regular <a href="https://www.ibm.com/cloud/file-storage">IBM Cloud File Storage</a> based on NFS as the default storage class. File Storage is designed to run RWX (read-write multiple nodes) workloads with proper security built around it. Therefore, File Storage <a href="https://cloud.ibm.com/docs/containers?topic=containers-security#container">does not allow <code>fsGroup</code> securityContext</a> unless it&rsquo;s configured with Group ID, which is needed for the <a href="https://github.com/arrikto/oidc-authservice">OIDC authentication service</a> and Kubeflow Jupyter server.</p>
<p>Therefore, you&rsquo;re recommended to set up the default storage class with Group ID support so that you can get the best experience from Kubeflow.</p>
<ol>
<li>
<p>Set the File Storage with Group ID support as the default storage class.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nv">NEW_STORAGE_CLASS</span><span class="o">=</span>ibmc-file-gold-gid
</span></span><span class="line"><span class="cl"><span class="nv">OLD_STORAGE_CLASS</span><span class="o">=</span><span class="k">$(</span>kubectl get sc -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io\/is-default-class==&#34;true&#34;)].metadata.name}&#39;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">kubectl patch storageclass <span class="si">${</span><span class="nv">NEW_STORAGE_CLASS</span><span class="si">}</span> -p <span class="s1">&#39;{&#34;metadata&#34;: {&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;true&#34;}}}&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># List all the (default) storage classes</span>
</span></span><span class="line"><span class="cl">kubectl get storageclass <span class="p">|</span> grep <span class="s2">&#34;(default)&#34;</span>
</span></span></code></pre></div><p>Example output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ibmc-file-gold-gid (default)   ibm.io/ibmc-file    Delete          Immediate           false                  14h
</span></span></code></pre></div></li>
<li>
<p>Make sure <code>ibmc-file-gold-gid</code> is the only <code>(default)</code> storage class. If there are two or more rows in the above output, unset the previous <code>(default)</code> storage classes with the command below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl patch storageclass <span class="si">${</span><span class="nv">OLD_STORAGE_CLASS</span><span class="si">}</span> -p <span class="s1">&#39;{&#34;metadata&#34;: {&#34;annotations&#34;:{&#34;storageclass.kubernetes.io/is-default-class&#34;:&#34;false&#34;}}}&#39;</span>
</span></span></code></pre></div></li>
</ol>
<h3 id="storage-setup-for-vpc-gen2-ibm-cloud-kubernetes-cluster">Storage setup for <strong>vpc-gen2</strong> IBM Cloud Kubernetes cluster</h3>
<p><strong>Note</strong>: To deploy Kubeflow, you don&rsquo;t need to change the storage setup for <code>vpc-gen2</code> Kubernetes cluster.</p>
<p>Currently, there is no option available for setting up RWX (read-write multiple nodes) type of storages.
RWX is not a mandatory requirement to run Kubeflow and most pipelines.
It is required by certain sample jobs/pipelines where multiple pods write results to a common storage.
A job or a pipeline can also write to a common object storage like <code>minio</code>, so the absence of this feature is
not a blocker for working with Kubeflow.
Examples of jobs/pipelines that will not work, are:
<a href="https://github.com/kubeflow/training-operator/tree/master/examples/tensorflow/mnist_with_summaries">Distributed training with Kubeflow TFJob</a></p>
<p>If you are on <code>vpc-gen2</code> and still need RWX, you may try <a href="https://portworx.com/products/features/">portworx enterprise product</a>.
To set it up on IBM Cloud use the <a href="https://docs.portworx.com/portworx-install-with-kubernetes/cloud/ibm/">portworx install with IBM Cloud</a> guide.</p>
<h2 id="installation">Installation</h2>
<p>Choose either <strong>single user</strong> or <strong>multi-tenant</strong> section based on your usage.</p>
<p>If you&rsquo;re experiencing issues during the installation because of conflicts on your Kubeflow deployment, you can <a href="/docs/ibm/deploy/uninstall-kubeflow">uninstall Kubeflow</a> and install it again.</p>
<h2 id="single-user">Single user</h2>
<p>Using kustomize together with kubectl to deploy kubeflow:</p>
<ol>
<li>
<p>Clone the manifest repo as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">git clone https://github.com/IBM/manifests.git -b v1.6-branch ibm-manifests-160
</span></span></code></pre></div></li>
<li>
<p>Change directory to <code>ibm-manifests-160</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> ibm-manifests-160
</span></span></code></pre></div></li>
<li>
<p>Generate password for default user: <code>user@example.com</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">python3 -c <span class="s1">&#39;from passlib.hash import bcrypt; import getpass; print(bcrypt.using(rounds=12, ident=&#34;2y&#34;).hash(getpass.getpass()))&#39;</span>
</span></span></code></pre></div><p>Type your password and press <code>&lt;Enter&gt;</code> after you see <code>Password:</code> prompt. Copy the hash code for next step.</p>
</li>
<li>
<p>Edit <code>dist/stacks/ibm/application/dex-auth/custom-env.yaml</code> and fill the relevant field
with the hash code from previous step:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">staticPasswords:
</span></span><span class="line"><span class="cl">- email: user@example.com
</span></span><span class="line"><span class="cl">  hash: &lt;enter the generated hash here&gt;
</span></span></code></pre></div><p>You can also change the email value if needed.</p>
</li>
<li>
<p>Apply the <code>kustomize</code> file under <code>iks-single</code> folder for single user deployment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="k">while</span> ! kustomize build iks-single <span class="p">|</span> kubectl apply -f -<span class="p">;</span> <span class="k">do</span> <span class="nb">echo</span> <span class="s2">&#34;Retrying to apply resources&#34;</span><span class="p">;</span> sleep 10<span class="p">;</span> <span class="k">done</span>
</span></span></code></pre></div></li>
</ol>
<h3 id="accessing-your-cluster">Accessing your cluster</h3>
<p>The Kubeflow endpoint is exposed with <a href="https://kubernetes.io/docs/concepts/services-networking/service/#nodeport">NodePort</a> <code>30380</code>. To get a static ip, you can <a href="#expose-the-kubeflow-endpoint-as-a-loadbalancer">expose the Kubeflow endpoint as a LoadBalancer</a> and access the <strong>EXTERNAL_IP</strong>.</p>
<p>For single-user Kubeflow, IBM Cloud uses Dex authentication by default. You can access the cluster using
the email and password you specified in step 3 and 4 of <a href="#single-user">Single User</a></p>
<h2 id="multi-user-auth-enabled">Multi-user, auth-enabled</h2>
<p>Run the following steps to deploy Kubeflow with <a href="https://cloud.ibm.com/catalog/services/app-id">IBM Cloud AppID</a>
as an authentication provider.</p>
<p>The scenario is a Kubeflow cluster admin configures Kubeflow as a web
application in AppID and manages user authentication with builtin identity
providers (Cloud Directory, SAML, social log-in with Google or Facebook etc.) or
custom providers.</p>
<h3 id="prerequisites-1">Prerequisites</h3>
<p>For authentication,  IBM Cloud uses <a href="https://cloud.ibm.com/catalog/services/app-id">AppID</a></p>
<ol>
<li>
<p>Follow the <a href="https://cloud.ibm.com/catalog/services/app-id">Creating an App ID service instance on IBM Cloud</a> guide to learn about Kubeflow authentication.
You can also learn <a href="https://cloud.ibm.com/docs/appid?topic=appid-getting-started">how to use App ID</a> with different authentication methods.</p>
</li>
<li>
<p>Follow the <a href="https://cloud.ibm.com/docs/appid?topic=appid-app#app-register">Registering your app</a>
section of the App ID guide to create an application with type
<em>regularwebapp</em> under the provisioned AppID instance. Make sure the <em>scope</em>
contains <em>email</em>. Then retrieve the following configuration parameters from your AppID:</p>
<ul>
<li><code>clientId</code></li>
<li><code>secret</code></li>
<li><code>oAuthServerUrl</code></li>
</ul>
<p>You will be using these information in the subsequent sections.</p>
</li>
<li>
<p>Register the Kubeflow OIDC redirect page. The Kubeflow <code>REDIRECT_URL</code> URL is
<code>[http|https]://&lt;kubeflow-FQDN&gt;/login/oidc</code>, depends on if you enable the HTTPS or not.
<code>&lt;kubeflow-FQDN&gt;</code> is the endpoint for accessing Kubeflow. By default, the <code>&lt;kubeflow-FQDN&gt;</code>
on IBM Cloud is <code>&lt;worker_node_external_ip&gt;:30380</code>. To get a static ip, you can
<a href="#expose-the-kubeflow-endpoint-as-a-loadbalancer">expose the Kubeflow endpoint as a LoadBalancer</a>
and use the <strong>EXTERNAL_IP</strong> for your <code>&lt;kubeflow-FQDN&gt;</code>. Or use <code>ibmcloud ks nlb-dns</code> command
to map the <strong>EXTERNAL_IP</strong> to the generated FQDN for your cluster. In this case, you use the
generated FQDN as <code>kubeflow-FQDN</code>. If you enable HTTPS, you shall use generated FQDN.</p>
</li>
<li>
<p>Then, you need to place the Kubeflow OIDC <code>REDIRECT_URL</code> under <strong>Manage Authentication</strong> &gt; <strong>Authentication settings</strong> &gt; <strong>Add web redirect URLs</strong>.</p>
<img src="/docs/images/ibm/appid-redirect-settings.png" alt="APP ID Redirect Settings" class="mt-3 mb-3 border border-info rounded">
<p>Example:
<code>https://my-kubeflow-442dbba0442be6c8c50f31ed96b00601-0000.sjc04.containers.appdomain.cloud/login/oidc</code></p>
</li>
</ol>
<h3 id="deploy-using-kustomize-together-with-kubectl">Deploy: Using kustomize together with kubectl</h3>
<ol>
<li>
<p>Clone the manifest repo as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">git clone https://github.com/IBM/manifests.git -b v1.6-branch ibm-manifests-160
</span></span></code></pre></div></li>
<li>
<p>Change directory to <code>ibm-manifests-160</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> ibm-manifests-160
</span></span></code></pre></div></li>
<li>
<p>Update the <code>dist/stacks/ibm/application/oidc-authservice-appid/params.env</code>
with values collected in <a href="#prerequisites-1">Prereq</a> section.
You will need the following values:</p>
<ul>
<li><code>&lt;oAuthServerUrl&gt;</code> - replace <code>&lt;APP_ID_oauthServerUrl&gt;</code></li>
<li><code>&lt;kubeflow-FQDN&gt;</code> - fill in the FQDN of Kubeflow, if you don&rsquo;t know yet, just give a dummy one like
<code>localhost</code>. Then change it after you got one. Or get default FQDN of your cluster by this command:
<code>ibmcloud ks nlb-dns ls -c &lt;cluster name&gt;</code> (replace <code>&lt;cluter name&gt;</code> with your cluster name)</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">OIDC_PROVIDER=https://us-south.appid.cloud.ibm.com/oauth/v4/f341ff8b-a088-497a-same-5da4628df7fd
</span></span><span class="line"><span class="cl">REDIRECT_URL=https://my-kubeflow-442dbba0442be6c8c50f31ed96b00601-0000.sjc04.containers.appdomain.cloud/login/oidc
</span></span><span class="line"><span class="cl">OIDC_AUTH_URL=https://us-south.appid.cloud.ibm.com/oauth/v4/f341ff8b-a088-497a-same-5da4628df7fd/authorization
</span></span></code></pre></div></li>
<li>
<p>Update the <code>dist/stacks/ibm/application/oidc-authservice-appid/secret_params.env</code>
with values collected in <a href="#prerequisites-1">Prereq</a> section.
You will need the following values:</p>
<ul>
<li><code>&lt;clientId&gt;</code> - replace the <code>&lt;APP_ID_clientId&gt;</code></li>
<li><code>&lt;secret&gt;</code> - replace the <code>&lt;APP_ID_secret&gt;</code></li>
</ul>
<p>Example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">CLIENT_SECRET=NjNhZDA3ODAtM2I3MCSECRETLTkwN2QtNDdhYmU5ZGIyMTBl
</span></span><span class="line"><span class="cl">CLIENT_ID=52b3e496-8888-8888-ABC9-c0da309cdf52
</span></span></code></pre></div></li>
<li>
<p>You can apply the <code>kustomize</code> file in <code>iks-multi</code> folder:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">while</span> ! kustomize build iks-multi <span class="p">|</span> kubectl apply -f -<span class="p">;</span> <span class="k">do</span> <span class="nb">echo</span> <span class="s2">&#34;Retrying to apply resources&#34;</span><span class="p">;</span> sleep 10<span class="p">;</span> <span class="k">done</span>
</span></span></code></pre></div></li>
<li>
<p>If at any point the values change and you have to change them, you can either patch the
<a href="#patch-configmap">configmap</a> and <a href="#patch-secret">secret</a> or change the content in the
files and apply the kustomize again. You will need to restart authservice with
<code>kubectl delete pod -l app=authservice -n istio-system</code> .</p>
<p>To apply just the <code>oidc-authservice-appid</code> you can use this command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kustomize build dist/stacks/ibm/application/oidc-authservice-appid <span class="p">|</span> kubectl apply -f -
</span></span><span class="line"><span class="cl">kubectl delete pod -l <span class="nv">app</span><span class="o">=</span>authservice -n istio-system
</span></span></code></pre></div></li>
</ol>
<h3 id="verify-mutli-user-installation">Verify mutli-user installation</h3>
<p>Check the pod <code>authservice-0</code> is in running state in namespace <code>istio-system</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-SHELL" data-lang="SHELL"><span class="line"><span class="cl">kubectl get pod -l <span class="nv">app</span><span class="o">=</span>authservice -n istio-system
</span></span></code></pre></div><h3 id="extra-network-setup-requirement-for-vpc-gen2-clusters-only">Extra network setup requirement for <strong>vpc-gen2</strong> clusters only</h3>
<p><strong>Note</strong>: These steps are not required for <code>classic</code> clusters, i.e. where <code>WORKER_NODE_PROVIDER</code> is set to <code>classic</code>.</p>
<p>A <code>vpc-gen2</code> cluster does not assign a public IP address to the Kubernetes master node by default.
It provides access via a Load Balancer, which is configured to allow only a set of ports over public internet.
Access the cluster&rsquo;s resources in a <code>vpc-gen2</code> cluster, using one of the following options,</p>
<ul>
<li>
<p>Load Balancer method: To configure via a Load Balancer, go to <a href="#expose-the-kubeflow-endpoint-as-a-loadbalancer">Expose the Kubeflow endpoint as a LoadBalancer</a>.
This method is recommended when you have Kubeflow deployed with <a href="#multi-user-auth-enabled">Multi-user, auth-enabled</a> support — otherwise it will expose
cluster resources to the public.</p>
</li>
<li>
<p>Socks proxy method: If you need access to nodes or NodePort in the <code>vpc-gen2</code> cluster, this can be achieved by starting another instance in the
same <code>vpc-gen2</code> cluster and assigning it a public IP (i.e. the floating IP). Next, use SSH to log into the instance or create an SSH socks proxy,
such as <code>ssh -D9999 root@new-instance-public-ip</code>.</p>
</li>
</ul>
<p>Then, configure the socks proxy at <code>localhost:9999</code> and access cluster services.</p>
<ul>
<li><code>kubectl port-forward</code> method: To access Kubeflow dashboard, run <code>kubectl -n istio-system port-forward service/istio-ingressgateway 7080:http2</code>.
Then in a browser, go to<a href="http://127.0.0.1:7080/">http://127.0.0.1:7080/</a></li>
</ul>
<p><em><strong>Important notice</strong>: Exposing cluster/compute resources publicly without setting up a proper user authentication mechanism
is very insecure and can have very serious consequences(even legal). If there is no need to expose cluster services publicly,
Socks proxy method or <code>kubectl port-forward</code> method are recommended.</em></p>
<h2 id="next-steps-secure-the-kubeflow-dashboard-with-https">Next steps: secure the Kubeflow dashboard with HTTPS</h2>
<h3 id="prerequisites-2">Prerequisites</h3>
<p>For both <code>classic</code> and <code>vpc-gen2</code> cluster providers, make sure you have <a href="#multi-user-auth-enabled">Multi-user, auth-enabled</a> Kubeflow set up.</p>
<h3 id="setup">Setup</h3>
<p>Follow the steps in <a href="../authentication/#exposing-the-kubeflow-dashboard-with-dns-and-tls-termination">Exposing the Kubeflow dashboard with DNS and TLS termination</a>.
Then, you will have the required DNS name as Kubeflow FQDN to enable the OIDC flow for AppID:</p>
<ol>
<li>
<p>Follow the step <a href="https://cloud.ibm.com/docs/appid?topic=appid-managing-idp#add-redirect-uri">Adding redirect URIs</a>
to fill a URL for AppID to redirect to Kubeflow. The URL should look like <code>https://&lt;kubeflow-FQDN&gt;/login/oidc</code>.</p>
</li>
<li>
<p>Update the secret <code>appid-application-configuration</code> with the updated Kubeflow FQDN to replace <code>&lt;kubeflow-FQDN&gt;</code> in below command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-SHELL" data-lang="SHELL"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">REDIRECT_URL</span><span class="o">=</span>https://&lt;kubeflow-FQDN&gt;/login/oidc
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">PATCH</span><span class="o">=</span><span class="k">$(</span><span class="nb">printf</span> <span class="s1">&#39;{&#34;data&#34;: {&#34;REDIRECT_URL&#34;: &#34;%s&#34;}}&#39;</span> <span class="s2">&#34;</span><span class="nv">$REDIRECT_URL</span><span class="s2">&#34;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kubectl patch configmap/oidc-authservice-parameters -n istio-system -p<span class="o">=</span><span class="s2">&#34;</span><span class="nv">$PATCH</span><span class="s2">&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Restart the pod <code>authservice-0</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl delete pod -l <span class="nv">app</span><span class="o">=</span>authservice -n istio-system
</span></span></code></pre></div></li>
</ol>
<p>Then, visit <code>https://&lt;kubeflow-FQDN&gt;/</code>. The page should redirect you to AppID for authentication.</p>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="expose-the-kubeflow-endpoint-as-a-loadbalancer">Expose the Kubeflow endpoint as a LoadBalancer</h3>
<p>By default, the Kubeflow deployment on IBM Cloud only exposes the endpoint as <a href="https://kubernetes.io/docs/concepts/services-networking/service/#nodeport">NodePort</a> 30380. If you want to expose the endpoint as a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer">LoadBalancer</a>, run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl patch svc istio-ingressgateway -n istio-system -p <span class="s1">&#39;{&#34;spec&#34;: {&#34;type&#34;: &#34;LoadBalancer&#34;}}&#39;</span>
</span></span></code></pre></div><p>Then, you can locate the LoadBalancer in the <strong>EXTERNAL_IP</strong> column when you run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get svc istio-ingressgateway -n istio-system
</span></span></code></pre></div><p>There is a small delay, usually ~5 mins, for above commands to take effect.</p>
<h3 id="authservice-pod-taking-too-long-to-restart">Authservice pod taking too long to restart</h3>
<p>You might see the <code>authservice-0</code> pod taking some time to restart. If that happens you can delete to pod which will kick off restart from kubernetes reconciler.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl delete pod -l <span class="nv">app</span><span class="o">=</span>authservice -n istio-system
</span></span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-07a1bb96654c4b0fb7dfd8a4e48f6770">4.3.4 - Install Kubeflow on OpenShift</h1>
    <div class="lead">Instructions for deploying Kubeflow on IBM Cloud OpenShift</div>
	<p><strong>This guide has not yet been updated for Kubeflow 1.3</strong></p>
<p>This guide describes how to use the kfctl binary to deploy Kubeflow on IBM Cloud Kubernetes Service (IKS).</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>
<p>Authenticate with IBM Cloud</p>
<p>Log into IBM Cloud at <a href="https://cloud.ibm.com">IBM Cloud</a></p>
</li>
<li>
<p>Install OpenShift CLI</p>
<p>OpenShift CLI is the way to manage and access OpenShift cluster. You can <a href="https://cloud.ibm.com/docs/openshift?topic=openshift-openshift-cli">Install OpenShift CLI</a> from this instructions.</p>
</li>
<li>
<p>Create and access a OpenShift cluster on IKS</p>
<p>To deploy Kubeflow on IBM Cloud, you need a cluster running OpenShift on IKS. If you don&rsquo;t have a cluster running, follow the <a href="https://cloud.ibm.com/docs/openshift?topic=openshift-clusters">Create an IBM Cloud OpenShift cluster</a> guide.</p>
<p>To access the cluster follow these directions <a href="https://cloud.ibm.com/docs/openshift?topic=openshift-access_cluster">Access OpenShift Cluster</a>. We can easily get access from the openshift console on IBM Cloud<a href="https://cloud.ibm.com/docs/openshift?topic=openshift-access_cluster#access_oc_console">Connecting to the cluster from the console</a>.</p>
</li>
</ul>
<h2 id="installation">Installation</h2>
<p>If you&rsquo;re experiencing issues during the installation because of conflicts on your Kubeflow deployment, you can <a href="/docs/ibm/deploy/uninstall-kubeflow">uninstall Kubeflow</a> and install it again.</p>
<h3 id="single-user">Single user</h3>
<p>Run the following commands to set up and deploy Kubeflow for a single user without any authentication.</p>
<p><strong>Note</strong>: By default, Kubeflow deployment on IBM Cloud uses the <a href="https://github.com/kubeflow/kfp-tekton#kubeflow-pipelines-with-tekton">Kubeflow pipeline with the Tekton backend</a>.
If you want to use the Kubeflow pipeline with the Argo backend, you can change <code>CONFIG_URI</code> to this kfdef instead</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_openshift.v1.2.0.yaml
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="c1"># Set KF_NAME to the name of your Kubeflow deployment. This also becomes the</span>
</span></span><span class="line"><span class="cl"><span class="c1"># name of the directory containing your configuration.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># For example, your deployment name can be &#39;my-kubeflow&#39; or &#39;kf-test&#39;.</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">KF_NAME</span><span class="o">=</span>&lt;your choice of name <span class="k">for</span> the Kubeflow deployment&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set the path to the base directory where you want to store one or more </span>
</span></span><span class="line"><span class="cl"><span class="c1"># Kubeflow deployments. For example, /opt/.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Then set the Kubeflow application directory for this deployment.</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">BASE_DIR</span><span class="o">=</span>&lt;path to a base directory&gt;
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">KF_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span>/<span class="si">${</span><span class="nv">KF_NAME</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set the configuration file to use, such as:</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CONFIG_FILE</span><span class="o">=</span>kfctl_ibm.yaml
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CONFIG_URI</span><span class="o">=</span><span class="s2">&#34;https://raw.githubusercontent.com/kubeflow/manifests/master/distributions/kfdef/kfctl_openshift.master.kfptekton.yaml&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Generate Kubeflow:</span>
</span></span><span class="line"><span class="cl">mkdir -p <span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> <span class="si">${</span><span class="nv">KF_DIR</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">wget <span class="si">${</span><span class="nv">CONFIG_URI</span><span class="si">}</span> -O <span class="si">${</span><span class="nv">CONFIG_FILE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># On MacOS</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s1">&#39;&#39;</span> -e <span class="s1">&#39;s#https://github.com/kubeflow/manifests/archive/master.tar.gz#https://github.com/kubeflow/manifests/archive/552a4ba84567ed8c0f9abca12f15b8eed000426c.tar.gz#g&#39;</span> <span class="si">${</span><span class="nv">CONFIG_FILE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># On Linux</span>
</span></span><span class="line"><span class="cl">sed -i -e <span class="s1">&#39;s#https://github.com/kubeflow/manifests/archive/master.tar.gz#https://github.com/kubeflow/manifests/archive/552a4ba84567ed8c0f9abca12f15b8eed000426c.tar.gz#g&#39;</span> <span class="si">${</span><span class="nv">CONFIG_FILE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Deploy Kubeflow. You can customize the CONFIG_FILE if needed.</span>
</span></span><span class="line"><span class="cl">kfctl apply -V -f <span class="si">${</span><span class="nv">CONFIG_FILE</span><span class="si">}</span>
</span></span></code></pre></div><ul>
<li>
<p><strong>${KF_NAME}</strong> - The name of your Kubeflow deployment.
If you want a custom deployment name, specify that name here.
For example,  <code>my-kubeflow</code> or <code>kf-test</code>.
The value of KF_NAME must consist of lower case alphanumeric characters or
&lsquo;-&rsquo;, and must start and end with an alphanumeric character.
The value of this variable cannot be greater than 25 characters. It must
contain just a name, not a directory path.
This value also becomes the name of the directory where your Kubeflow
configurations are stored, that is, the Kubeflow application directory.</p>
</li>
<li>
<p><strong>${KF_DIR}</strong> - The full path to your Kubeflow application directory.</p>
</li>
</ul>
<p>The Kubeflow deployment is exposed with a Route. To find the Route you can use</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">oc get route -n istio-system istio-ingressgateway -o=jsonpath=&#39;{.spec.host}&#39;
</span></span></code></pre></div><h2 id="next-steps">Next steps</h2>
<p>To secure the Kubeflow dashboard with HTTPS, follow the steps in <a href="/docs/ibm/deploy/authentication/#setting-up-an-nlb">Exposing the Kubeflow dashboard with DNS and TLS termination</a>.</p>
<h2 id="additional-information">Additional information</h2>
<p>You can find general information about Kubeflow configuration in the guide to <a href="/docs/other-guides/kustomize/">configuring Kubeflow with kfctl and kustomize</a>.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7c1de18991e035a057289cfa97453b61">4.3.5 - Securing the Kubeflow authentication with HTTPS</h1>
    <div class="lead">How to secure the Kubeflow authentication with HTTPS using the network load balancer</div>
	<p>This guide describes how to secure the Kubeflow authentication with HTTPS.
You can enable HTTPS for Kubeflow dashboard (and other web UIs) using the
network load balancer (NLB) feature of the IBM Cloud Kubernetes service—choose
the <code>classic</code> worker nodes provider in the
<a href="../../create-cluster#setting-environment-variables">Setting environment variables</a>
section of the Create an IBM Cloud cluster guide.</p>
<p><strong>Note</strong>: For details on NLB, go to the official
<a href="https://cloud.ibm.com/docs/containers?topic=containers-loadbalancer-about">Classic: About network load balancers</a>
guide.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Install and configure the
<a href="https://cloud.ibm.com/docs/cli?topic=cli-getting-started">IBM Cloud CLI</a>.</li>
<li>Install
<a href="../install-kubeflow-on-iks/#multi-user-auth-enabled">multi-user, auth-enabled Kubeflow</a>.</li>
</ul>
<h2 id="setting-up-an-nlb">Setting up an NLB</h2>
<p>To set up an NLB for your Kubernetes cluster, follow the official
<a href="https://cloud.ibm.com/docs/containers?topic=containers-loadbalancer">Classic: Setting up basic load balancing with an NLB 1.0</a>
guide. Notice that the setup process for a multi-zone cluster differs from that
of a single-zone cluster. For details, go to
<a href="https://cloud.ibm.com/docs/containers?topic=containers-loadbalancer#multi_zone_config">Setting up an NLB 1.0 in a multi-zone cluster</a>.</p>
<ol>
<li>
<p>To use the existing Istio ingress gateway (instead of creating a new
service), you need to update the service type of <code>istio-ingressgateway</code> to
<code>LoadBalancer</code> from <code>NodePort</code>. Run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl patch svc istio-ingressgateway -n istio-system -p <span class="s1">&#39;{&#34;spec&#34;:{&#34;type&#34;:&#34;LoadBalancer&#34;}}&#39;</span>
</span></span></code></pre></div></li>
<li>
<p>Verify that the NLB was created successfully. It might take a few minutes for
the service to be created and an IP address to be made available. Run the
command below and check if you can see the <code>LoadBalancer Ingress</code> IP address:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl describe service istio-ingressgateway -n istio-system <span class="p">|</span> grep <span class="s2">&#34;LoadBalancer Ingress&#34;</span>
</span></span></code></pre></div></li>
<li>
<p>Store the external IP of the <code>istio-ingressgateway</code> service in an environment
variable:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">INGRESS_GATEWAY_IP</span><span class="o">=</span><span class="k">$(</span>kubectl -n istio-system get service istio-ingressgateway -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">&#39;{.status.loadBalancer.ingress[0].ip}&#39;</span><span class="k">)</span>
</span></span></code></pre></div></li>
</ol>
<h2 id="exposing-the-kubeflow-dashboard-with-dns-and-tls-termination">Exposing the Kubeflow dashboard with DNS and TLS termination</h2>
<p>The following instructions use the Kubeflow dashboard as an example. However,
they apply to other web UI applications, since they all go through the Istio
ingress gateway.</p>
<ol>
<li>
<p>Store the Kubernetes cluster name in an environment variable by running the
following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLUSTER_NAME</span><span class="o">=</span>&lt;cluster_name&gt;
</span></span></code></pre></div></li>
<li>
<p>Create a DNS domain and certificates for the IP of the service
<code>istio-ingressgateway</code> in namespace <code>istio-system</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks nlb-dns create classic --cluster <span class="nv">$CLUSTER_NAME</span> --ip <span class="nv">$INGRESS_GATEWAY_IP</span> --secret-namespace istio-system
</span></span></code></pre></div></li>
<li>
<p>List the registered domain names:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">ibmcloud ks nlb-dns ls --cluster <span class="nv">$CLUSTER_NAME</span>
</span></span></code></pre></div></li>
<li>
<p>Wait until the status of the certificate—the fourth field—of the new domain
name becomes <code>created</code>. Then, save the value of the column <code>SSL Cert Secret Name</code> in environment variables by running these commands (replace
<code>{SECRET_NAME}</code> with the secret&rsquo;s name as shown in the <code>SSL Cert Secret Name</code>
column):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">INGRESS_GATEWAY_SECRET</span><span class="o">={</span>SECRET_NAME<span class="o">}</span>
</span></span></code></pre></div><p><strong>Note</strong>: If there is more than one entry in the output, choose the one
that matches the IP address from <code>LoadBalancer Ingress</code> (step 2) of service
<code>istio-ingressgateway</code>.</p>
</li>
<li>
<p>Create a secret named <code>istio-ingressgateway-certs</code> for the
<code>istio-ingressgateway</code> pods in namespace <code>istio-system</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get secret <span class="nv">$INGRESS_GATEWAY_SECRET</span> -n istio-system -o yaml &gt; istio-ingressgateway-certs.yaml
</span></span></code></pre></div></li>
<li>
<p>Update the <code>istio-ingressgateway-certs.yaml</code> file by changing the value of
<code>metadata.name</code> to <code>istio-ingressgateway-certs</code> and the value of
<code>metadata.namespace</code> to <code>istio-system</code>. Then, run the following commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl apply -f istio-ingressgateway-certs.yaml -n istio-system
</span></span><span class="line"><span class="cl">kubectl rollout restart deploy istio-ingressgateway -n istio-system
</span></span><span class="line"><span class="cl">rm istio-ingressgateway-certs.yaml
</span></span></code></pre></div></li>
<li>
<p>Update the gateway <code>kubeflow-gateway</code> to expose port <code>443</code>. Create a resource
file <code>kubeflow-gateway.yaml</code> as follows by replacing <code>&lt;hostname&gt;</code> with the value
of the column <code>Hostname</code> in step 4:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-YAML" data-lang="YAML"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">networking.istio.io/v1alpha3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow-gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">istio</span><span class="p">:</span><span class="w"> </span><span class="l">ingressgateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">servers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="s1">&#39;&lt;hostname&gt;&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="m">443</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">HTTPS</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">tls</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l">SIMPLE</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">privateKey</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/istio/ingressgateway-certs/tls.key</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">serverCertificate</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/istio/ingressgateway-certs/tls.crt</span><span class="w">
</span></span></span></code></pre></div></li>
<li>
<p>Verify that the traffic is routed via HTTPS by using the value of
above-mentioned <code>Hostname</code> in your browser. It should redirect traffic from an
HTTP address to HTTPS address automatically.</p>
</li>
</ol>
<p><strong>Note</strong>: The certificates for the NLB DNS host secret expire every <strong>90</strong> days.
The secret in the <code>default</code> namespace is automatically renewed by IBM Cloud
Kubernetes Service 37 days before it expires. After this secret is updated, you
must manually copy it to the <code>istio-ingressgateway-certs</code> secret by repeating
commands in step 5 and 6.</p>
<h2 id="optional---kserve-configuration">Optional - KServe configuration</h2>
<p>With this HTTPS setup, you need to make additional changes to get KServe to work.</p>
<ol>
<li>
<p>First, update the Knative domain that is used for the KServe routes to the
hostname that you used when updating <code>kubeflow-gateway</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl edit configmap config-domain -n knative-serving
</span></span></code></pre></div><p>This will open your default text editor, and you will see something like:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-YAML" data-lang="YAML"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">_example</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">    ################################
</span></span></span><span class="line"><span class="cl"><span class="sd">    #                              #
</span></span></span><span class="line"><span class="cl"><span class="sd">    #    EXAMPLE CONFIGURATION     #
</span></span></span><span class="line"><span class="cl"><span class="sd">    #                              #
</span></span></span><span class="line"><span class="cl"><span class="sd">    ################################
</span></span></span><span class="line"><span class="cl"><span class="sd">    # ...
</span></span></span><span class="line"><span class="cl"><span class="sd">    example.com: |</span><span class="w">    
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">...</span><span class="w">
</span></span></span></code></pre></div><p>Add a line above the <code>_example</code> key with your hostname as the key and an empty string value.
Be sure to update <code>&lt;hostname&gt;</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-YAML" data-lang="YAML"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">&lt;hostname&gt;</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">_example</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">   </span><span class="w">   </span><span class="l">...</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">...</span><span class="w">
</span></span></span></code></pre></div><p>Then, save and exit. The routes for your InferenceServices will start using this new domain.</p>
</li>
<li>
<p>Since the certificates provided by IBM Cloud only allow for a single level of domain name added
to the base domain, the domain template for Knative needs to be adjusted so that the certificates
will be valid for the InferenceService routes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl edit configmap config-network -n knative-serving
</span></span></code></pre></div><p>This will open your default text editor, and you will again see something like:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-YAML" data-lang="YAML"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">_example</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">    ################################
</span></span></span><span class="line"><span class="cl"><span class="sd">    #                              #
</span></span></span><span class="line"><span class="cl"><span class="sd">    #    EXAMPLE CONFIGURATION     #
</span></span></span><span class="line"><span class="cl"><span class="sd">    #                              #
</span></span></span><span class="line"><span class="cl"><span class="sd">    ################################
</span></span></span><span class="line"><span class="cl"><span class="sd">    # ...</span><span class="w">    
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">...</span><span class="w">
</span></span></span></code></pre></div><p>Add a line above the <code>_example</code> key with the <code>domainTemplate</code> key like the following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-YAML" data-lang="YAML"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">domainTemplate</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;{{.Name}}-{{.Namespace}}.{{.Domain}}&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">_example</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">   </span><span class="w">   </span><span class="l">...</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">...</span><span class="w">
</span></span></span></code></pre></div><p>Save and exit. The default template uses a two-level subdomain (i.e. <code>{{.Name}}.{{.Namespace}}.{{.Domain}}</code>),
so this just adjusts it to use one.</p>
</li>
<li>
<p>Adjust <code>kubeflow-gateway</code> one more time, adding a wildcard host in the HTTPS <code>hosts</code> section.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl edit gateway kubeflow-gateway -n kubeflow
</span></span></code></pre></div><p>This will open your default text editor, but you can also optionally edit <code>kubeflow-gateway.yaml</code> file
you created previously. Here, just add another entry to the HTTPS <code>hosts</code> list containing your hostname prepended
with a <code>*.</code> so that the Knative subdomains are properly routed.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-YAML" data-lang="YAML"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">networking.istio.io/v1alpha3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow-gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">istio</span><span class="p">:</span><span class="w"> </span><span class="l">ingressgateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">servers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="s1">&#39;&lt;hostname&gt;&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="s1">&#39;*.&lt;hostname&gt;&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="m">443</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">HTTPS</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">tls</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l">SIMPLE</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">privateKey</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/istio/ingressgateway-certs/tls.key</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">serverCertificate</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/istio/ingressgateway-certs/tls.crt</span><span class="w">
</span></span></span></code></pre></div><p>Save and exit.</p>
</li>
</ol>
<p>After these adjustments, InferenceServices should now be reachable via HTTPS. To test out an external
prediction, you can use the <code>authservice_session</code> cookie for the Kubeflow dashboard site from the browser. Once
the content of the cookie is retrieved from the browser, it can be added as a header in your request
(e.g. <code>&quot;Cookie: authservice_session=MTYwNz...&quot;</code>). For example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">curl -v https://sklearn-iris-kserve-test.kf-dev-442dbba0442be6c8c50f31ed96b00532-0001.sjc03.containers.appdomain.cloud/v1/models/sklearn-iris:predict -d <span class="s1">&#39;{&#34;instances&#34;: [[6.8,  2.8,  4.8,  1.4],[6.0,  3.4,  4.5,  1.6]]}&#39;</span> -H <span class="s2">&#34;Cookie: authservice_session=MTYwODMyODk5M3xOd3dBTkVzeU5VSlJRazlQVnpWT1dGUldWa0ZXVDBRMVFsY3pVVFZHVVVGV01rWkRORmd6VmxCVVNsQkVSa2xaUlZVMFRUVldVMEU9fJBHfRCAvs6nSh_J04VlBEq_yqhkUvc5Z1Mqahe9klOd&#34;</span>
</span></span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4befd9d928cd0502f971527786e1a7ec">4.3.6 - Uninstall Kubeflow</h1>
    <div class="lead">Instructions for uninstalling Kubeflow</div>
	<p>Uninstall Kubeflow on your IBM Cloud IKS cluster.</p>
<ol>
<li>
<p>Go to your Kubeflow deployment directory where you download the
IBM manifests repository: <a href="https://github.com/IBM/manifests.git">https://github.com/IBM/manifests.git</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> ibm-manifests-160
</span></span></code></pre></div></li>
<li>
<p>Run the following command to get Kubeflow Profiles:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get profile
</span></span></code></pre></div></li>
<li>
<p>Delete all Kubeflow Profiles manually:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl delete profile --all
</span></span></code></pre></div><p>Use the following command to check all namespaces for Kubeflow Profiles
are removed properly:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get ns
</span></span></code></pre></div><p>Make sure no namespace is in the <code>Terminating</code> state.</p>
</li>
<li>
<p>Remove Kubeflow:</p>
<p>For single-user deployment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kustomize build iks-single <span class="p">|</span> kubectl delete -f -
</span></span></code></pre></div><p>For multi-user deployment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kustomize build iks-multi <span class="p">|</span> kubectl delete -f -
</span></span></code></pre></div></li>
</ol>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-365cfb913c393415a0eac321a2efbf41">4.4 - Pipelines on IBM Cloud Kubernetes Service (IKS)</h1>
    <div class="lead">Instructions for using Kubeflow Pipelines on IBM Cloud Kubernetes Service (IKS)</div>
	<p>By default, Kubeflow Pipelines on IBM Cloud are running with the Tekton backend. In this guide you&rsquo;ll learn how to use the Kubeflow Pipelines with the Tekton backend <a href="https://github.com/kubeflow/kfp-tekton">(kfp-tekton)</a>. This assumes you have deployed <a href="https://www.kubeflow.org/docs/ibm/deploy/">Kubeflow on IBM Cloud using the instructions on this website</a>.</p>
<p>You can also do a <a href="https://github.com/kubeflow/kfp-tekton/blob/master/guides/kfp_tekton_install.md#standalone-kubeflow-pipelines-with-tekton-backend-deployment">standalone installation of Kubeflow Pipelines with Tekton</a> if you don&rsquo;t want whole of Kubeflow.</p>
<img src="/docs/ibm/kfp-tekton.png" alt="KFP-Tekton">
<p>In this tutorial, we use the below single step pipeline as our example</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kfp</span> <span class="kn">import</span> <span class="n">dsl</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">echo_op</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dsl</span><span class="o">.</span><span class="n">ContainerOp</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;echo&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">image</span><span class="o">=</span><span class="s1">&#39;busybox&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">command</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sh&#39;</span><span class="p">,</span> <span class="s1">&#39;-c&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">arguments</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;echo &#34;Got scheduled&#34;&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@dsl</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;echo&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">description</span><span class="o">=</span><span class="s1">&#39;echo pipeline&#39;</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">echo_pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">echo</span> <span class="o">=</span> <span class="n">echo_op</span><span class="p">()</span>
</span></span></code></pre></div><h1 id="declare-the-python-client-for-kubeflow-pipelines">Declare the Python Client for Kubeflow Pipelines</h1>
<h2 id="1-single-user-kubeflow-pipelines-deployment-with-the-sdk">1. Single-user Kubeflow Pipelines deployment with the SDK</h2>
<ul>
<li>You will be using the Kubeflow Pipelines with Tekton SDK (<a href="https://pypi.org/project/kfp-tekton/"><code>kfp-tekton</code></a>) v0.4.0 or above.</li>
<li>If you have deployed Kubeflow on IBM Cloud using the
<a href="https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_ibm.v1.2.0.yaml"><code>kfctl_ibm.v1.2.0.yaml</code></a>
manifest you can configure (<a href="https://pypi.org/project/kfp-tekton/"><code>kfp-tekton</code></a>) SDK to list all your Kubeflow Pipelines experiments as follows:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kfp_tekton</span> <span class="kn">import</span> <span class="n">TektonClient</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">KUBEFLOW_PUBLIC_ENDPOINT_URL</span> <span class="o">=</span> <span class="s1">&#39;http://&lt;YOUR_KF_PUBLIC_ENDPOINT_URL&gt;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">KUBEFLOW_PROFILE_NAME</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">client</span> <span class="o">=</span> <span class="n">TektonClient</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">KUBEFLOW_PUBLIC_ENDPOINT_URL</span><span class="si">}</span><span class="s1">/pipeline&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">experiments</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">list_experiments</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="n">KUBEFLOW_PROFILE_NAME</span><span class="p">)</span>
</span></span></code></pre></div><p><strong>Note</strong>: <code>&lt;YOUR_KF_PUBLIC_ENDPOINT_URL&gt;</code> is the EXTERNAL_IP you exposed as a LoadBalancer following <a href="https://www.kubeflow.org/docs/ibm/deploy/install-kubeflow-on-iks/#expose-the-kubeflow-endpoint-as-a-loadbalancer"><code>this instruction</code></a>. If you have not done that step during Kubeflow setup, please include port 31380 because the Kubeflow endpoint is exposed with NodePort 31380.</p>
<h2 id="2-authenticating-multi-user-kubeflow-pipelines-with-the-sdk">2. Authenticating multi-user Kubeflow Pipelines with the SDK</h2>
<ul>
<li>You will be using the Kubeflow Pipelines SDK (<a href="https://pypi.org/project/kfp-tekton/"><code>kfp-tekton</code></a>) v0.4.0 or above.</li>
<li>Note that this feature is available with multi-user, auth-enabled Kubeflow installation deployed from the <a href="https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_ibm_multi_user.v1.2.0.yaml"><code>kfctl_ibm_multi_user.v1.2.0.yaml</code></a>
manifest.</li>
</ul>
<p><strong>Note</strong>: You&rsquo;re highly recommended enabling HTTPS for the public endpoint of Kubeflow because this method transports sensitive information like session cookie values over edge network.</p>
<p>It requires authentication via the public endpoint of Kubeflow deployment when using the Kubeflow Pipelines multi-user feature with Pipelines SDK.</p>
<p>You need to provide the following three variables if you&rsquo;re using an in-cluster Jupyter notebook or a remote client machine:</p>
<ol>
<li><code>KUBEFLOW_PUBLIC_ENDPOINT_URL</code> - Kubeflow public endpoint URL. You can obtain it from command <code>ibmcloud ks nlb-dns ls --cluster &lt;your-cluster-name&gt;</code>.</li>
<li><code>SESSION_COOKIE</code> - A session cookie starts with <code>authservice_session=</code>. You can obtain it from your browser after authenticated from Kubeflow UI. Notice that this session cookie expires in 24 hours, so you need to obtain it again after cookie expired.</li>
<li><code>KUBEFLOW_PROFILE_NAME</code> - Your Kubeflow profile name</li>
</ol>
<p>Once you provide the three variables, the SDK can use the following Python code to list all your Kubeflow Pipelines experiments:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kfp_tekton</span> <span class="kn">import</span> <span class="n">TektonClient</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">KUBEFLOW_PUBLIC_ENDPOINT_URL</span> <span class="o">=</span> <span class="s1">&#39;https://xxxx.&lt;region-name&gt;.containers.appdomain.cloud&#39;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># this session cookie looks like &#34;authservice_session=xxxxxxx&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">SESSION_COOKIE</span> <span class="o">=</span> <span class="s1">&#39;authservice_session=xxxxxxx&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">KUBEFLOW_PROFILE_NAME</span> <span class="o">=</span> <span class="s1">&#39;&lt;your-profile-name&gt;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">client</span> <span class="o">=</span> <span class="n">TektonClient</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">host</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">KUBEFLOW_PUBLIC_ENDPOINT_URL</span><span class="si">}</span><span class="s1">/pipeline&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">cookies</span><span class="o">=</span><span class="n">SESSION_COOKIE</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">experiments</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">list_experiments</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="n">KUBEFLOW_PROFILE_NAME</span><span class="p">)</span>
</span></span></code></pre></div><p>Pipelines components like experiments and runs are isolated by Kubeflow profiles. A Kubeflow user can only see Pipelines experiments and runs belonging to this user&rsquo;s Kubeflow profile.</p>
<h1 id="upload-pipelines">Upload pipelines</h1>
<p>Once you have declared the Python client, your Kubeflow pipelines can be uploaded using Python.</p>
<p>Run the following code inside a Python session to upload the pipelines. This example shows different versions of the pipeline using the Python client.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initial version of the compiled pipeline</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline_file_path</span> <span class="o">=</span> <span class="s1">&#39;echo_pipeline.yaml&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline_name</span> <span class="o">=</span> <span class="s1">&#39;echo_pipeline&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># For the purpose of this tutorial, we will be using the same pipeline for both version.</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline_version_file_path</span> <span class="o">=</span> <span class="s1">&#39;echo_pipeline.yaml&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline_version_name</span> <span class="o">=</span> <span class="s1">&#39;new_echo_pipeline&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Upload initial version of the pipeline</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pipeline_file_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">pipeline_uploads</span><span class="o">.</span><span class="n">upload_pipeline</span><span class="p">(</span><span class="n">pipeline_file</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">pipeline_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Upload new version of the pipeline</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline_version_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pipeline_version_file_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline_version</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">pipeline_uploads</span><span class="o">.</span><span class="n">upload_pipeline_version</span><span class="p">(</span><span class="n">pipeline_version_file</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                   <span class="n">name</span><span class="o">=</span><span class="n">pipeline_version_name</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                   <span class="n">pipelineid</span><span class="o">=</span><span class="n">pipeline</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>
</span></span></code></pre></div><h1 id="run-pipelines-from-the-sdk">Run pipelines from the SDK</h1>
<p>The <code>TektonClient</code> can run pipelines using one of the below sources:</p>
<ol>
<li><a href="#run-pipelines-from-the-python-dsl-source-code">Python DSL source code</a></li>
<li><a href="#run-pipelines-from-the-compiled-pipeline-file">Compiled pipeline file</a></li>
<li><a href="#run-pipelines-from-the-list-of-uploaded-pipelines">Uploaded pipelines on KFP engine</a></li>
</ol>
<h2 id="run-pipelines-from-the-python-dsl-source-code">Run pipelines from the Python DSL source code</h2>
<p>To learn about executing pipelines using the Python DSL source code, try the code below in a Python session using the <code>echo_pipeline</code> example.
The <code>create_run_from_pipeline_func</code> takes the DSL source code to compile and run it directly using the Kubeflow pipeline API without
uploading it to the pipeline list. This method is recommended if you are doing quick experiments without version control.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># You can overwrite the pipeline default parameters by providing a dictionary of key-value arguments.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># If you don&#39;t want to overwrite the default parameters, then define the arguments as an empty dictionary.</span>
</span></span><span class="line"><span class="cl"><span class="n">arguments</span><span class="o">=</span><span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">client</span><span class="o">.</span><span class="n">create_run_from_pipeline_func</span><span class="p">(</span><span class="n">echo_pipeline</span><span class="p">,</span> <span class="n">arguments</span><span class="o">=</span><span class="n">arguments</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="n">KUBEFLOW_PROFILE_NAME</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="run-pipelines-from-the-compiled-pipeline-file">Run pipelines from the compiled pipeline file</h2>
<p>Alternatively, you can also run the pipeline directly using a pre-compiled file.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">EXPERIMENT_NAME</span> <span class="o">=</span> <span class="s1">&#39;Demo Experiments&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">experiment</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">create_experiment</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">EXPERIMENT_NAME</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="n">KUBEFLOW_PROFILE_NAME</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">run</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">run_pipeline</span><span class="p">(</span><span class="n">experiment</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="s1">&#39;echo-pipeline&#39;</span><span class="p">,</span> <span class="s1">&#39;echo_pipeline.yaml&#39;</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="run-pipelines-from-the-list-of-uploaded-pipelines">Run pipelines from the list of uploaded pipelines</h2>
<p>Similarly, you can also run the pipeline from the list of uploaded pipelines using the same <code>run_pipeline</code> function.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">EXPERIMENT_NAME</span> <span class="o">=</span> <span class="s1">&#39;Demo Experiments&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">experiment</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">create_experiment</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">EXPERIMENT_NAME</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="n">KUBEFLOW_PROFILE_NAME</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Find the pipeline ID that you want to use.</span>
</span></span><span class="line"><span class="cl"><span class="n">client</span><span class="o">.</span><span class="n">list_pipelines</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">run</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">run_pipeline</span><span class="p">(</span><span class="n">experiment</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="n">pipeline_id</span><span class="o">=</span><span class="s1">&#39;925415d5-18e9-4e08-b57f-3b06e3e54648&#39;</span><span class="p">,</span> <span class="n">job_name</span><span class="o">=</span><span class="s1">&#39;echo_pipeline_run&#39;</span><span class="p">)</span>
</span></span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-5da5a179b77d3d1cc1c87723ac674d84">4.5 - Using IBM Cloud Container Registry (ICR)</h1>
    <div class="lead">This guide covers certain scenarios involving using private container images with the IBM Cloud Container Registry (ICR).</div>
	<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Install and configure the <a href="https://cloud.ibm.com/docs/cli?topic=cli-getting-started">IBM Cloud CLI</a>.</li>
<li>Install the CLI plug-in for the IBM Cloud Container Registry by running the command <code>ibmcloud plugin install container-registry</code>.</li>
<li>Create a namespace in ICR with the command <code>ibmcloud cr namespace-add &lt;my_namespace&gt;</code>, replace <code>&lt;my_namespace&gt;</code> with your preferred name.</li>
</ul>
<p><strong>Note</strong>: The <a href="https://cloud.ibm.com/docs/Registry?topic=Registry-getting-started#gs_registry_namespace_add">ICR namespace</a> is different from the Kubeflow Profile namespace. The ICR namespace is used to group container images stored in ICR, while a <a href="/docs/components/multi-tenancy/overview/">Kubeflow Profile</a> namespace is a group of all Kubernetes clusters owned by a user.</p>
<h2 id="image-pull-secret">Image pull secret</h2>
<p>As a Kubernetes cluster uses the Secret of <code>docker-registry</code> type to authenticate with a container registry to pull a private image, it needs an image pull secret to pull container images from IBM Cloud Container Registry. You can use the default image pull secret set up by the cluster or your account&rsquo;s IAM API key.</p>
<h3 id="using-a-default-image-pull-secret">Using a default image pull secret</h3>
<p>By default, the IBM Cloud Kubernetes cluster is set up to pull images from only your account&rsquo;s namespace in IBM Cloud Container Registry by using the secret <code>all-icr-io</code> in the <code>default</code> namespace. A cluster admin can copy this secret to any Kubernetes namespace used as Kubeflow profile. For example, run below command to copy the secret <code>all-icr-io</code> to the <code>anonymous</code> namespace:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get secret all-icr-io -n default -o yaml \
</span></span><span class="line"><span class="cl">| sed &#39;s/namespace: default/namespace: anonymous/g&#39; \
</span></span><span class="line"><span class="cl">| kubectl -n anonymous create -f -
</span></span></code></pre></div><p>Once this secret is ready in your Kubeflow profile, a data scientist can use it to pull container images from ICR.</p>
<p>See details and FAQs from the official guide <a href="https://cloud.ibm.com/docs/containers?topic=containers-registry">Setting up an image registry</a>.</p>
<h3 id="getting-an-iam-api-key">Getting an IAM API Key</h3>
<p>You will need an IBM Cloud IAM API Key to work with ICR if you:</p>
<ol>
<li>Have no access to the default image pull secret <code>all-icr-io</code> from the <code>default</code> namespace.</li>
<li>Need to access container images in other IBM Cloud accounts.</li>
<li>Need customized IAM policy by using a separate IAM service ID.</li>
</ol>
<p>If you don&rsquo;t have an IBM Cloud IAM API Key, follow the official guide <a href="https://cloud.ibm.com/docs/account?topic=account-userapikey#create_user_key">Create an API Key</a>.</p>
<p>Once you get your IBM Cloud IAM API Key, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n &lt;my_namespace&gt; create secret docker-registry &lt;secret_name&gt; \
</span></span><span class="line"><span class="cl">--docker-server=&lt;registry_domain_name&gt; \
</span></span><span class="line"><span class="cl">--docker-username=iamapikey \
</span></span><span class="line"><span class="cl">--docker-password=&lt;ibm_cloud_iam_api_key&gt; \
</span></span><span class="line"><span class="cl">--docker-email=&lt;docker_email&gt;
</span></span></code></pre></div><p><strong>Notes</strong>:</p>
<ul>
<li><code>&lt;my_namespace&gt;</code>: your namespace to use with ICR to create an image pull secret.</li>
<li><code>&lt;ibm_cloud_iam_api_key&gt;</code>: your IBM Cloud API Key.</li>
<li><code>&lt;secret_name&gt;</code>: a unique name for the pull image secret, such as <code>us-icr-io</code>, for example.</li>
<li><code>&lt;registry_domain_name&gt;</code>: the image registry where your registry namespace is set up. Use regional domain name when storing container images in specific region, such as <code>us.icr.io</code> when using region <code>en-us</code> and <code>uk.icr.io</code> when using region <code>eu-gb</code>. See full list of regional domain names from the <a href="https://cloud.ibm.com/docs/Registry?topic=Registry-registry_overview#registry_regions_local">About IBM Cloud Container Registry page</a>.</li>
<li><code>&lt;docker_email&gt;</code>: your docker email address or any fictional email address, such as <code>a@b.c</code>.</li>
</ul>
<h2 id="scenarios">Scenarios</h2>
<h3 id="using-container-images-from-icr-in-kubeflow-pipelines">Using container images from ICR in Kubeflow Pipelines</h3>
<p>The pull image secret may be set in Kubeflow Pipelines SDK&rsquo;s <a href="https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.dsl.html#kfp.dsl.PipelineConf"><code>PipelineConf</code></a>. Refer to this <a href="https://github.com/kubeflow/pipelines/blob/ef381aafccf916482d16774cac3b8568d06dff9e/samples/core/imagepullsecrets/imagepullsecrets.py#L55"><code>imagepullsecrets.py</code></a> sample in Kubeflow Pipelines project for usage.</p>
<h3 id="using-notebook-images-from-icr-in-a-jupyter-notebook">Using notebook images from ICR in a Jupyter Notebook</h3>
<p>When a namespace is created for Kubeflow with its profile controller, a default service account <code>default-editor</code> is created in that namespace. Before creating a Notebook Server, run following command to patch the service account. Replace <code>&lt;secret_name&gt;</code> with the ICR pull image secret name and <code>&lt;my_namespace&gt;</code> with the Kubeflow profile namespace.</p>
<p>Replace <code>&lt;my_namespace&gt;</code> with your namespace then run below command to patch the service account <code>default-editor</code> with this image pull secret:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-SHELL" data-lang="SHELL"><span class="line"><span class="cl">kubectl patch serviceaccount default-editor <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>-p <span class="s1">&#39;{&#34;imagePullSecrets&#34;: [{&#34;name&#34;: &#34;&lt;secret-name&gt;&#34;}]}&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>-n &lt;my_namespace&gt;
</span></span></code></pre></div><p>The service account should be updated. Then, when you create the Notebook Server through Kubeflow dashboard, you should be able to choose a Custom Image. Afterwards, set the notebook image path from the ICR as follows:</p>
<p><img src="/docs/images/ibm/notebook-custom-image.png" 
alt="Notebook Custom Image"
class="mt-3 mb-3 border border-info rounded"></p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d51039ac0de408f10e0b3a994766675d">4.6 - End-to-end Kubeflow on IBM Cloud</h1>
    <div class="lead">Running Kubeflow using IBM Cloud Kubernetes Service (IKS)</div>
	<p>This is a guide for an end-to-end example of Kubeflow on <a href="https://cloud.ibm.com/docs/containers?topic=containers-getting-started">IBM Cloud Kubernetes Service (IKS)</a>. The core steps will be to take a base Tensorflow model, modify it for distributed training, serve the resulting model with TFServing, and deploy a web application that uses the trained model.</p>
<h2 id="introduction">Introduction</h2>
<h3 id="overview-of-iks">Overview of IKS</h3>
<p><a href="https://cloud.ibm.com/docs/containers?topic=containers-getting-started">IBM Cloud Kubernetes Service (IKS)</a> enables the deployment of containerized applications in Kubernetes clusters with specialized tools for management of the systems.</p>
<p>The <a href="https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started">IBM Cloud CLI</a> can be used for creating, developing, and deploying cloud applications.</p>
<p>Here&rsquo;s a list of IBM Cloud services you will use:</p>
<ul>
<li><a href="https://www.ibm.com/cloud/container-service/">IKS</a></li>
<li><a href="https://www.ibm.com/cloud/object-storage">IBM Cloud Object Storage</a></li>
</ul>
<h3 id="the-model-and-the-data">The model and the data</h3>
<p>This tutorial trains a <a href="https://www.tensorflow.org/">TensorFlow</a> model on the
<a href="http://yann.lecun.com/exdb/mnist/index.html">MNIST dataset</a>, which is the <em>hello world</em> for machine learning.</p>
<p>The MNIST dataset contains a large number of images of hand-written digits in
the range 0 to 9, as well as the labels identifying the digit in each image.</p>
<p>After training, the model can classify incoming images into 10 categories
(0 to 9) based on what it&rsquo;s learned about handwritten images. In other words,
you send an image to the model, and the model does its best to identify the
digit shown in the image.
<img src="/docs/images/gcp-e2e-ui-prediction.png"
alt="Prediction UI"
class="mt-3 mb-3 p-3 border border-info rounded"></p>
<p>In the above screenshot, the image shows a hand-written <strong>7</strong>. This image was
the input to the model. The table below the image shows a bar graph for each
classification label from 0 to 9, as output by the model. Each bar
represents the probability that the image matches the respective label.
Judging by this screenshot, the model seems pretty confident that this image
is a 7.</p>
<h3 id="the-overall-workflow">The overall workflow</h3>
<p>The following diagram shows what you accomplish by following this guide:</p>
<p><img src="/docs/images/ibm-e2e-kubeflow.png" 
alt="ML workflow for training and serving an MNIST model"
class="mt-3 mb-3 border border-info rounded"></p>
<p>In summary:</p>
<ul>
<li>Setting up <a href="https://www.kubeflow.org/">Kubeflow</a> on <a href="https://www.ibm.com/cloud/container-service/">IKS</a>.</li>
<li>Training the model:
<ul>
<li>Packaging a Tensorflow program in a container.</li>
<li>Submitting a Tensorflow training (<a href="https://www.tensorflow.org/api_guides/python/train">tf.train</a>) job.</li>
</ul>
</li>
<li>Using the model for prediction (inference):
<ul>
<li>Saving the trained model to <a href="https://www.ibm.com/cloud/object-storage">IBM Cloud Object Storage</a>.</li>
<li>Using <a href="https://www.tensorflow.org/tfx/guide/serving">Tensorflow Serving</a> to serve the model.</li>
<li>Running the simple web app to send prediction request to the model and display the result.</li>
</ul>
</li>
</ul>
<p>It&rsquo;s time to get started!</p>
<h2 id="run-the-mnist-tutorial-on-iks">Run the MNIST Tutorial on IKS</h2>
<ol>
<li>Follow the <a href="/docs/ibm/deploy/install-kubeflow">IKS instructions</a> to deploy Kubeflow.</li>
<li>Launch a Jupyter notebook.</li>
<li>Launch a terminal in Jupyter and clone the Kubeflow examples repo.
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">git clone https://github.com/kubeflow/examples.git git_kubeflow-examples
</span></span></code></pre></div><ul>
<li><strong>Tip</strong>: When you start a terminal in Jupyter, run the command <code>bash</code> to start
a bash terminal which is much more friendly than the default shell.</li>
<li><strong>Tip</strong>: You can change the URL for your notebook from &lsquo;/tree&rsquo; to &lsquo;/lab&rsquo; to switch to using Jupyterlab.</li>
</ul>
</li>
<li>Open the notebook <code>mnist/mnist_ibm.ipynb</code>.</li>
<li>Follow the notebook to train and deploy MNIST on Kubeflow.</li>
</ol>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f0cfc318fbada4d21ed442fc9bc7b049">5 - Kubeflow on Nutanix Karbon</h1>
    <div class="lead">Running Kubeflow on Nutanix Karbon</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-c4b39053be31a6a0e8e923da0b738a30">5.1 - Install Kubeflow on Nutanix Karbon</h1>
    <div class="lead">How to deploy Kubeflow on a Nutanix Karbon cluster</div>
	<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>
<p>Make sure you first create a Kubernetes cluster using Nutanix Karbon. See <a href="https://portal.nutanix.com/page/documents/details?targetId=Karbon-v2_2:kar-karbon-deploy-karbon-t.html">Nutanix Karbon documentation</a> at the Nutanix Support Portal.</p>
</li>
<li>
<p>Install <a href="https://www.terraform.io/downloads.html">Terraform</a> based on your platform</p>
</li>
<li>
<p>Install kubectl from <a href="https://kubernetes.io/docs/tasks/tools/#kubectl">Install Tools</a></p>
</li>
<li>
<p>Download <a href="https://portal.nutanix.com/page/documents/details?targetId=Karbon-v2_2:kar-karbon-download-kubeconfig-t.html">Kubeconfig</a> of your deployed Karbon cluster.</p>
</li>
</ul>
<h2 id="installing-kubeflow">Installing Kubeflow</h2>
<p>Do these steps to deploy Kubeflow 1.6.0 on your Karbon cluster.</p>
<ol>
<li>
<p>Download the terraform script to deploy kubeflow on Nutanix Karbon by cloning the Github repository shown.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">git clone https://github.com/nutanix/karbon-platform-services.git
</span></span><span class="line"><span class="cl">cd automation/infrastructure/terraform/kcs/install_kubeflow
</span></span></code></pre></div></li>
<li>
<p>Create <code>env.tfvars</code> file in the same folder with the following cluster variables. Override other variables from variables.tf file if required.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">prism_central_username = &#34;enter username&#34;
</span></span><span class="line"><span class="cl">prism_central_password = &#34;enter password&#34;
</span></span><span class="line"><span class="cl">prism_central_endpoint = &#34;enter endpoint_ip_or_host_fqdn&#34;
</span></span><span class="line"><span class="cl">karbon_cluster_name    = &#34;enter karbon_cluster_name&#34;
</span></span><span class="line"><span class="cl">kubeconfig_filename    = &#34;enter karbon_cluster_name-kubectl.cfg&#34;
</span></span><span class="line"><span class="cl">kubeflow_version       = &#34;1.6.0&#34;
</span></span></code></pre></div></li>
<li>
<p>Apply terraform commands to deploy Kubeflow in the cluster.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">terraform init
</span></span><span class="line"><span class="cl">terraform plan --var-file=env.tfvars
</span></span><span class="line"><span class="cl">terraform apply --var-file=env.tfvars
</span></span></code></pre></div></li>
<li>
<p>Make sure all the pods are running before continuing to the next step.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ kubectl -n kubeflow get pods
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">NAME                                                         READY   STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">admission-webhook-deployment-65dcd649d8-468g9                1/1     Running   0          3m39s
</span></span><span class="line"><span class="cl">cache-deployer-deployment-6b78494889-6lfg9                   2/2     Running   1          3m1s
</span></span><span class="line"><span class="cl">cache-server-bff956474-lm952                                 2/2     Running   0          3m
</span></span><span class="line"><span class="cl">centraldashboard-6b5fb79878-h9dqn                            1/1     Running   0          3m40s
</span></span><span class="line"><span class="cl">jupyter-web-app-deployment-75559c6c87-mt4q2                  1/1     Running   0          3m1s
</span></span><span class="line"><span class="cl">katib-controller-79f44b76bb-t7rzl                            1/1     Running   0          3m
</span></span><span class="line"><span class="cl">katib-db-manager-6d9857f658-p4786                            1/1     Running   0          2m59s
</span></span><span class="line"><span class="cl">katib-mysql-586f79b694-2qcl5                                 1/1     Running   0          2m59s
</span></span><span class="line"><span class="cl">katib-ui-5fdb7869cf-jmssr                                    1/1     Running   0          3m
</span></span><span class="line"><span class="cl">kfserving-controller-manager-0                               2/2     Running   0          3m15s
</span></span><span class="line"><span class="cl">kubeflow-pipelines-profile-controller-6cfd6bf9bd-cptgg       1/1     Running   0          2m59s
</span></span><span class="line"><span class="cl">metacontroller-0                                             1/1     Running   0          3m15s
</span></span><span class="line"><span class="cl">metadata-envoy-deployment-6756c995c9-gqkbd                   1/1     Running   0          3m
</span></span><span class="line"><span class="cl">metadata-grpc-deployment-7cb87744c7-4crm9                    2/2     Running   3          3m40s
</span></span><span class="line"><span class="cl">metadata-writer-6bf5cfd7d8-fgq9f                             2/2     Running   0          3m40s
</span></span><span class="line"><span class="cl">minio-5b65df66c9-9z7mg                                       2/2     Running   0          2m59s
</span></span><span class="line"><span class="cl">....
</span></span></code></pre></div></li>
</ol>
<h2 id="add-a-new-kubeflow-user">Add a new Kubeflow user</h2>
<p>New users are created using the Profile resource. A new namespace is created with the same Profile name. For creating a new user with email <code>user@example.com</code> in a namespace <code>project1</code>, apply the following profile</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cat &lt;&lt;EOF | kubectl apply -f -
</span></span><span class="line"><span class="cl">apiVersion: kubeflow.org/v1beta1
</span></span><span class="line"><span class="cl">kind: Profile
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">    name: project1   # replace with the name of profile you want, this will be the user&#39;s namespace name
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">    owner:
</span></span><span class="line"><span class="cl">        kind: User
</span></span><span class="line"><span class="cl">        name: user2@example.com   # replace with the user email
</span></span><span class="line"><span class="cl">EOF
</span></span></code></pre></div><p>If you are using basic authentication, add the user credentials in dex which is the default OpenId Connect provider in Kubeflow. Generate the hash by using bcrypt (available at <a href="https://bcrypt-generator.com">https://bcrypt-generator.com</a>) in the following configmap</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl edit cm dex -o yaml -n auth
</span></span></code></pre></div><p>Add the following  under staticPasswords section</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">- email: user2@example.com
</span></span><span class="line"><span class="cl">  hash: &lt;hash&gt;
</span></span><span class="line"><span class="cl">  username: user2
</span></span></code></pre></div><h2 id="setup-a-loadbalancer-optional">Setup a LoadBalancer (Optional)</h2>
<p>If you already have a load balancer set up for your Karbon cluster, you can skip this step. If you do not wish to
expose the kubeflow dashboard to an external load balancer IP, you can also skip this step.
If not, you can install the <a href="https://metallb.universe.tf/">MetalLB</a> load balancer manifests on your Karbon cluster.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/namespace.yaml
</span></span><span class="line"><span class="cl">$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/metallb.yaml
</span></span></code></pre></div><p>After the manifests have been applied, we need to configure MetalLB with the IP range that it can use to assign external IPs to services of type LoadBalancer. You can find the range from the subnet in Prism Central’s <a href="https://portal.nutanix.com/page/documents/details?targetId=Nutanix-Flow-Networking-Guide:ear-flow-nw-view-subnet-list-pc-r.html">networking and security</a> settings.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">kind: ConfigMap
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  namespace: metallb-system
</span></span><span class="line"><span class="cl">  name: config
</span></span><span class="line"><span class="cl">data:
</span></span><span class="line"><span class="cl">  config: |
</span></span><span class="line"><span class="cl">    address-pools:
</span></span><span class="line"><span class="cl">      - name: default
</span></span><span class="line"><span class="cl">        protocol: layer2
</span></span><span class="line"><span class="cl">        addresses:
</span></span><span class="line"><span class="cl">        - &lt;IP_ADDRESS_RANGE: x.x.x.x-x.x.x.x&gt;
</span></span></code></pre></div><p>Create a ConfigMap with the following information, substitute the addresses field with your IP address range, and apply it to the cluster.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ kubectl apply -f metallb-configmap.yaml
</span></span></code></pre></div><h2 id="access-kubeflow-central-dashboard">Access Kubeflow Central Dashboard</h2>
<p>There are multiple ways to acces your Kubeflow Central Dashboard:</p>
<ul>
<li>
<p>Port Forward: The default way to access Kubeflow Central Dashboard is by using Port-Forward. You can port forward the istio ingress gateway to local port 8080.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl --kubeconfig=&lt;karbon_k8s_cluster_kubeconfig_path&gt; port-forward svc/istio-ingressgateway -n istio-system 8080:80
</span></span></code></pre></div><p>You can now access the Kubeflow Central Dashboard at http://localhost:8080. At the Dex login page, enter user credentials that you previously created.</p>
</li>
<li>
<p>NodePort: For accessing through NodePort, you need to configure HTTPS. Create a certificate using cert-manager for your Worker node IP in your cluster. Add HTTPS to kubeflow gateway as given in <a href="https://istio.io/latest/docs/tasks/traffic-management/ingress/secure-ingress/">Istio Secure Gateways</a>. Then access your cluster at</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">https://&lt;worknernode-ip&gt;:&lt;https-nodeport&gt;
</span></span></code></pre></div></li>
<li>
<p>LoadBalancer: If you have a LoadBalancer set up (See optional &ldquo;Setup a LoadBalancer&rdquo; section above), you can access the dashboard using the external IP by making the following changes.</p>
<ul>
<li>Update Istio Gateway to expose port 443 with HTTPS and make port 80 redirect to 443:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n kubeflow edit gateways.networking.istio.io kubeflow-gateway
</span></span></code></pre></div>The updated gateway spec should look like:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">networking.istio.io/v1alpha3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow-gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">istio</span><span class="p">:</span><span class="w"> </span><span class="l">ingressgateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">servers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="s1">&#39;*&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">HTTP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="c"># Upgrade HTTP to HTTPS</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">tls</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">httpsRedirect</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="s1">&#39;*&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="m">443</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">HTTPS</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">tls</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l">SIMPLE</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">privateKey</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/istio/ingressgateway-certs/tls.key</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">serverCertificate</span><span class="p">:</span><span class="w"> </span><span class="l">/etc/istio/ingressgateway-certs/tls.crt</span><span class="w">
</span></span></span></code></pre></div></li>
<li>Change the type of the istio-ingressgateway service to LoadBalancer
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system  patch service istio-ingressgateway -p &#39;{&#34;spec&#34;: {&#34;type&#34;: &#34;LoadBalancer&#34;}}&#39;
</span></span></code></pre></div>Get the IP address for the <code>LoadBalancer</code>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system get svc istio-ingressgateway -o jsonpath=&#39;{.status.loadBalancer.ingress[0]}&#39;
</span></span></code></pre></div>Set the <code>REDIRECT_URL</code> in <code>oidc-authservice-parameters</code> configmap to something like <code>https://x.x.x.x/login/oidc</code> where the <code>x.x.x.x</code> is the IP address of your istio-ingressgateway.
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system edit configmap oidc-authservice-parameters
</span></span></code></pre></div>Append the same to the <code>redirectURIs</code> list in <code>dex</code> configmap
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n auth edit configmap dex
</span></span></code></pre></div>Rollout restart authservice and dex
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system rollout restart statefulset authservice
</span></span><span class="line"><span class="cl">kubectl -n auth rollout restart deployment dex
</span></span></code></pre></div>Create a <code>certificate.yaml</code> with the YAML below to create a self-signed certificate
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: cert-manager.io/v1alpha2
</span></span><span class="line"><span class="cl">kind: Certificate
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: istio-ingressgateway-certs
</span></span><span class="line"><span class="cl">  namespace: istio-system
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  commonName: istio-ingressgateway.istio-system.svc
</span></span><span class="line"><span class="cl">  ipAddresses:
</span></span><span class="line"><span class="cl">    - &lt;ISTIO_INGRESSGATEWAY_IP_ADDRESS: x.x.x.x&gt;
</span></span><span class="line"><span class="cl">  isCA: true
</span></span><span class="line"><span class="cl">  issuerRef:
</span></span><span class="line"><span class="cl">    kind: ClusterIssuer
</span></span><span class="line"><span class="cl">    name: kubeflow-self-signing-issuer
</span></span><span class="line"><span class="cl">  secretName: istio-ingressgateway-certs
</span></span></code></pre></div>Apply <code>certificate.yaml</code> to the <code>istio-system</code> namespace
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n istio-system apply -f certificate.yaml
</span></span></code></pre></div></li>
<li>You can now access the kubeflow dashboard by navigating to the istio-ingressgateway external IP e.g. <code>x.x.x.x</code></li>
</ul>
</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-f1ea8fcd5328b8423a137feaddd395e9">5.2 - Integrate with Nutanix Storage</h1>
    <div class="lead">How to integrate Nutanix Storage in Kubeflow</div>
	<h2 id="nutanix-objects-in-kubeflow-pipeline">Nutanix Objects in Kubeflow Pipeline</h2>
<p>You can use standard s3 boto api to upload and download objects from a Kubeflow Pipeline. See <a href="https://portal.nutanix.com/page/documents/details?targetId=Objects-v3_2:Objects-v3_2">Nutanix Objects Docs</a> for more details on creating object store and buckets.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">import boto3
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">bucket_name=&#34;ml-pipeline-storage&#34;
</span></span><span class="line"><span class="cl">object_name=&#34;models&#34;
</span></span><span class="line"><span class="cl">object_store_access_key_id=&#34;&lt;key_id&gt;&#34;
</span></span><span class="line"><span class="cl">object_store_secret_access_key=&#34;&lt;access_key&gt;&#34;
</span></span><span class="line"><span class="cl">host=&#34;http://&lt;Nutanix Objects Store Domain Name&gt;&#34;
</span></span><span class="line"><span class="cl">region_name=&#39;us-west-1&#39;
</span></span><span class="line"><span class="cl">s3_client = boto3.client(
</span></span><span class="line"><span class="cl">                 &#39;s3&#39;,
</span></span><span class="line"><span class="cl">                 endpoint_url=host,
</span></span><span class="line"><span class="cl">                 aws_access_key_id=object_store_access_key_id,
</span></span><span class="line"><span class="cl">                 aws_secret_access_key=object_store_secret_access_key,
</span></span><span class="line"><span class="cl">                 region_name=region_name,
</span></span><span class="line"><span class="cl">                 verify=False)
</span></span><span class="line"><span class="cl">response = s3_client.upload_file(f&#39;./test_upload_data.txt&#39;, bucket_name, object_name)
</span></span></code></pre></div><h2 id="nutanix-volumes-in-kubeflow-pipeline">Nutanix Volumes in Kubeflow Pipeline</h2>
<p>Nutanix volumes are created with the default storage class configured in the Karbon cluster. See <a href="https://portal.nutanix.com/page/documents/details?targetId=Karbon-v2_2:kar-karbon-storage-class-r.html">Default Storage Class</a> of Nutanix Karbon for more details about creating storage classes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">vop = dsl.VolumeOp(
</span></span><span class="line"><span class="cl">   name=&#34;Create a volume to share data between stages on Nutanix Volumes&#34;,
</span></span><span class="line"><span class="cl">   resource_name=&#34;data-volume&#34;,
</span></span><span class="line"><span class="cl">   size=&#34;1Gi&#34;,
</span></span><span class="line"><span class="cl">   modes=dsl.VOLUME_MODE_RWO)
</span></span></code></pre></div><h2 id="nutanix-files-in-kubeflow-pipeline">Nutanix Files in Kubeflow Pipeline</h2>
<p>Create a storage class to dynamically provision Nutanix File shares. See <a href="https://portal.nutanix.com/page/documents/details?targetId=CSI-Volume-Driver-v2_3:csi-csi-plugin-manage-dynamic-nfs-t.html">Files Storage Class</a> of Nutanix Karbon for more details on creating storage classes for dynamic NFS Share provisioning with Nutanix Files.
Once storage class is setup, you can use <code>VolumeOp</code> operation to create volume on Nutanix Files.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">vop = dsl.VolumeOp(
</span></span><span class="line"><span class="cl">   name=&#34;Create a volume to share data between stages on Nutanix Files&#34;,
</span></span><span class="line"><span class="cl">   resource_name=&#34;data-volume&#34;,
</span></span><span class="line"><span class="cl">   size=&#34;1Gi&#34;,
</span></span><span class="line"><span class="cl">   modes=dsl.VOLUME_MODE_RWM,
</span></span><span class="line"><span class="cl">   storage_class=&#34;files-sc&#34;)
</span></span></code></pre></div><h2 id="using-nutanix-objects-as-an-artifact-store">Using Nutanix Objects as an artifact store</h2>
<p>In order to use Nutanix Objects as an underlying artifact store, we need to edit the <code>workflow-controller-configmap</code> ConfigMap in the <code>kubeflow</code> namespace. See <a href="https://portal.nutanix.com/page/documents/details?targetId=Objects-v3_2:Objects-v3_2">Nutanix Objects Docs</a> for more details on creating object store and buckets.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n kubeflow edit configmap workflow-controller-configmap 
</span></span></code></pre></div><p>In the ConfigMap, we need to modify the s3 config with the Nutanix Objects config:</p>
<ul>
<li>endpoint: This is endpoint for Nutanix Objects store</li>
<li>bucket: This is the name of the Objects store bucket</li>
<li>accessKeySecret: reference to the access key ID in kubernetes secret for Objects store</li>
<li>secretKeySecret: reference to the secret access key in kubernetes secret for Objects store</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">    s3:
</span></span><span class="line"><span class="cl">      endpoint: &#34;x.x.x.x&#34;
</span></span><span class="line"><span class="cl">      bucket: &#34;ml-pipeline-storage&#34;
</span></span><span class="line"><span class="cl">      keyFormat: &#34;artifacts/{{workflow.name}}/{{pod.name}}&#34;
</span></span><span class="line"><span class="cl">      # insecure will disable TLS. Primarily used for minio installs not configured with TLS
</span></span><span class="line"><span class="cl">      insecure: true
</span></span><span class="line"><span class="cl">      accessKeySecret:
</span></span><span class="line"><span class="cl">        name: mlpipeline-ntnx-objects-artifact
</span></span><span class="line"><span class="cl">        key: object_store_access_key_id
</span></span><span class="line"><span class="cl">      secretKeySecret:
</span></span><span class="line"><span class="cl">        name: mlpipeline-ntnx-objects-artifact
</span></span><span class="line"><span class="cl">        key: object_store_secret_access_key
</span></span></code></pre></div><p>We also need to create the secret that is being referenced in the ConfigMap above</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">kind: Secret
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: mlpipeline-ntnx-objects-artifact
</span></span><span class="line"><span class="cl">stringData:
</span></span><span class="line"><span class="cl">  object_store_access_key_id: &lt;ACCESS_KEY_ID&gt;
</span></span><span class="line"><span class="cl">  object_store_secret_access_key: &lt;SECRET_ACCESS_KEY&gt;
</span></span><span class="line"><span class="cl">  region: us-east-1
</span></span></code></pre></div><p>After creating the secret we need to deploy the secret in the user namespace.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl -n kubeflow-user-example-com apply -f mlpipeline-ntnx-objects-artifact-secret.yaml 
</span></span></code></pre></div><p><strong>Note</strong>: installing this secret in kubeflow namespace does not work, it has be in present in user&rsquo;s namespace</p>
<p><img src="/docs/images/nutanix/objects_browser.png" alt="objects_browser"></p>
<p>To verify this is working correctly, you can check Nutanix Objects browser to see if your artifacts are created and show
up inside your buckets.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-dfe39f17cfbea245375f242bad9886ed">5.3 - Uninstall Kubeflow</h1>
    <div class="lead">How to uninstall Kubeflow from a Nutanix Karbon cluster</div>
	<h2 id="uninstall-kubeflow">Uninstall Kubeflow</h2>
<p>To delete a Kubeflow installation, apply the following command from your terraform script folder</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">terraform destroy -var-file=env.tfvars
</span></span></code></pre></div>
</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a58acf930788dfecacd2359cbd4fd9c2">6 - Arrikto Enterprise Kubeflow</h1>
    <div class="lead">Kubeflow distribution with additional automation, reproducibility, portability, and security features</div>
	<p>The <a href="https://www.arrikto.com/enterprise-kubeflow/" target="_blank">Arrikto Enterprise Kubeflow (EKF)</a> distribution extends the capabilities of the Kubeflow platform with additional automation, reproducibility, portability, and security features.</p>
<ul>
<li><em>Automation</em>: Orchestrate your end-to-end ML workflow with a click of a button. Start by tagging cells in Jupyter Notebooks to define pipeline steps, hyperparameter tuning, GPU usage, and metrics tracking. Click a button to create pipeline components and KFP DSL, resolve dependencies, inject data objects into each step, deploy the data science pipeline, and serve the best model. Or use the Kale SDK to do all the above with your preferred IDE.</li>
<li><em>Reproducibility</em>: Snapshot pipeline code, libraries, and data for every step with Arrikto’s Rok data management platform. Roll back to any machine learning pipeline step at it’s exact execution state for easy debugging. Collaborate with other data scientists through a Git-style publish/subscribe versioning workflow.</li>
<li><em>Portability</em>: Deploy and upgrade your Kubeflow environment with a proven GitOps process across all major public clouds, and on-prem infrastructure. Move ML workflows seamlessly across with Rok Registry.</li>
<li><em>Security</em>: Manage teams and user access via GitLab or any ID provider via Istio/OIDC. Isolate user ML data access within their own namespace while enabling notebook and pipeline collaboration in shared namespaces. Manage secrets and credentials securely, and efficiently.</li>
</ul>

</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9dd844c1b66939a21475d04273f4463e">7 - Arrikto Kubeflow as a Service</h1>
    <div class="lead">Kubeflow as a Service makes it easy for everyone to develop and serve models with the click of a button.</div>
	<p>To get started go to <a href="https://kubeflow.arrikto.com/">https://kubeflow.arrikto.com/</a></p>

</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3c2bf8504fc1ae4887ce2cd776b9fc62">8 - Charmed Kubeflow from Canonical</h1>
    <div class="lead">A production-ready, free-to-use, Kubeflow distribution from Canonical for easy consumption anywhere, from workstations to on-prem, public cloud and edge.</div>
	<p><a href="https://charmed-kubeflow.io/">Charmed Kubeflow</a> from <a href="https://www.canonical.com/">Canonical</a> delivers a powerful, sophisticated end-to-end MLOps platform which you can deploy using any conformant Kubernetes distribution. The solution enables optimised AI training and modelling, in a robust and automated way, allowing data scientists to focus on AI/ML projects, instead of underlying infrastructure. The enterprise-ready platform is available backed with 24/7 support, expert set-up services, and managed services with a service level agreement (SLA). With a growing ecosystem of powerful extensions and integrations including MLFlow and Seldon, the Charmed Kubeflow solution amplifies productivity levels for data scientists and machine learning engineers working with advanced analytics and AI.</p>
<p>Charmed Kubeflow is Canonical’s version of the upstream Kubeflow project. It uses charms, allowing components to easily connect and work effectively for different use cases. Main features include:</p>
<ul>
<li><em>Free to use</em>: Charmed Kubeflow is offered as free, open-source software. You don’t need a support agreement or license to deploy Charmed Kubeflow.</li>
<li><em>Integrated</em>: A growing ecosystem of extensions and integrations, such as <a href="https://mlflow.org/">MLFlow</a> to help scale up your data science initiatives.</li>
<li><em>Available</em>: Charmed Kubeflow is a fully portable solution for any cloud, including on-premise Kubernetes. Train your teams once to work anywhere.</li>
<li><em>Supported</em>: <a href="https://ubuntu.com/support">24/7 support</a>, <a href="https://ubuntu.com/ai/services">professional services</a> and <a href="https://ubuntu.com/managed/apps">managed services</a> with guaranteed SLA are available.</li>
<li><em>Community-driven:</em> Charmed Kubeflow is an <a href="htt://github.com/canonical/bundle-kubeflow">open-source product</a> driven by the community’s <a href="https://discourse.charmhub.io/tag/kubeflow">feedback</a>. You can always contribute or get our team’s support.</li>
</ul>
<hr>
<p><a href="https://charmed-kubeflow.io/docs/quickstart">Charmed Kubeflow deployment guide</a>
Instructions for Kubeflow deployment with Kubeflow Charmed Operators</p>
<p><a href="https://charmed-kubeflow.io/docs/upgrade">Charmed Kubeflow upgrade guide</a>
Instructions to update to the latest version of Charmed Kubeflow.</p>
<p><a href="https://charmed-kubeflow.io/tutorials">Charmed Kubeflow tutorials</a>
A collection of fun tutorials once you get started with Kubeflow</p>
<p><a href="https://charmed-kubeflow.io/docs">Charmed Kubeflow documentation</a>
Official documentation of Charmed Kubeflow</p>

</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-e2fc0dc1a2cade96e8c94789ed8dc553">9 - Kubeflow Operator</h1>
    <div class="lead">Deploying and managing Kubeflow with Kubeflow Operator</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-03edfaefabf5db9d82dbeaab04c99345">9.1 - Introduction</h1>
    <div class="lead">Kubeflow Operator introduction</div>
	<p>This guide describes the Kubeflow Operator and the current supported releases of Kubeflow Operator.</p>
<h2 id="kubeflow-operator">Kubeflow Operator</h2>
<p>Kubeflow Operator helps deploy, monitor and manage the lifecycle of Kubeflow. Built using the <a href="https://coreos.com/blog/introducing-operator-framework">Operator Framework</a> which offers an open source toolkit to build, test, package operators and manage the lifecycle of operators.</p>
<p>The operator is currently in incubation phase and is based on this <a href="https://docs.google.com/document/d/1vNBZOM-gDMpwTbhx0EDU6lDpyUjc7vhT3bdOWWCRjdk/edit#">design doc</a>. It is built on top of <em>KfDef</em> CR, and uses <em>kfctl</em> as the nucleus for Controller. Current roadmap for this Operator is listed <a href="https://github.com/kubeflow/kfctl/issues/193">here</a>. The Operator is also <a href="https://operatorhub.io/operator/kubeflow">published on OperatorHub</a>.</p>
<p>Applications and components to be deployed as part of Kubeflow platform are defined in the KfDef configuration manifest. Each application has a <a href="https://github.com/kubernetes-sigs/kustomize">kustomize</a> configuration with all its resource manifests. KfDef <code>spec</code> includes the <code>applications</code> field.  Application are specified in the <code>kustomizeConfig</code> field. <code>parameters</code> and <code>overlays</code> may be used to provide custom setting for the application. <code>repoRef</code> field specifies the path to retrieve the application&rsquo;s kustomize configuration.</p>
<p>KfDef <code>spec</code> may also include a <code>plugins</code> field for certain cloud platforms, including AWS and GCP. It is used by the platforms to preprocess certain tasks before Kubeflow deployment.</p>
<p>An example of KfDef is as follow:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kfdef.apps.kubeflow.org/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">KfDef</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">applications</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="c"># Install Istio</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">kustomizeConfig</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">repoRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">manifests</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">stacks/ibm/application/istio-stack</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">istio-stack</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="c"># Install Kubeflow applications.</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">kustomizeConfig</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">repoRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">manifests</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">stacks/ibm</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow-apps</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="c"># Other applications</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">kustomizeConfig</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">repoRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">manifests</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">stacks/ibm/application/spark-operator</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">spark-operator</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="c"># Model Serving applications</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">kustomizeConfig</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">repoRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">manifests</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">knative/installs/generic</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">knative</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">kustomizeConfig</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">repoRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">manifests</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">kfserving/installs/generic</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kfserving</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">repos</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">manifests</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">uri</span><span class="p">:</span><span class="w"> </span><span class="l">https://github.com/kubeflow/manifests/archive/master.tar.gz</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">master</span><span class="w">
</span></span></span></code></pre></div><p>More KfDef examples may be found in Kubeflow <a href="https://github.com/kubeflow/manifests/tree/master/kfdef">manifests</a> repo. Users can pick one there and make some modification to fit their requirements. <a href="https://github.com/opendatahub-io">OpenDataHub</a> project also maintains a KfDef <a href="https://github.com/opendatahub-io/manifests/blob/v1.0-branch-openshift/kfdef/kfctl_openshift.yaml">manifest</a> for Kubeflow deployment on OpenShift Container Platforms.</p>
<p>The operator watches on all KfDef configuration instances in the cluster as custom resources (CR) and manage them. It handles reconcile requests to all the <em>KfDef</em> instances. To understand more on the operator controller behavior, refer to this <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/master/pkg/doc.go">controller-runtime link</a>.</p>
<p>Kubeflow Operator shares the same packages and functions as the <code>kfctl</code> CLI, which is the command line approach to deploy Kubeflow. Therefore, the deployment flow is similar except that the <code>ownerReferences</code> metadata is added for each application&rsquo;s Kubernetes object. The KfDef CR is the parent of all these objects. Kubeflow Operator does better in tearing down the Kubeflow deployment than the CLI approach. When the KfDef CR is deleted, Kubernetes garbage collection mechanism then takes over the responsibility to remove all and only the resources deployed through this KfDef configuration.</p>
<p>One of the many good reasons to use an operator is to monitor the resources. The Kubeflow Operator also watches all child resources of the KfDef CR. Should any of these resources be deleted, the operator would try to apply the resource manifest and bring the object up again.</p>
<p>The operator responds to following events:</p>
<ul>
<li>
<p>When a <em>KfDef</em> instance is created or updated, the operator&rsquo;s <em>reconciler</em> will be notified of the event and invoke the <code>Apply</code> functions provided by the <a href="https://github.com/kubeflow/kfctl/tree/master/pkg"><code>kfctl</code> package</a> to deploy Kubeflow. The Kubeflow resources specified with the manifests will be owned by the <em>KfDef</em> instance with their <code>ownerReferences</code> set.</p>
</li>
<li>
<p>When a <em>KfDef</em> instance is deleted, since the owner is deleted, all the secondary resources owned by it will be deleted through the <a href="https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/">garbage collection</a>. In the mean time, the <em>reconciler</em> will be notified of the event and remove the finalizers.</p>
</li>
<li>
<p>When any resource deployed as part of a <em>KfDef</em> instance is deleted, the operator&rsquo;s <em>reconciler</em> will be notified of the event and invoke the <code>Apply</code> functions provided by the <a href="https://github.com/kubeflow/kfctl/tree/master/pkg"><code>kfctl</code> package</a> to re-deploy the Kubeflow. The deleted resource will be recreated with the same manifest as specified when the <em>KfDef</em> instance is created.</p>
</li>
</ul>
<p>Deploying Kubeflow with the Kubeflow Operator includes two steps: <a href="/docs/methods/operator/install-operator">installing the Kubeflow Operator</a> followed by <a href="/docs/methods/operator/deploy/operator">deploying</a> the KfDef custom resource.</p>
<h2 id="current-tested-operators-and-pre-built-images">Current Tested Operators and Pre-built Images</h2>
<p>Kubeflow Operator controller logic is based on the <a href="https://github.com/kubeflow/kfctl/tree/master/pkg"><code>kfctl</code> package</a>, so for each major release of <code>kfctl</code>, an operator image is built and tested with that version of <a href="github.com/kubeflow/manifests"><code>manifests</code></a> to deploy a <em>KfDef</em> instance. Following table shows what releases have been tested.</p>
<table>
<thead>
<tr>
<th>branch tag</th>
<th>operator image</th>
<th>manifests version</th>
<th>kfdef example</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/kubeflow/kfctl/tree/v1.0">v1.0</a></td>
<td><a href="https://hub.docker.com/layers/aipipeline/kubeflow-operator/v1.0.0/images/sha256-63d00b29a61ff5bc9b0527c8a515cd4cb55de474c45d8e0f65742908ede4d88f?context=repo">aipipeline/kubeflow-operator:v1.0.0</a></td>
<td><a href="https://github.com/kubeflow/manifests/tree/f56bb47d7dc5378497ad1e38ea99f7b5ebe7a950">1.0.0</a></td>
<td><a href="https://github.com/kubeflow/manifests/blob/f56bb47d7dc5378497ad1e38ea99f7b5ebe7a950/kfdef/kfctl_k8s_istio.v1.0.0.yaml">kfctl_k8s_istio.v1.0.0.yaml</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://github.com/kubeflow/kfctl/tree/v1.0.1">v1.0.1</a></td>
<td><a href="https://hub.docker.com/layers/aipipeline/kubeflow-operator/v1.0.1/images/sha256-828024b773040271e4b547ce9219046f705fb7123e05503d5a2d1428dfbcfb6e?context=repo">aipipeline/kubeflow-operator:v1.0.1</a></td>
<td><a href="https://github.com/kubeflow/manifests/tree/v1.0.1">1.0.1</a></td>
<td><a href="https://github.com/kubeflow/manifests/blob/v1.0.1/kfdef/kfctl_k8s_istio.v1.0.1.yaml">kfctl_k8s_istio.v1.0.1.yaml</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://github.com/kubeflow/kfctl/tree/v1.0.2">v1.0.2</a></td>
<td><a href="https://hub.docker.com/layers/aipipeline/kubeflow-operator/v1.0.2/images/sha256-18d2ca6f19c1204d5654dfc4cc08032c168e89a95dee68572b9e2aaedada4bda?context=repo">aipipeline/kubeflow-operator:v1.0.2</a></td>
<td><a href="https://github.com/kubeflow/manifests/tree/v1.0.2">1.0.2</a></td>
<td><a href="https://github.com/kubeflow/manifests/blob/v1.0.2/kfdef/kfctl_k8s_istio.v1.0.2.yaml">kfctl_k8s_istio.v1.0.2.yaml</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://github.com/kubeflow/kfctl/tree/v1.1.0">v1.1.0</a></td>
<td><a href="https://hub.docker.com/layers/aipipeline/kubeflow-operator/v1.0.0/images/sha256-63d00b29a61ff5bc9b0527c8a515cd4cb55de474c45d8e0f65742908ede4d88f?context=explore">aipipeline/kubeflow-operator:v1.1.0</a></td>
<td><a href="https://github.com/kubeflow/manifests/tree/v1.1.0">1.1.0</a></td>
<td><a href="https://github.com/kubeflow/manifests/blob/v1.1-branch/kfdef/kfctl_ibm.v1.1.0.yaml">kfctl_ibm.v1.1.0.yaml</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://github.com/kubeflow/kfctl">master</a></td>
<td><a href="https://hub.docker.com/layers/aipipeline/kubeflow-operator/master/images/sha256-e81020c426a12237c7cf84316dbbd0efda76e732233ddd57ef33543381dfb8a1?context=repo">aipipeline/kubeflow-operator:master</a></td>
<td><a href="https://github.com/kubeflow/manifests">master</a></td>
<td><a href="https://github.com/kubeflow/manifests/blob/master/kfdef/kfctl_ibm.yaml">kfctl_ibm.yaml</a></td>
<td>as of 07/29/2020</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Note: if building a customized operator for a specific version of Kubeflow is desired, you can run <code>git checkout</code> to that specific branch tag. Keep in mind to use the matching version of manifests.</p>
</blockquote>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bbe2c4524a0d40b13e62b3d1b362c63c">9.2 - Installing Kubeflow Operator</h1>
    <div class="lead">Instructions for installing the Kubeflow Operator</div>
	<p>This guide describes how to install the Kubeflow Operator.</p>
<p>There are different ways to install the Kubeflow Operator, choose one of the following:</p>
<h2 id="1-installing-the-kubeflow-operator-through-the-operatorhubiohttpsoperatorhubiooperatorkubeflow">1. Installing the Kubeflow Operator through the <a href="https://operatorhub.io/operator/kubeflow">operatorhub.io</a></h2>
<p>The stable release of Kubeflow Operator is published to the <a href="https://operatorhub.io">operatorhub.io</a>. Navigate to the <a href="https://operatorhub.io/operator/kubeflow">operatorhub.io</a>, click on the <code>Install</code> and follow the instructions there to install the operator.</p>
<p><img src="/docs/images/operator-operatorhubio-kubeflow.png" 
alt="Kubeflow Operator in OperatorHub"
class="mt-3 mb-3 border border-info rounded"></p>
<p>Verify the Kubeflow Operator is running with following command.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get pod -n operators
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">NAME                                 READY   STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">kubeflow-operator-55876578df-25mq5   1/1     Running   <span class="m">0</span>          17h
</span></span></code></pre></div><h2 id="2-installing-the-kubeflow-operator-with-kustomize-and-kubectl">2. Installing the Kubeflow Operator with <code>kustomize</code> and <code>kubectl</code></h2>
<p>Previous method is convenient and simple enough without any knowledges of the Operator SDK. However, if any of the following reasons applies, choose this approach to install the operator.</p>
<ol>
<li>You want to install a different release of Kubeflow Operator since the Kubeflow KfDef manifests may not be compatible from release to release.</li>
<li>You want to install the latest release of Kubeflow Operator.</li>
</ol>
<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li>Install <a href="https://github.com/kubernetes-sigs/kustomize/blob/master/docs/INSTALL.md">kustomize</a></li>
</ul>
<h3 id="clone-the-kfctlhttpsgithubcomkubeflowkfctlgit-repo-and-switch-to-the-desired-release-branch">Clone the <a href="https://github.com/kubeflow/kfctl.git"><code>kfctl</code></a> repo and switch to the desired release branch</h3>
<p>Clone the repo and switch to the desired release branch with the following <code>git</code> commands</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">git clone https://github.com/kubeflow/kfctl.git
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kfctl
</span></span><span class="line"><span class="cl">git checkout v1.1-branch
</span></span></code></pre></div><h3 id="create-operators-namespace-and-update-the-operator-manifests">Create <code>operators</code> namespace and update the operator manifests</h3>
<p>The <code>operators</code> namespace is the namespace where the Kubeflow Operator will be installed to. Create the namespace and update the manifests with these commands</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">OPERATOR_NAMESPACE</span><span class="o">=</span>operators
</span></span><span class="line"><span class="cl">kubectl create ns <span class="si">${</span><span class="nv">OPERATOR_NAMESPACE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> deploy
</span></span><span class="line"><span class="cl">kustomize edit <span class="nb">set</span> namespace <span class="si">${</span><span class="nv">OPERATOR_NAMESPACE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># only deploy this if the k8s cluster is 1.15+ and has resource quota support, which will allow only one _kfdef_ instance or one deployment of Kubeflow on the cluster. This follows the singleton model, and is the current recommended and supported mode.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># kustomize edit add resource kustomize/include/quota</span>
</span></span></code></pre></div><h3 id="install-the-operator">Install the operator</h3>
<p>Install the Kubeflow Operator with the <code>kustomize</code> and <code>kubectl</code> commands</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kustomize build <span class="p">|</span> kubectl apply -f -
</span></span></code></pre></div><p>Verify the Kubeflow Operator is running with following command.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get pod -n operators
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">NAME                                 READY   STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">kubeflow-operator-55876578df-25mq5   1/1     Running   <span class="m">0</span>          17h
</span></span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-56551ff9a7eea6eaffb455e46cbe6039">9.3 - Installing Kubeflow</h1>
    <div class="lead">Instructions for Kubeflow deployment with Kubeflow Operator</div>
	<p>This guide describes how to use the Kubeflow Operator to deploy Kubeflow. As mentioned in the Operator <a href="/docs/methods/operator/introduction.md">introduction</a>, the Operator also allows you to monitor and manage the Kubeflow installation beyond the initial installation.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Kubeflow Operator needs to be deployed on your cluster for rest of steps to work. Please follow the <a href="/docs/methods/operator/install-operator"><code>Install the Kubeflow Operator</code></a> guide to install the Kubeflow Operator</li>
</ul>
<h2 id="deployment-instructions">Deployment Instructions</h2>
<p>The Kubeflow Operator uses the KfDef as its custom resource. You can compose a KfDef configuration or pick a default KfDef from the Kubeflow <a href="https://github.com/kubeflow/manifests/tree/master/kfdef">manifests</a> repo. Keep in mind choosing the release that will work with the Kubeflow Operator.</p>
<h3 id="prepare-kfdef-configuration">Prepare KfDef configuration</h3>
<p>The <code>metadata.name</code> field must be set for the KfDef manifests whether it is downloaded from the Kubeflow manifests repo or is originally written. Following example shows how to prepare the KfDef manifests</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="c1"># download a default KfDef configuration from remote repo</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">KFDEF_URL</span><span class="o">=</span>https://raw.githubusercontent.com/kubeflow/manifests/v1.1-branch/kfdef/kfctl_ibm.yaml
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">KFDEF</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">KFDEF_URL</span><span class="si">}</span><span class="s2">&#34;</span> <span class="p">|</span> rev <span class="p">|</span> cut -d/ -f1 <span class="p">|</span> rev<span class="k">)</span>
</span></span><span class="line"><span class="cl">curl -L <span class="si">${</span><span class="nv">KFDEF_URL</span><span class="si">}</span> &gt; <span class="si">${</span><span class="nv">KFDEF</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># add metadata.name field</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Note: yq can be installed from https://github.com/mikefarah/yq</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">KUBEFLOW_DEPLOYMENT_NAME</span><span class="o">=</span>kubeflow
</span></span><span class="line"><span class="cl">yq w <span class="si">${</span><span class="nv">KFDEF</span><span class="si">}</span> <span class="s1">&#39;metadata.name&#39;</span> <span class="si">${</span><span class="nv">KUBEFLOW_DEPLOYMENT_NAME</span><span class="si">}</span> &gt; <span class="si">${</span><span class="nv">KFDEF</span><span class="si">}</span>.tmp <span class="o">&amp;&amp;</span> mv <span class="si">${</span><span class="nv">KFDEF</span><span class="si">}</span>.tmp <span class="si">${</span><span class="nv">KFDEF</span><span class="si">}</span>
</span></span></code></pre></div><h3 id="deploy-the-kubeflow-with-the-kubeflow-operator">Deploy the Kubeflow with the Kubeflow Operator</h3>
<p>Kubeflow Operator is watching on any KfDef resource in the Kubernetes cluster. Depends on how the operator is installed, there are a couple of ways to start the Kubeflow deployment. You can always manually run with following commands</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="c1"># create the namespace for Kubeflow deployment</span>
</span></span><span class="line"><span class="cl"><span class="nv">KUBEFLOW_NAMESPACE</span><span class="o">=</span>kubeflow
</span></span><span class="line"><span class="cl">kubectl create ns <span class="si">${</span><span class="nv">KUBEFLOW_NAMESPACE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># create the KfDef custom resource</span>
</span></span><span class="line"><span class="cl">kubectl create -f <span class="si">${</span><span class="nv">KFDEF</span><span class="si">}</span> -n <span class="si">${</span><span class="nv">KUBEFLOW_NAMESPACE</span><span class="si">}</span>
</span></span></code></pre></div><p>Note: in the example above, ${KFDEF} points to a local KfDef configuration file, however, it can also points to a remote URL containing a valid KfDef configuration.</p>
<h3 id="watch-the-deployment-progress">Watch the deployment progress</h3>
<p>The Kubeflow deployment is carried on by the operator, you can watch the progress with this command</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl logs deployment/kubeflow-operator -n <span class="si">${</span><span class="nv">OPERATOR_NAMESPACE</span><span class="si">}</span> -f
</span></span></code></pre></div><p>Verify the Kubeflow deployment by monitoring the pods in the ${KUBEFLOW_NAMESPACE}</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl get pod -n <span class="si">${</span><span class="nv">KUBEFLOW_NAMESPACE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">NAME                                                     READY   STATUS    RESTARTS   AGE
</span></span><span class="line"><span class="cl">admission-webhook-bootstrap-stateful-set-0               1/1     Running   <span class="m">3</span>          2m26s
</span></span><span class="line"><span class="cl">admission-webhook-deployment-5bc5f97cfd-chjnm            1/1     Running   <span class="m">2</span>          46s
</span></span><span class="line"><span class="cl">application-controller-stateful-set-0                    1/1     Running   <span class="m">0</span>          2m30s
</span></span><span class="line"><span class="cl">argo-ui-669bcd8bfc-5dk5c                                 1/1     Running   <span class="m">0</span>          45s
</span></span><span class="line"><span class="cl">cache-deployer-deployment-b75f5c5f6-42n6l                2/2     Running   <span class="m">1</span>          45s
</span></span><span class="line"><span class="cl">cache-server-85bccd99bd-tfrnr                            2/2     Running   <span class="m">0</span>          44s
</span></span><span class="line"><span class="cl">centraldashboard-8849f64cf-l45zc                         1/1     Running   <span class="m">0</span>          44s
</span></span><span class="line"><span class="cl">jupyter-web-app-deployment-6c568f4cbc-pd68m              1/1     Running   <span class="m">0</span>          43s
</span></span><span class="line"><span class="cl">kubeflow-pipelines-profile-controller-846cc56f44-cmbbf   1/1     Running   <span class="m">0</span>          43s
</span></span><span class="line"><span class="cl">metacontroller-0                                         1/1     Running   <span class="m">0</span>          96s
</span></span><span class="line"><span class="cl">metadata-writer-59d755696c-fh6px                         2/2     Running   <span class="m">0</span>          43s
</span></span><span class="line"><span class="cl">minio-d45d44d4f-rmxft                                    1/1     Running   <span class="m">0</span>          42s
</span></span><span class="line"><span class="cl">ml-pipeline-6bc56cd86d-kn7zt                             1/2     Running   <span class="m">0</span>          42s
</span></span><span class="line"><span class="cl">ml-pipeline-persistenceagent-6f99b56974-x2f52            2/2     Running   <span class="m">0</span>          41s
</span></span><span class="line"><span class="cl">ml-pipeline-scheduledworkflow-d596b8bd-qdz6m             2/2     Running   <span class="m">0</span>          41s
</span></span><span class="line"><span class="cl">ml-pipeline-ui-8695cc6b46-hr8p5                          2/2     Running   <span class="m">0</span>          40s
</span></span><span class="line"><span class="cl">ml-pipeline-viewer-crd-5998ff7f56-5rn4s                  2/2     Running   <span class="m">2</span>          40s
</span></span><span class="line"><span class="cl">ml-pipeline-visualizationserver-cbbb5b5b-w7rbd           2/2     Running   <span class="m">0</span>          39s
</span></span><span class="line"><span class="cl">mysql-76597cf5b5-jpsrx                                   1/2     Running   <span class="m">0</span>          39s
</span></span><span class="line"><span class="cl">notebook-controller-deployment-756587d86-fffg8           1/1     Running   <span class="m">0</span>          38s
</span></span><span class="line"><span class="cl">profiles-deployment-865b78d47f-pbgl4                     2/2     Running   <span class="m">0</span>          38s
</span></span><span class="line"><span class="cl">workflow-controller-54dccb7dc4-hkg9s                     1/1     Running   <span class="m">0</span>          37s
</span></span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c27380d2054257759993453b91fecf50">9.4 - Uninstalling Kubeflow</h1>
    <div class="lead">Instructions for uninstalling Kubeflow with Kubeflow Operator</div>
	<p>This guide describes how to delete the Kubeflow deployment when it is deployed with the Kubeflow Operator.</p>
<p>To delete the Kubeflow deployment, simply delete the KfDef custom resource from the cluster.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl delete kfdef <span class="si">${</span><span class="nv">KUBEFLOW_DEPLOYMENT_NAME</span><span class="si">}</span> -n <span class="si">${</span><span class="nv">KUBEFLOW_NAMESPACE</span><span class="si">}</span>
</span></span></code></pre></div><p>Note: ${KUBEFLOW_DEPLOYMENT_NAME} and ${KUBEFLOW_NAMESPACE} are defined in the <a href="/docs/methods/operator/install-kubeflow">Installing Kubeflow</a> guide.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d080e1444da199bf0366d4c6b830fc29">9.5 - Uninstalling Kubeflow Operator</h1>
    <div class="lead">Instructions for uninstalling Kubeflow Operator</div>
	<p>This guide describes how to uninstall the Kubeflow Operator.</p>
<p>You can always uninstall the operator with following commands</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="c1"># switch to the cloned kfctl directory</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> kfctl
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># uninstall the operator</span>
</span></span><span class="line"><span class="cl">kubectl delete -f deploy/operator.yaml -n <span class="si">${</span><span class="nv">OPERATOR_NAMESPACE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">kubectl delete clusterrolebinding kubeflow-operator
</span></span><span class="line"><span class="cl">kubectl delete -f deploy/service_account.yaml -n <span class="si">${</span><span class="nv">OPERATOR_NAMESPACE</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">kubectl delete -f deploy/crds/kfdef.apps.kubeflow.org_kfdefs_crd.yaml
</span></span><span class="line"><span class="cl">kubectl delete ns <span class="si">${</span><span class="nv">OPERATOR_NAMESPACE</span><span class="si">}</span>
</span></span></code></pre></div>
</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-2d2c088c1d636b7a383c0f880da89c73">9.6 - Troubleshooting</h1>
    <div class="lead">Troubleshooting with Kubeflow deployment</div>
	<p>This guide provides some tips on troubleshooting known Kubeflow deployment problems.</p>
<h3 id="remove-leftover-cluster-wide-resources-after-the-kubeflow-is-uninstalled">Remove leftover cluster-wide resources after the Kubeflow is uninstalled</h3>
<p>After Kubeflow deployment is uninstalled, some <em>mutatingwebhookconfigurations</em> and <em>validatingwebhookconfigurations</em> resources are cluster-wide resources and may not be removed as their owner is not the <em>KfDef</em> instance. To remove them, run following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl delete mutatingwebhookconfigurations --all
</span></span><span class="line"><span class="cl">kubectl delete validatingwebhookconfigurations --all
</span></span></code></pre></div>
</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9a55a407eb867eaec1ec07fe3a2078f4">10 - Kubeflow on OpenShift</h1>
    <div class="lead">Running Kubeflow 1.6 on OpenShift 4.9</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-74cfacc63bdd2d6abb4a692b4893c7c3">10.1 - Install Kubeflow on OpenShift</h1>
    <div class="lead">Instructions for deploying Kubeflow on an OpenShift cluster from the command line</div>
	<p>This guide describes how to use the <code>kfctl</code> CLI to deploy Kubeflow 1.6 on an existing OpenShift 4.9 cluster.</p>
<h2 id="prerequisites">Prerequisites</h2>
<h3 id="openshift-4-cluster">OpenShift 4 cluster</h3>
<ul>
<li>You need to have access to an OpenShift 4.9 cluster as <code>cluster-admin</code> to be able to deploy Kubeflow.</li>
<li>You can use <a href="https://code-ready.github.io/crc/">Code Ready Containers</a> (CRC) to run a local cluster, use <a href="https://try.openshift.com">try.openshift.com</a> to create a new cluster or use an existing cluster.</li>
<li>Install <a href="https://docs.openshift.com/container-platform/4.2/cli_reference/openshift_cli/getting-started-cli.html"><code>oc</code> command-line tool</a> to communicate with the cluster.</li>
</ul>
<h4 id="code-ready-containers">Code Ready Containers</h4>
<p>If you are using Code Ready Containers, you need to make sure you have enough resources configured for the VM:</p>
<p>Recommended:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">16 GB memory
</span></span><span class="line"><span class="cl">6 CPU
</span></span><span class="line"><span class="cl">45 GB disk space
</span></span></code></pre></div><p>Minimal:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">10 GB memory
</span></span><span class="line"><span class="cl">6 CPU
</span></span><span class="line"><span class="cl">30 GB disk space (default for CRC)
</span></span></code></pre></div><h2 id="installing-kubeflow">Installing Kubeflow</h2>
<p>Use the following steps to install Kubeflow 1.6 on OpenShift 4.9.</p>
<ol>
<li>Clone the <code>manifests</code> repo in the  <a href="https://github.com/opendatahub-io/manifests/tree/v1.6-branch-openshift">opendatahub-io</a> org.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">git clone https://github.com/opendatahub-io/manifests.git --branch v1.6-branch-openshift
</span></span></code></pre></div><ol start="2">
<li>Create <code>kubeflow</code> namespace</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">oc create ns kubeflow
</span></span></code></pre></div><ol start="4">
<li>Deploy all Kubeflow components under <code>openshift</code> stack using the following command:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">while ! kustomize build openshift/example/istio | kubectl apply -f -; do echo &#34;Retrying to apply resources&#34;; sleep 10; done
</span></span></code></pre></div><ol start="5">
<li>
<p>Wait until all the pods are running.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ oc get pods -n kubeflow
</span></span><span class="line"><span class="cl">NAME                                               READY   STATUS              RESTARTS   AGE
</span></span><span class="line"><span class="cl">admission-webhook-deployment-6748884cff-wb7kp      1/1     Running             0          42h
</span></span><span class="line"><span class="cl">cache-deployer-deployment-799f449d59-5zl2l         1/1     Running             0          42h
</span></span><span class="line"><span class="cl">cache-server-67849767c5-7w44j                      1/1     Running             0          42h
</span></span><span class="line"><span class="cl">centraldashboard-78f95899fc-8rt8k                  1/1     Running             0          42h
</span></span><span class="line"><span class="cl">metadata-envoy-deployment-67fd74f564-tsrxm         1/1     Running             0          42h
</span></span><span class="line"><span class="cl">metadata-grpc-deployment-9d547547d-g9cq7           1/1     Running             0          42h
</span></span><span class="line"><span class="cl">metadata-writer-7776fc6f6f-4f4hp                   1/1     Running             0          42h
</span></span><span class="line"><span class="cl">minio-5cb67d5f6d-l9665                             1/1     Running             0          42h
</span></span><span class="line"><span class="cl">ml-pipeline-6d4fbc667b-hhqsw                       1/1     Running             0          42h
</span></span><span class="line"><span class="cl">ml-pipeline-persistenceagent-667c448c65-r9sn5      1/1     Running             0          42h
</span></span><span class="line"><span class="cl">ml-pipeline-scheduledworkflow-5b9769fc8b-s9nt8     1/1     Running             0          42h
</span></span><span class="line"><span class="cl">ml-pipeline-ui-6f9f496b7-9rr4s                     1/1     Running             0          42h
</span></span><span class="line"><span class="cl">ml-pipeline-viewer-crd-77ccffd6d4-n4x55            1/1     Running             0          42h
</span></span><span class="line"><span class="cl">ml-pipeline-visualizationserver-6c7b448b99-5ttn4   1/1     Running             0          42h
</span></span><span class="line"><span class="cl">mysql-7659b8f58c-npr57                             1/1     Running             0          42h
</span></span><span class="line"><span class="cl">profiles-deployment-7c8446984b-nvvh7               2/2     Running             0          42h
</span></span><span class="line"><span class="cl">workflow-controller-7899f6947-gz7km                1/1     Running             0          42h    
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></div></li>
<li>
<p>The command below looks up the URL of the Kubeflow user interface assigned by the OpenShift cluster. You can open the printed URL in your browser to access the Kubeflow user interface.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">oc get routes -n istio-system istio-ingressgateway -o jsonpath=&#39;http://{.spec.host}/&#39;
</span></span></code></pre></div></li>
</ol>

</div>



    
	
  

    
	
  



          </main>
        </div>
      </div>
      
<footer class="bg-dark pt-3 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Kubeflow mailing list" aria-label="Kubeflow mailing list">
    <a class="text-white" target="_blank" rel="noopener" href="https://www.kubeflow.org/docs/about/community/#kubeflow-mailing-list" aria-label="Kubeflow mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" rel="noopener" href="https://twitter.com/kubeflow" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" rel="noopener" href="https://stackoverflow.com/questions/tagged/kubeflow" aria-label="Stack Overflow">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener" href="https://github.com/GoogleCloudPlatform/kubeflow-distribution" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" rel="noopener" href="https://www.kubeflow.org/docs/about/community/#kubeflow-slack" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-5 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2022 Google | Documentation Distributed under CC BY 4.0</small>
        <p><small class="ml-1"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></small></p>
	
		<p class="mt-2"><a href="/kubeflow-docs/docs/about/">About</a></p>
	
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="/kubeflow-docs/js/main.min.301c0110334bec1b4445867d27dc7dd69b2df859154ccf252005508bc860cc5a.js" integrity="sha256-MBwBEDNL7BtERYZ9J9x91pst&#43;FkVTM8lIAVQi8hgzFo=" crossorigin="anonymous"></script>
<script src='/kubeflow-docs/js/tabpane-persist.js'></script>

  </body>
</html>
