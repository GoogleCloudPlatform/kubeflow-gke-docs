<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.105.0">
<link rel="canonical" type="text/html" href="https://gkcalat.github.io/kubeflow-docs/docs/external-add-ons/">
<link rel="alternate" type="application/rss&#43;xml" href="https://gkcalat.github.io/kubeflow-docs/docs/external-add-ons/index.xml">
<meta name="robots" content="noindex, nofollow">

<script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "WebSite",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gkcalat.github.io\/kubeflow-docs"
      },
      "articleSection" : "docs",
      "name" : "External Add-Ons",
      "headline" : "External Add-Ons",
      "description" : "Additional tools that may be integrated with a Kubeflow deployment or distribution.",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "0001",
      "datePublished": "0001-01-01 00:00:00 \u002b0000 UTC",
      "dateModified" : "0001-01-01 00:00:00 \u002b0000 UTC",
      "url" : "https:\/\/gkcalat.github.io\/kubeflow-docs\/docs\/external-add-ons\/",
      "wordCount" : "0",
      "keywords" : [ "Kubeflow" ]
  }
  </script>

<link rel="shortcut icon" href="/kubeflow-docs/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/kubeflow-docs/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/kubeflow-docs/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/kubeflow-docs/favicons/favicon-32x32.png" sizes="32x32">
<link rel="manifest" href="/kubeflow-docs/favicons/manifest.json">
<meta name="msapplication-config" content="/kubeflow-docs/favicons/browserconfig.xml" />
<meta name="msapplication-TileColor" content="#4279f4">
<meta name="theme-color" content="#4279f4">

<title>External Add-Ons | Kubeflow on Google Cloud Platform</title>
<meta name="description" content="Additional tools that may be integrated with a Kubeflow deployment or distribution.">
<meta property="og:title" content="External Add-Ons" />
<meta property="og:description" content="Additional tools that may be integrated with a Kubeflow deployment or distribution." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://gkcalat.github.io/kubeflow-docs/docs/external-add-ons/" /><meta property="og:site_name" content="Kubeflow on Google Cloud Platform" />

<meta itemprop="name" content="External Add-Ons">
<meta itemprop="description" content="Additional tools that may be integrated with a Kubeflow deployment or distribution."><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="External Add-Ons"/>
<meta name="twitter:description" content="Additional tools that may be integrated with a Kubeflow deployment or distribution."/>




<link rel="preload" href="/kubeflow-docs/scss/main.min.5006b434b14f73c3d5537a7ba4c8ecef723ddc93f3a3c8530e19d24f75486ea9.css" as="style">
<link href="/kubeflow-docs/scss/main.min.5006b434b14f73c3d5537a7ba4c8ecef723ddc93f3a3c8530e19d24f75486ea9.css" rel="stylesheet" integrity="">

<script
  src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
  crossorigin="anonymous"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-GK9XL47N6S"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-GK9XL47N6S', { 'anonymize_ip': false });
}
</script>

  </head>
  <body class="td-section">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand-md navbar-dark  td-navbar">
        <a class="navbar-brand" href="/kubeflow-docs/">
		<span class="navbar-logo"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 276.93 274.55"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M95.9 62.15l4.1 102.1 73.75-94.12a6.79 6.79.0 019.6-1.11l46 36.92-15-65.61z" fill="#4279f4"/><path fill="#0028aa" d="M102.55 182.98h65.42l-40.17-32.23-25.25 32.23z"/><path fill="#014bd1" d="M180.18 83.92l-44 56.14 46.88 37.61 44.47-55.76-47.35-37.99z"/><path fill="#bedcff" d="M83.56 52.3l.01-.01 38.69-48.52-62.39 30.05-15.41 67.51 39.1-49.03z"/><path fill="#6ca1ff" d="M45.32 122.05l41.44 51.96-3.95-98.98-37.49 47.02z"/><path fill="#a1c3ff" d="M202.31 28.73 142.65.0l-37.13 46.56 96.79-17.83z"/><path d="M1.6 272v-44.78h5.74v23.41l20.48-23.41h6.4l-17.39 19.7 19 25.07H29.1l-15.92-20.8-5.84 6.65V272zm40.02-9.79V240h5.43v22.39a4.67 4.67.0 002.35 4.19 11 11 0 0011 0 4.69 4.69.0 002.33-4.19V240h5.43v22.19a9.08 9.08.0 01-4.1 7.87 16.2 16.2.0 01-18.37.0 9.07 9.07.0 01-4.07-7.85zM77.46 272v-48h5.43v16.81a29.29 29.29.0 019.32-1.73 13.1 13.1.0 016.2 1.41 10.71 10.71.0 014.18 3.74 18.07 18.07.0 012.23 5.06 21.26 21.26.0 01.73 5.58q0 8.43-4.38 12.79T87.35 272zm5.43-4.87h4.55q6.77.0 9.72-2.95t3-9.51a14.21 14.21.0 00-2-7.52 6.55 6.55.0 00-6-3.22 24.73 24.73.0 00-9.25 1.54zm29.47-11.19q0-7.71 4.09-12.3a13.75 13.75.0 0110.8-4.59q13.35.0 13.36 18.86h-22.82a12.3 12.3.0 002.9 7.07q2.59 3.11 7.9 3.1a24.92 24.92.0 0010.55-2v5a27.74 27.74.0 01-9.86 1.87 19.83 19.83.0 01-7.7-1.37 13.31 13.31.0 01-5.28-3.76 16.21 16.21.0 01-3-5.38 20.84 20.84.0 01-.94-6.5zm5.62-2.12h17.26a14.91 14.91.0 00-2.37-7.12 6.44 6.44.0 00-5.62-2.78 8.2 8.2.0 00-6.21 2.72 12.07 12.07.0 00-3.04 7.18z" fill="#4279f4" stroke="#4279f4" stroke-miterlimit="10" stroke-width="3.2"/><path d="M147.32 244.89V240h5v-7.59a8.14 8.14.0 012.31-6.05 7.79 7.79.0 015.69-2.28h7.86V229h-5c-2.21.0-3.67.45-4.37 1.34s-1.06 2.55-1.06 5V240h8.46v4.87h-8.46V272h-5.44v-27.1zM175.26 272v-48h5.43v48zm19.15-3.95a17.86 17.86.0 1112.33 4.9 16.57 16.57.0 01-12.33-4.9zm3.84-20.65a13.16 13.16.0 000 17.2 12.07 12.07.0 0017 0 13.09 13.09.0 000-17.2 12.07 12.07.0 00-17 0zm30.2-7.4h5.75l7.3 25.32 7.43-25.32h5.36l7.34 25.34L269 240h5.74l-10.04 32h-6.12l-6.83-24.58L245 272h-6.47z" fill="#0028aa" stroke="#0028aa" stroke-miterlimit="10" stroke-width="3.2"/></g></g></svg></span><span class="text-uppercase font-weight-bold">Kubeflow on Google Cloud Platform</span>
	</a>
	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main_navbar" aria-controls="main_navbar" aria-expanded="false" aria-label="Toggle navigation">
		<span class="navbar-toggler-icon"></span>
	</button>
	<div class="collapse navbar-collapse ml-md-auto" id="main_navbar">
		<ul class="navbar-nav ml-auto pt-4 pt-md-0 my-2 my-md-1">
			
			
			<li class="nav-item mr-2 mr-lg-4 mt-1 mt-lg-0">
				
				
				
				
				
				
				<a class="nav-link" href="/kubeflow-docs/docs/" ><span>Docs</span></a>
			</li>
			
			<li class="nav-item mr-2 mr-lg-4 mt-1 mt-lg-0">
				
				
				
				
				
				
				<a class="nav-link" href="/kubeflow-docs/news/" ><span>News</span></a>
			</li>
			
			<li class="nav-item mr-2 mr-lg-4 mt-1 mt-lg-0">
				
				
				
				
				
				
				<a class="nav-link" href="https://github.com/GoogleCloudPlatform/kubeflow-distribution" target="_blank" ><i class='fa-brands fa-github pr-2'></i><span>Source</span></a>
			</li>
			
			
			<li class="nav-item dropdown mt-1 mt-lg-0 mr-2">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
	Releases
</a>
<div class="dropdown-menu dropdown-menu-md-right dropdown-menu-lg-left" aria-labelledby="navbarDropdownMenuLink">
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/v1.6-release/docs">latest</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/v1.6-release/docs">v1.6</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/v1.5-release/docs">v1.5</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/v1.4-release/docs">v1.4</a>
	
	<a class="dropdown-item" href="https://gkcalat.github.io/kubeflow-docs/main/docs">dev</a>
	
</div>

			</li>
			
			
		</ul>
	</div>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            




<div class="td-content">
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This is the multi-page printable view of this section.
<a href="#" onclick="print();return false;">Click here to print</a>.
</p><p>
<a href="/kubeflow-docs/docs/external-add-ons/">Return to the regular view of this page</a>.
</p>
</div>



<h1 class="title">External Add-Ons</h1>
<div class="lead">Additional tools that may be integrated with a Kubeflow deployment or distribution.</div>




    <ul>
    
  
  
  
  

  
    
    
	
<li>1: <a href="#pg-c40225c5be27423a083c106cc64c0d51">Elyra</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>2: <a href="#pg-7fbae57ae82eabe6de269baae18ecabb">Istio</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>2.1: <a href="#pg-169c9e80445ee7a786ab34e9f86678b8">Istio Usage in Kubeflow</a></li>


    
  

    </ul>
    
  
    
    
	
<li>3: <a href="#pg-d99632ab95c75097ee3aef766cc52357">Kale</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>4: <a href="#pg-4f1c9c8ab068ef731ca35543a427b7b4">KServe</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>4.1: <a href="#pg-15bbdeccd5d20c0050c4ae305ffd6218">KServe</a></li>


    
  
    
    
	
<li>4.2: <a href="#pg-acff66d23d7c7d3586b4d8ff94e133a5">Migration</a></li>


    
  
    
    
	
<li>4.3: <a href="#pg-1d68cdc0f643957989aed2fc989e0ede">Models UI</a></li>


    
  
    
    
	
<li>4.4: <a href="#pg-ec2ce5b333fb900fdc270d79e3d82c1b">Run your first InferenceService</a></li>


    
  

    </ul>
    
  
    
    
	
<li>5: <a href="#pg-580f002c431f1164489f2d51dc7b67f7">Fairing</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.1: <a href="#pg-438b7809283effefe16dd15e83ba1934">Overview of Kubeflow Fairing</a></li>


    
  
    
    
	
<li>5.2: <a href="#pg-a84f31f425711f3e0cddc13239820304">Install Kubeflow Fairing</a></li>


    
  
    
    
	
<li>5.3: <a href="#pg-3337d0ae311b0fe9342c532be557e678">Configure Kubeflow Fairing</a></li>


    
  
    
    
	
<li>5.4: <a href="#pg-eca1245e8180b42aff844c8f9dd77dae">Fairing on Azure</a></li>


    
    <ul>
        
  
  
  
  

  

    </ul>
    
  
    
    
	
<li>5.5: <a href="#pg-9a587e37ad7a62dd0654ff1eed12a596">Fairing on GCP</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.5.1: <a href="#pg-a1e551ade6d34b3ad1e2901b129d9c1a">Configure Kubeflow Fairing with Access to GCP</a></li>


    
  
    
    
	
<li>5.5.2: <a href="#pg-4b2739af619e49823a1aa74b7fcdbb6d">GCP Samples and Tutorials</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.5.2.1: <a href="#pg-5cc6802d62798ba1ebe9d6cca8600ea9">Train and Deploy on GCP from a Local Notebook</a></li>


    
  
    
    
	
<li>5.5.2.2: <a href="#pg-656baba8a2689835585aa3ac071d3dd9">Train and Deploy on GCP from a Kubeflow Notebook</a></li>


    
  

    </ul>
    
  

    </ul>
    
  
    
    
	
<li>5.6: <a href="#pg-aa5f7153490b8c6be91ce5c4e17ef94f">Tutorials</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.6.1: <a href="#pg-fdf27b5ffdb640b8636dbdc6acc3436e">Other Samples and Tutorials</a></li>


    
  

    </ul>
    
  
    
    
	
<li>5.7: <a href="#pg-cca7dd273c2da2edff5d8a8155b942f3">Reference</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>5.7.1: <a href="#pg-c952cee2f080abc66b03de0b66a321a1"> Kubeflow Fairing SDK Reference</a></li>


    
  

    </ul>
    
  

    </ul>
    
  
    
    
	
<li>6: <a href="#pg-c2886c51a8da8d6861953f8f0cbdec4e">Feature Store</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>6.1: <a href="#pg-1151208ab46a907bcb9a8aa4c91e827f">Introduction to Feast</a></li>


    
  
    
    
	
<li>6.2: <a href="#pg-107bc41b7c52562cbc5f3663aabf54fa">Getting started with Feast</a></li>


    
  

    </ul>
    
  
    
    
	
<li>7: <a href="#pg-4a81d5e91c644e4da361430d155bd3e3">Tools for Serving</a></li>


    
    <ul>
        
  
  
  
  

  
    
    
	
<li>7.1: <a href="#pg-b846ae272222f6de23d4042d43dfeea9">Overview</a></li>


    
  
    
    
	
<li>7.2: <a href="#pg-3108d9ddd997e24e5b8436c63b4914d4">Seldon Core Serving</a></li>


    
  
    
    
	
<li>7.3: <a href="#pg-bd9c19194ad8f66e71fc6b468874ed49">BentoML</a></li>


    
  
    
    
	
<li>7.4: <a href="#pg-145f96a34e4a8b370a6013fbd8a1b5e1">MLRun Serving Pipelines</a></li>


    
  
    
    
	
<li>7.5: <a href="#pg-d97b7cb2a1b9c6a681aceec8809193ff">NVIDIA Triton Inference Server</a></li>


    
  
    
    
	
<li>7.6: <a href="#pg-a3d0c6465054fdbbd7e6aec57771eb34">TensorFlow Serving</a></li>


    
  
    
    
	
<li>7.7: <a href="#pg-dec844de279b9242c3a930b5ff4eda64">TensorFlow Batch Prediction</a></li>


    
  

    </ul>
    
  

    </ul>


<div class="content">
      
</div>
</div>


  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-c40225c5be27423a083c106cc64c0d51">1 - Elyra</h1>
    <div class="lead">Elyra enables data scientists to visually create end-to-end machine learning (ML) workflows.</div>
	<p>Elyra aims to help data scientists, machine learning engineers and AI developers
through the model development life cycle complexities. Elyra integrates with JupyterLab
providing a Pipeline visual editor that enables low code/no code creation of Pipelines
that can be executed in a Kubeflow environment.</p>
<p>Below is an example of a Piepline created with Elyra, you can identify the components/tasks
and related properties that are all managed in the visual editor.</p>
<img src="/docs/external-add-ons/elyra/elyra-pipeline-covid-scenario.png" alt="A pipeline example created using Elyra Pipeline Visual Editor" class="mt-3 mb-3 p-3 border border-info rounded" />
<p>To learn more about Elyra, visit <a href="https://github.com/elyra-ai/elyra" target="_blank">Elyra GitHub project</a></p>
<p>To enable Elyra in your Kubeflow Environment, visit <a href="https://elyra.readthedocs.io/en/stable/recipes/using-elyra-with-kubeflow-notebook-server.html" target="_blank">Using Elyra with the Kubeflow Notebook Server</a></p>

</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-7fbae57ae82eabe6de269baae18ecabb">2 - Istio</h1>
    <div class="lead">Documentation on Istio component in Kubeflow</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-169c9e80445ee7a786ab34e9f86678b8">2.1 - Istio Usage in Kubeflow</h1>
    <div class="lead">Managing access to Kubeflow applications and resources via Istio</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<p>Kubeflow v0.6 onwards deploys Istio along with configuration to enable
end-to-end authentication and access control. This setup is the foundation
of multi-tenancy support in Kubeflow. A Kubeflow deployment without Istio is
not possible.</p>
<h2 id="a-gentle-introduction-to-istio">A gentle introduction to Istio</h2>
<p>Most modern applications are built using a distributed microservices
architecture. This ensures that each individual service is simple and has a
well defined responsibility. Complex systems and platforms are generally
built by combining many such microservices. Each microservice defines its own
APIs and the services interact with each other using these APIs in order to
serve end-user requests.</p>
<p>The term <em>service mesh</em> is used to describe the network of microservices that
make up such applications and the interactions between them. As a service mesh
grows in size and complexity, it can become harder to understand and manage.
Its requirements can include discovery, load balancing, failure recovery,
metrics, and monitoring. A service mesh also often has more complex operational
requirements, like A/B testing, canary rollouts, rate limiting, access control,
and end-to-end authentication.</p>
<p><a href="https://istio.io/">Istio</a> is a pioneering and highly performant open source
implementation of service mesh by Google. For further details, you can read the
<a href="https://istio.io/docs/concepts/what-is-istio/">conceptual overview</a> of Istio.</p>
<h2 id="why-kubeflow-needs-istio">Why Kubeflow needs Istio</h2>
<p>Kubeflow is a collection of tools, frameworks and services that are deployed
together into a single Kubernetes cluster to enable end-to-end ML workflows.
Most of these components or services are developed independently and help with
different parts of the workflow. Developing a complete ML workflow or an ML
development environment requires combining multiple services and components.
Kubeflow provides the underlying infrastructure that makes it possible to put
such disparate components together.</p>
<p>Kubeflow uses Istio as a uniform way to secure, connect, and monitor microservices. Specifically:</p>
<ul>
<li>Securing service-to-service communication in a Kubeflow deployment with
strong identity-based authentication and authorization.</li>
<li>A policy layer for supporting access controls and quotas.</li>
<li>Automatic metrics, logs, and traces for traffic within the deployment
including cluster ingress and egress.</li>
</ul>
<h2 id="istio-in-kubeflow">Istio in Kubeflow</h2>
<p>The following diagram illustrates how user requests interact with services in
Kubeflow. It walks through the process when a user requests to create a new
notebook server via the Notebooks Servers UI accessible through the Kubeflow Central Dashboard.</p>
<!--
Note for authors: The source of the diagram is
in the "Doc diagrams" folder in the public Kubeflow shared drive.
-->
<p><img src="/docs/images/Istio-in-KF.svg" 
alt="Select active profile "
class="mt-3 mb-3 border border-info rounded"></p>
<ol>
<li>The user request is intercepted by an identification proxy which talks to
a SSO service provider such as IAM on Cloud Services Provider or Active
Directory/LDAP on-premises.</li>
<li>When the user is authenticated, the request is modified by the Istio
Gateway to include a JWT Header token containing the identity of the user.
All requests throughout the service mesh carry this token along.</li>
<li>The Istio RBAC policies are applied on the incoming request to validate
the access to the service and the requested namespace. If either of those
are inaccessible to the user, an error response is sent back.</li>
<li>If the request is validated, it is forwarded to the appropriate controller
(Notebooks Controller in this case).</li>
<li>Notebooks Controller validates authorization with Kubernetes RBAC and creates the
notebook pod in the namespace that the user requested.</li>
</ol>
<p>Further actions by the user with the notebook to create training jobs or other
resources in the namespace go through a similar process. Profiles Controller
manages the creation of profiles, and creates and applies appropriate Istio
policies. For more details, please see <a href="/docs/components/multi-tenancy/">multi-user
isolation</a>.</p>
<h2 id="deploying-kubeflow-without-istio">Deploying Kubeflow without Istio</h2>
<p>Currently it is not possible to deploy Kubeflow without Istio. Kubeflow needs the Istio
Custom Resource Definitions (CRDs) to express the new route to access the
created Notebook from the Gateway.</p>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d99632ab95c75097ee3aef766cc52357">3 - Kale</h1>
    <div class="lead">Kale enables data scientists to orchestrate end-to-end machine learning (ML) workflows.</div>
	<p>Kale simplifies the use of Kubeflow, giving data scientists the tool they need to orchestrate end-to-end ML workflows. Kale provides a UI in the form of a JupyterLab extension. You can annotate cells in Jupyter Notebooks to define: pipeline steps, hyperparameter tuning, GPU usage, and metrics tracking. Click a button to create pipeline components and KFP DSL, resolve dependencies, inject data objects into each step, deploy the data science pipeline, and serve the best model.</p>
<p>See <a href="https://codelabs.arrikto.com/codelabs/minikf-kale-katib-kfserving/#0" target="_blank">From Notebook to Kubeflow Pipelines to KFServing</a> for a tutorial overview of Kale.</p>

</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4f1c9c8ab068ef731ca35543a427b7b4">4 - KServe</h1>
    <div class="lead">Highly scalable and standards based Model Inference Platform on Kubernetes for Trusted AI</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-15bbdeccd5d20c0050c4ae305ffd6218">4.1 - KServe</h1>
    <div class="lead">Model serving using KServe</div>
	<img src="/docs/external-add-ons/kserve/pics/kserve.png" alt="KServe" width="640px">
<p>KServe enables serverless inferencing on Kubernetes and provides performant, high abstraction interfaces for common machine learning (ML) frameworks like TensorFlow, XGBoost, scikit-learn, PyTorch, and ONNX to solve production model serving use cases.</p>
<h3 id="kfserving-is-now-kservehttpskservegithubiowebsite07blogarticles2021-09-27-kfserving-transition"><a href="https://kserve.github.io/website/0.7/blog/articles/2021-09-27-kfserving-transition/">KFServing is now KServe</a></h3>
<p>The KFServing GitHub repository has been transferred to an independent KServe GitHub organization under the stewardship of the Kubeflow Serving Working Group leads.</p>
<h3 id="kserve-docshttpskservegithubiowebsite07"><a href="https://kserve.github.io/website/0.7/">KServe Docs</a></h3>
<p>The majority of KServe docs will be available on the new docs website and it is recommended to refer to the docs on the KServe website rather than this website</p>
<p>You can use KServe to do the following:</p>
<ul>
<li>
<p>Provide a Kubernetes <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definition</a> for serving ML models on arbitrary frameworks.</p>
</li>
<li>
<p>Encapsulate the complexity of autoscaling, networking, health checking, and server configuration to bring cutting edge serving features like GPU autoscaling, scale to zero, and canary rollouts to your ML deployments.</p>
</li>
<li>
<p>Enable a simple, pluggable, and complete story for your production ML inference server by providing prediction, pre-processing, post-processing and explainability out of the box.</p>
</li>
</ul>
<p>Our strong community contributions help KServe to grow. We have a Technical Steering Committee driven by Bloomberg, IBM Cloud, Seldon, Amazon Web Services (AWS) and NVIDIA.  Please browse the <a href="https://github.com/KServe/KServe">KServe GitHub repo</a> and <a href="https://github.com/KServe/KServe/issues">raise issues</a> to give us feedback!</p>
<h2 id="install-with-kubeflow">Install with Kubeflow</h2>
<p>KServe works with Kubeflow 1.5. Kustomize installation files are <a href="https://github.com/kubeflow/manifests/tree/master/contrib/kserve">located in the manifests repo</a>.
Check the examples running KServe on Istio/Dex in the <a href="https://github.com/KServe/KServe/tree/master/docs/samples/istio-dex"><code>KServe/KServe</code></a> repository. For installation on major cloud providers with Kubeflow, follow their installation docs.</p>
<p>Kubeflow 1.5 includes KServe v0.7 which promoted the core InferenceService API from v1alpha2 to v1beta1 stable and added ModelMesh component to the release. Additionally, LFAI Trusted AI Projects on AI Fairness, AI Explainability and Adversarial Robustness have been integrated with KServe, and we have made KServe available on OpenShift as well. To know more, please read the <a href="https://kserve.github.io/website/blog/articles/2021-10-11-KServe-0.7-release/">release blog</a> and follow the <a href="https://github.com/KServe/KServe/releases/tag/v0.7.0">release notes</a></p>
<h2 id="standalone-kserve">Standalone KServe</h2>
<h3 id="quickstart-installhttpskservegithubiowebsite07get_started"><a href="https://kserve.github.io/website/0.7/get_started/">Quickstart Install</a></h3>
<p>KServe Quickstart Environments are for experimentation use only. For production installation, see our <a href="https://kserve.github.io/website/0.7/admin/serverless/">Administrator&rsquo;s Guide</a></p>
<h2 id="learn-more">Learn more</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=lj_X2ND2BBI">Kubeflow 101: What is KFserving?</a></li>
<li><a href="https://drive.google.com/file/d/16oqz6dhY5BR0u74pi9mDThU97Np__AFb/view">KFServing 101 slides</a>.</li>
<li><a href="https://kccncna19.sched.com/event/UaZo/introducing-kfserving-serverless-model-serving-on-kubernetes-ellis-bigelow-google-dan-sun-bloomberg">Kubecon Introducing KFServing</a>.</li>
<li><a href="https://www.youtube.com/watch?v=sE_A54T2n6k">Serving Machine Learning Models at Scale Using KServe - Yuzhui Liu, Bloomberg</a></li>
<li><a href="https://www.youtube.com/watch?v=0YmM_h7PvpI">KServe (Kubeflow KFServing) Live Coding Session</a></li>
<li><a href="https://www.youtube.com/watch?v=0H-HvK8zIUI">TFiR: Let’s Talk About IBM’s ModelMesh, KServe And Other Open Source AI/ML Technologies</a> | Animesh Singh |</li>
<li><a href="https://www.youtube.com/watch?v=la3Y0lXuKRM">KubeCon 2021: Serving Machine Learning Models at Scale Using KServe</a> | Animesh Singh |</li>
</ul>
<h3 id="kserve-key-links"><strong>KServe Key Links</strong></h3>
<ul>
<li><a href="https://kserve.github.io/website/"><u>Website</u></a></li>
<li><a href="https://github.com/kserve/kserve/"><u>Github</u></a></li>
<li><a href="https://kubeflow.slack.com/join/shared_invite/zt-n73pfj05-l206djXlXk5qdQKs4o1Zkg#/"><u>Slack(#kubeflow-kfserving)</u></a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-acff66d23d7c7d3586b4d8ff94e133a5">4.2 - Migration</h1>
    <div class="lead">Migrating from KFServing to KServe</div>
	<p>The migration job will by default delete the leftover KFServing installation after migrating the InferenceServices from
<code>serving.kubeflow.org</code> to <code>serving.kserve.io</code>.</p>
<h3 id="migrating-from-kubeflow-based-kfserving">Migrating from Kubeflow-based KFServing</h3>
<ol>
<li>
<p>Install Kubeflow-based KServe 0.7 using the <a href="https://github.com/kserve/kserve/blob/master/install/v0.7.0/kserve_kubeflow.yaml">install YAML</a></p>
<ul>
<li>This will not affect existing services yet.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl apply -f https://raw.githubusercontent.com/kserve/kserve/master/install/v0.7.0/kserve_kubeflow.yaml
</span></span></code></pre></div></li>
<li>
<p>Run the <a href="https://github.com/kserve/kserve/blob/master/hack/kserve_migration/kserve_migration_job_kubeflow.yaml">KServe Migration YAML</a> for Kubeflow-based installations</p>
<ul>
<li>
<p>This will begin the migration. Any errors here may affect your existing services.</p>
</li>
<li>
<p>If you do not want to delete the KFServing resources after migrating, download and edit the env <code>REMOVE_KFSERVING</code>
in the YAML before applying it</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl apply -f https://raw.githubusercontent.com/kserve/kserve/master/hack/kserve_migration/kserve_migration_job_kubeflow.yaml
</span></span></code></pre></div></li>
<li>
<p>Clean up the migration resources</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl delete ClusterRoleBinding cluster-migration-rolebinding
</span></span><span class="line"><span class="cl">kubectl delete ClusterRole cluster-migration-role
</span></span><span class="line"><span class="cl">kubectl delete ServiceAccount cluster-migration-svcaccount -n kubeflow
</span></span></code></pre></div></li>
<li>
<p>Update the models web app to use the new InferenceService API group <code>serving.kserve.io</code></p>
<ul>
<li>Change the deployment image to <code>kserve/models-web-app:v0.7.0</code></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl edit deployment kfserving-models-web-app -n kubeflow
</span></span></code></pre></div></li>
<li>
<p>Update the cluster role to be able to access the new InferenceService API group <code>serving.kserve.io</code></p>
<ul>
<li>Edit the <code>apiGroups</code> from <code>serving.kubeflow.org</code> to <code>serving.kserve.io</code></li>
<li>This is a temporary fix until the next Kubeflow release includes these changes</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl edit clusterrole kfserving-models-web-app-cluster-role
</span></span></code></pre></div></li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-1d68cdc0f643957989aed2fc989e0ede">4.3 - Models UI</h1>
    <div class="lead">Web app for managing Model servers</div>
	<p>The Models web app is responsible for allowing the user to manipulate the Model Servers in their Kubeflow cluster. To achieve this it provides a user friendly way to handle the lifecycle of <code>InferenceService</code> CRs.</p>
<p>The web app currently works with <code>v1beta1</code> versions of <code>InferenceService</code> objects.</p>
<p>The web app is also exposing information from the underlying Knative resources,
like Conditions from the Knative Configurations, Route and Revisions as well as
live logs from the Model server pod.</p>
<h2 id="installation-and-access">Installation and Access</h2>
<p>Refer <a href="https://github.com/kserve/models-web-app/#development">https://github.com/kserve/models-web-app/#development</a> for installation</p>
<p>The web app includes the following resources:</p>
<ul>
<li>A <code>Deployment</code> for running the backend server, and serving the static frontend files</li>
<li>A <code>Service</code> for configuring the incluster network traffic</li>
<li>A <code>ServiceAccount</code> and <code>ClusterRole{Binding}</code> to give the necessary
permissions to the web app&rsquo;s Pod</li>
<li>A <code>VirtualService</code> for exposing the app via the cluster&rsquo;s Istio Ingress
Gateway</li>
</ul>
<h3 id="kubeflow">Kubeflow</h3>
<p>The web app is included as a part of the Kubeflow <a href="https://github.com/kubeflow/manifests/tree/v1.5-branch/contrib/kserve/models-web-app">1.5
release</a> manifests.
It is <a href="https://github.com/kubeflow/manifests/blob/v1.5-branch/apps/centraldashboard/upstream/base/configmap.yaml#L30">exposed</a> via the
Central Dashboard, out of the box.</p>
<h3 id="standalone">Standalone</h3>
<p>In this case all the resources of the web app will be installed in the
<code>kserve</code> namespace. Users can access the web app either via the
<code>knative-ingress-gateway.knative-serving</code> Istio Ingress Gateway or by
port-forwarding the backend.</p>
<h4 id="port-forwarding">Port forwarding</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># set the following ENV vars in the app&#39;s Deployment</span>
</span></span><span class="line"><span class="cl">kubectl edit -n kserve deployments.apps kserve-models-web-app
</span></span><span class="line"><span class="cl"><span class="c1"># APP_PREFIX: /</span>
</span></span><span class="line"><span class="cl"><span class="c1"># APP_DISABLE_AUTH: &#34;True&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># APP_SECURE_COOKIES: &#34;False&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># expose the app under localhost:5000</span>
</span></span><span class="line"><span class="cl">kubectl port-forward -n kserve svc/kserve-models-web-app 5000:80
</span></span></code></pre></div><h2 id="authorization">Authorization</h2>
<h3 id="subjectaccessreviews">SubjectAccessReviews</h3>
<p>The web app has a mechanism for performing authentication and authorization
checks, to ensure that user actions are compliant with the cluster&rsquo;s RBAC,
which is only enabled in the <em>kubeflow</em> manifests of the app. This mechanism
can be toggled by leveraging the <code>APP_DISABLE_AUTH: &quot;True&quot; | &quot;False&quot;</code> ENV Var.</p>
<p>This mechanism is only enabled in the <em>kubeflow</em> manifests since in a Kubeflow
installation all requests that end up in the web app&rsquo;s Pod will also contain a custom
header that denotes the user. In a Kubeflow installation there&rsquo;s an authentication
component in front of the cluster that ensures only logged in users can
access the cluster&rsquo;s services. In the standalone mode such a
component might not always be deployed.</p>
<p>The web app will be using the value from this custom header to extract the name
of the <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">K8s
user</a>
that made the request. Then it will create a
<a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/#determine-whether-a-request-is-allowed-or-denied">SubjectAccessReview</a>
to check if the user has permissions to perform the specific action, for
example deleting an InferenceService in a namespace.</p>


<div class="alert alert-info" role="alert">
<h4 class="alert-heading">Tip</h4>

    If you are port-forwarding the app via <strong>kubectl port-forward</strong> then you will
need to set <strong>APP_DISABLE_AUTH=&ldquo;True&rdquo;</strong> in the web app&rsquo;s Deployment. When
port-forwarding the authentication header will not be set, which will result in the
web app raising <strong>401</strong> errors.

</div>

<h3 id="namespace-selection">Namespace selection</h3>
<p>Both in <em>standalone</em> and in <em>kubeflow</em> setups the user needs to be able to
select a Namespace in order to interact with the InferenceServices in it.</p>
<p>In <em>standalone</em> mode the web app will show a dropdown that will show all the
namespaces to the user and allow them to select any of them. The backend will
make a LIST request to the API Server to get all the namespaces. In this case
the only authorization check that takes place is in the K8s API Server that
ensures the <a href="https://github.com/kserve/models-web-app/blob/release-0.7/config/base/rbac.yaml">web app Pod&rsquo;s
ServiceAccount</a>
has permissions to list namespaces.</p>
<p>In <em>kubeflow</em> mode the <a href="/docs/components/central-dash/overview/">Central
Dashboard</a> is responsible for the
Namespace selection. Once the user selects a namespace then the Dashboard will
inform the iframed Models web app about the newly selected namespace. The
Models web app itself won&rsquo;t expose a dropdown namespace selector in this mode.</p>
<h2 id="use-cases">Use Cases</h2>
<p>Currently users can do the following workflows via this web app:</p>
<ul>
<li>See a list of the existing InferenceService CRs in a Namespace</li>
<li>Create a new InferenceService by providing a YAML</li>
<li>Inspect an InferenceService
<ul>
<li>View the live status of the InferenceService</li>
<li>Inspect the K8s Conditions of the underlying Knative resources</li>
<li>View the logs of the created Model server Pod, for that InferenceService</li>
<li>Inspect the YAML contents as they are stored in the K8s API Server</li>
<li>View some basic metrics</li>
</ul>
</li>
</ul>
<h3 id="listing">Listing</h3>
<p>The main page of the app provides a list of all the InferenceServices that are
deployed in the selected Namespace. The frontend periodically polls the backend
for the latest state of InferenceServices.</p>
<img src="/docs/external-add-ons/kserve/pics/webapp-list.png" alt="Models web app main page">
<h3 id="creating">Creating</h3>
<p>The page for creating a new InferenceService. The user can paste the YAML
object of the InferenceService they wish to create.</p>
<p>Note that the backend will override the provided <code>.metadata.namespace</code> field of
the submitted object, to prevent users from trying to create InferenceServices
in other namespaces.</p>
<img src="/docs/external-add-ons/kserve/pics/webapp-new.png" alt="Models web app create page">
<h3 id="deleting">Deleting</h3>
<p>Users can delete an existing InferenceService by clicking on the
<i class="fas fa-trash"></i> icon next to an InferenceService, in the main page
that lists all the namespaced resources.</p>


<div class="alert alert-info" role="alert">
<h4 class="alert-heading">Note</h4>

    The backend is using <a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#foreground-cascading-deletion">foreground cascading
deletion</a>
when deleting an InferenceService. This means that the InferenceService CR will
be deleted from the K8s API Server only once the underlying resources have been
deleted.

</div>

<h3 id="inspecting">Inspecting</h3>
<p>Users can click on the name of an InferenceService, from the main page, and
view a more detailed summary of the CR&rsquo;s state. In this page users can inspect:</p>
<ol>
<li>The overview of the InferenceService&rsquo;s status (OVERVIEW)</li>
<li>A user friendly representation of the CR&rsquo;s spec (DETAILS)</li>
<li>Metrics from the underlying resources (METRICS)</li>
<li>Logs from the created Pods (LOGS)</li>
<li>The YAML file as is in the K8s API Server (YAML)</li>
</ol>
<img src="/docs/external-add-ons/kserve/pics/webapp-overview.png" alt="Models web app overview page">


<div class="alert alert-info" role="alert">
<h4 class="alert-heading">Note</h4>

    <p>To gather the logs the backend will:</p>
<ol>
<li>Filter all the pods that have a <code>serving.knative.dev/revision</code> label</li>
<li>Get the logs from the <code>kserve-container</code></li>
</ol>


</div>

<h2 id="metrics">Metrics</h2>
<p>As mentioned in the above sections the web app allows users to inspect the
metrics from the InferenceService. This tab will <strong>not</strong> be enable by default.
In order to expose it the users will need to install Grafana and Prometheus.</p>
<p>Currently the frontend is expecting to find a Grafana exposed in the <code>/grafana</code>
prefix. This Grafana instance will need to have specific dashboards in order
for the app to embed them in iframes. We are working on making this more
generic to allow people to expose their own graphs.</p>
<p>You can install Grafana and Prometheus, for the web app to consume, by
installing</p>
<ol>
<li>the <code>monitoring-core.yaml</code> and
<code>monitoring-metrics-prometheus.yaml</code> files from the <a href="https://github.com/knative/serving/releases/tag/v0.18.0">Knative 0.18
release</a></li>
<li>the following yaml files for exposing Grafana outside the cluster, by
allowing <strong>anonymous access</strong></li>
</ol>
<ul class="nav nav-tabs" id="grafana-installation-yamls" role="tablist"><li class="nav-item"><a data-toggle="tab" class="nav-link active" href="#grafana-installation-yamls-0" role="tab" aria-controls="grafana-installation-yamls-0" aria-selected="true">ConfigMap</a></li>
	  
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#grafana-installation-yamls-1" role="tab" aria-controls="grafana-installation-yamls-1">VirtualService</a></li>
		<li class="nav-item"><a data-toggle="tab" class="nav-link" href="#grafana-installation-yamls-2" role="tab" aria-controls="grafana-installation-yamls-2">AuthorizationPolicy</a></li></ul>
<div class="tab-content" id="grafana-installation-yamls"><div id="grafana-installation-yamls-0" class="tab-pane show active" role="tabpanel" aria-labelledby="grafana-installation-yamls-0">

<p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-custom-config</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">knative-monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">serving.knative.dev/release</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;v0.11.0&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">custom.ini</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">    # You can customize Grafana via changing the context of this field.
</span></span></span><span class="line"><span class="cl"><span class="sd">    [auth.anonymous]
</span></span></span><span class="line"><span class="cl"><span class="sd">    # enable anonymous access
</span></span></span><span class="line"><span class="cl"><span class="sd">    enabled = true
</span></span></span><span class="line"><span class="cl"><span class="sd">    [security]
</span></span></span><span class="line"><span class="cl"><span class="sd">    allow_embedding = true
</span></span></span><span class="line"><span class="cl"><span class="sd">    [server]
</span></span></span><span class="line"><span class="cl"><span class="sd">    root_url = &#34;/grafana&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">    serve_from_sub_path = true</span><span class="w">    
</span></span></span></code></pre></div></div>
  <div id="grafana-installation-yamls-1" class="tab-pane" role="tabpanel" aria-labelledby="grafana-installation-yamls-1">

<p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">networking.istio.io/v1alpha3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">VirtualService</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">knative-monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">gateways</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">kubeflow/kubeflow-gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="s1">&#39;*&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">http</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">match</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">uri</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">prefix</span><span class="p">:</span><span class="w"> </span><span class="l">/grafana/</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">route</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">destination</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">grafana.knative-monitoring.svc.cluster.local</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">port</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="m">30802</span><span class="w">
</span></span></span></code></pre></div></div>
  <div id="grafana-installation-yamls-2" class="tab-pane" role="tabpanel" aria-labelledby="grafana-installation-yamls-2">

<p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">security.istio.io/v1beta1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">AuthorizationPolicy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">models-web-app</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">ALLOW</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">from</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">source</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">principals</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">kustomize.component</span><span class="p">:</span><span class="w"> </span><span class="l">kserve-models-web-app</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">app.kubernetes.io/component</span><span class="p">:</span><span class="w"> </span><span class="l">kserve-models-web-app</span><span class="w">
</span></span></span></code></pre></div></div></div>



<div class="alert alert-info" role="alert">
<h4 class="alert-heading">Note</h4>

    If you installed the app in the <em>standalone</em> mode then you will need to instead
use the <strong>knative-serving/knative-ingress-gateway</strong> Ingress Gateway and deploy
the AuthorizationPolicy in the <strong>kserve</strong> namespace instead.

</div>

<p>After applying these YAMLs, based on your installation mode, and ensuring the
Grafana instance is exposed under <code>/grafana</code> the web app will show the
<code>METRICS</code> tab.</p>
<img src="/docs/external-add-ons/kserve/pics/webapp-metrics.png" alt="Models web app metrics page">
<h2 id="configurations">Configurations</h2>
<p>The following is a list of ENV var that can configure different aspects of the
application.</p>
<table>
<thead>
<tr>
<th>ENV Var</th>
<th>Default value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>APP_PREFIX</td>
<td>&ldquo;/models&rdquo;</td>
<td>Controls the app&rsquo;s prefix, by setting the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base">base-url</a> element</td>
</tr>
<tr>
<td>APP_DISABLE_AUTH</td>
<td>&ldquo;False&rdquo;</td>
<td>Controls whether the app should use SubjectAccessReviews to ensure the user is authorized to perform an action</td>
</tr>
<tr>
<td>APP_SECURE_COOKIES</td>
<td>&ldquo;True&rdquo;</td>
<td>Controls whether the app should use <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie#Secure">Secure</a> CSRF cookies. By default the app expects to be exposed with https</td>
</tr>
<tr>
<td>CSRF_SAMESITE</td>
<td>&ldquo;Strict&rdquo;</td>
<td>Controls the <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie#SameSite">SameSite value</a> of the CSRF cookie</td>
</tr>
<tr>
<td>USERID_HEADER</td>
<td>&ldquo;kubeflow-userid&rdquo;</td>
<td>Header in each request that will contain the username of the logged in user</td>
</tr>
<tr>
<td>USERID_PREFIX</td>
<td>&quot;&quot;</td>
<td>Prefix to remove from the <code>USERID_HEADER</code> value to extract the logged in user name</td>
</tr>
</tbody>
</table>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-ec2ce5b333fb900fdc270d79e3d82c1b">4.4 - Run your first InferenceService</h1>
    <div class="lead">A tutorial on building and deploying a model using the KServe Python SDK</div>
	<!--
AUTOGENERATED FROM content/en/docs/external-add-ons/kserve/first_isvc_kserve.ipynb
PLEASE UPDATE THE JUPYTER NOTEBOOK AND REGENERATE THIS FILE USING scripts/nb_to_md.py.-->
<style>
.notebook-links {display: flex; margin: 1em 0;}
.notebook-links a {padding: .75em; margin-right: .75em; font-weight: bold;}
a.colab-link {
padding-left: 3.25em;
background-image: url(/docs/images/logos/colab.ico);
background-repeat: no-repeat;
background-size: contain;
}
a.github-link {
padding-left: 2.75em;
background-image: url(/docs/images/logos/github.png);
background-repeat: no-repeat;
background-size: auto 75%;
background-position: left center;
}
</style>
<div class="notebook-links">
<a class="colab-link" href="https://colab.research.google.com/github/kubeflow/website/blob/master/content/en/docs/external-add-ons/kserve/first_isvc_kserve.ipynb">Run in Google Colab</a>
<a class="github-link" href="https://github.com/kubeflow/website/blob/master/content/en/docs/external-add-ons/kserve/first_isvc_kserve.ipynb">View source on GitHub</a>
</div>
<p>The InferenceService custom resource is the primary interface that is used for deploying models on KServe. Inside an InferenceService, users can specify multiple
components that are used for handling inference requests. These components are the predictor, transformer, and explainer. Learn more <a href="https://kserve.github.io/website/0.7/modelserving/data_plane/">here</a>.</p>
<p>In this tutorial, you will deploy an InferenceService with a predictor that will load a scikit-learn model trained with the <a href="https://archive.ics.uci.edu/ml/datasets/iris">iris</a> dataset.
This dataset has three output class: Iris Setosa, Iris Versicolour, and Iris Virginica.</p>
<p>You will then send an inference request to your deployed model in order to get a prediction for the class of iris plant your request corresponds to.</p>
<h2 id="before-you-begin">Before you begin</h2>
<p>First, install the KServe SDK using the following command. If you run this command in a Jupyter notebook, restart the kernel after installing the SDK.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">kserve</span><span class="o">==</span><span class="mf">0.7.0</span>
</span></span></code></pre></div><h2 id="import-kubernetesclient-and-kserve-packages">Import <code>kubernetes.client</code> and <code>kserve</code> packages</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kubernetes</span> <span class="kn">import</span> <span class="n">client</span> 
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kserve</span> <span class="kn">import</span> <span class="n">KServeClient</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kserve</span> <span class="kn">import</span> <span class="n">constants</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kserve</span> <span class="kn">import</span> <span class="n">utils</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kserve</span> <span class="kn">import</span> <span class="n">V1beta1InferenceService</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kserve</span> <span class="kn">import</span> <span class="n">V1beta1InferenceServiceSpec</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kserve</span> <span class="kn">import</span> <span class="n">V1beta1PredictorSpec</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">kserve</span> <span class="kn">import</span> <span class="n">V1beta1SKLearnSpec</span>
</span></span></code></pre></div><h2 id="declare-namespace">Declare Namespace</h2>
<p>This will retrieve the current namespace of your Kubernetes context. The InferenceService will be deployed in this namespace.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">namespace</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">get_default_target_namespace</span><span class="p">()</span>
</span></span></code></pre></div><h2 id="define-inferenceservice">Define InferenceService</h2>
<p>Next, define the InferenceService based on several key parameters. In the <code>predictor</code> parameter, a <code>V1beta1PredictorSpec</code> object with an embedded <code>V1beta1SKLearnSpec</code> object is created.
Inside the <code>V1beta1SKLearnSpec</code> object, a storage URI is provided, pointing to the location of the trained iris model in cloud storage.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">name</span><span class="o">=</span><span class="s1">&#39;sklearn-iris&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">kserve_version</span><span class="o">=</span><span class="s1">&#39;v1beta1&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">api_version</span> <span class="o">=</span> <span class="n">constants</span><span class="o">.</span><span class="n">KSERVE_GROUP</span> <span class="o">+</span> <span class="s1">&#39;/&#39;</span> <span class="o">+</span> <span class="n">kserve_version</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">isvc</span> <span class="o">=</span> <span class="n">V1beta1InferenceService</span><span class="p">(</span><span class="n">api_version</span><span class="o">=</span><span class="n">api_version</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                               <span class="n">kind</span><span class="o">=</span><span class="n">constants</span><span class="o">.</span><span class="n">KSERVE_KIND</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                               <span class="n">metadata</span><span class="o">=</span><span class="n">client</span><span class="o">.</span><span class="n">V1ObjectMeta</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="n">namespace</span><span class="p">,</span> <span class="n">annotations</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sidecar.istio.io/inject&#39;</span><span class="p">:</span><span class="s1">&#39;false&#39;</span><span class="p">}),</span>
</span></span><span class="line"><span class="cl">                               <span class="n">spec</span><span class="o">=</span><span class="n">V1beta1InferenceServiceSpec</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                               <span class="n">predictor</span><span class="o">=</span><span class="n">V1beta1PredictorSpec</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                               <span class="n">sklearn</span><span class="o">=</span><span class="p">(</span><span class="n">V1beta1SKLearnSpec</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">storage_uri</span><span class="o">=</span><span class="s2">&#34;gs://kfserving-samples/models/sklearn/iris&#34;</span><span class="p">))))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><h2 id="create-inferenceservice">Create InferenceService</h2>
<p>Now, with the InferenceService defined, you can now create it by calling the <code>create</code> method of the <code>KServeClient</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">KServe</span> <span class="o">=</span> <span class="n">KServeClient</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">KServe</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">isvc</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="check-the-inferenceservice">Check the InferenceService</h2>
<p>Run the following command to watch the InferenceService until it is ready (or times out).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">KServe</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="n">namespace</span><span class="p">,</span> <span class="n">watch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="perform-inference">Perform Inference</h2>
<p>Next, you can try sending an inference request to the deployed model in order to get predictions. This notebook assumes that you running
it in your Kubeflow cluster and will use the internal URL of the InferenceService.</p>
<p>The Python <code>requests</code> library will be used to send a POST request containing your payload.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">requests</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">isvc_resp</span> <span class="o">=</span> <span class="n">KServe</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="n">namespace</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">isvc_url</span> <span class="o">=</span> <span class="n">isvc_resp</span><span class="p">[</span><span class="s1">&#39;status&#39;</span><span class="p">][</span><span class="s1">&#39;address&#39;</span><span class="p">][</span><span class="s1">&#39;url&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">isvc_url</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">inference_input</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;instances&#39;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">6.8</span><span class="p">,</span>  <span class="mf">2.8</span><span class="p">,</span>  <span class="mf">4.8</span><span class="p">,</span>  <span class="mf">1.4</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span>  <span class="mf">3.4</span><span class="p">,</span>  <span class="mf">4.5</span><span class="p">,</span>  <span class="mf">1.6</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">isvc_url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">inference_input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</span></span></code></pre></div><p>You should see two predictions returned (i.e. <code>{&quot;predictions&quot;: [1, 1]}</code>). Both sets of data points sent for inference correspond to the flower with index <code>1</code>.
In this case, the model predicts that both flowers are &ldquo;Iris Versicolour&rdquo;.</p>
<p>To learn more about sending inference requests, please check out the <a href="https://kserve.github.io/website/0.7/get_started/first_isvc/#3-determine-the-ingress-ip-and-ports">KServe guide</a>.</p>
<h2 id="run-performance-test-optional">Run Performance Test (Optional)</h2>
<p>If you want to load test the deployed model, try deploying the Kubernetes Job to drive load to the InferenceService.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">kubectl</span> <span class="n">create</span> <span class="o">-</span><span class="n">f</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">raw</span><span class="o">.</span><span class="n">githubusercontent</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">kserve</span><span class="o">/</span><span class="n">kserve</span><span class="o">/</span><span class="n">release</span><span class="o">-</span><span class="mf">0.7</span><span class="o">/</span><span class="n">docs</span><span class="o">/</span><span class="n">samples</span><span class="o">/</span><span class="n">v1beta1</span><span class="o">/</span><span class="n">sklearn</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">perf</span><span class="o">.</span><span class="n">yaml</span> <span class="o">-</span><span class="n">n</span> <span class="n">kubeflow</span><span class="o">-</span><span class="n">user</span><span class="o">-</span><span class="n">example</span><span class="o">-</span><span class="n">com</span>
</span></span></code></pre></div><h3 id="get-job-name">Get Job Name</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">kubectl</span> <span class="n">get</span> <span class="n">pods</span> <span class="o">--</span><span class="n">namespace</span><span class="o">=</span><span class="n">kubeflow</span><span class="o">-</span><span class="n">user</span><span class="o">-</span><span class="n">example</span><span class="o">-</span><span class="n">com</span> <span class="o">|</span> <span class="n">grep</span> <span class="n">load</span>
</span></span></code></pre></div><h3 id="check-the-job-logs">Check the Job Logs</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">kubectl</span> <span class="n">logs</span> <span class="o">&lt;</span><span class="n">job</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">n</span> <span class="n">kubeflow</span><span class="o">-</span><span class="n">user</span><span class="o">-</span><span class="n">example</span><span class="o">-</span><span class="n">com</span>
</span></span></code></pre></div><p>The output should look like similar to the following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Requests      [total, rate, throughput]         30000, 500.02, 499.99
</span></span><span class="line"><span class="cl">Duration      [total, attack, wait]             1m0s, 59.998s, 3.336ms
</span></span><span class="line"><span class="cl">Latencies     [min, mean, 50, 90, 95, 99, max]  1.743ms, 2.748ms, 2.494ms, 3.363ms, 4.091ms, 7.749ms, 46.354ms
</span></span><span class="line"><span class="cl">Bytes In      [total, mean]                     690000, 23.00
</span></span><span class="line"><span class="cl">Bytes Out     [total, mean]                     2460000, 82.00
</span></span><span class="line"><span class="cl">Success       [ratio]                           100.00%
</span></span><span class="line"><span class="cl">Status Codes  [code:count]                      200:30000
</span></span><span class="line"><span class="cl">Error Set:
</span></span></code></pre></div><h2 id="delete-inferenceservice">Delete InferenceService</h2>
<p>When you are done with your InferenceService, you can delete it by running the following.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">KServe</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="n">namespace</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="next-steps">Next Steps</h2>
<p><a href="https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/samples/contrib/kubeflow-e2e-mnist/kubeflow-e2e-mnist.ipynb"><strong>Kubeflow Pipelines E2E MNIST Tutorial</strong></a> - provides an end-to-end test sequence (i.e. start a notebook, run a pipeline, execute training, hyperparameter tuning, and model serving with KServe).</p>
<div class="notebook-links">
<a class="colab-link" href="https://colab.research.google.com/github/kubeflow/website/blob/master/content/en/docs/external-add-ons/kserve/first_isvc_kserve.ipynb">Run in Google Colab</a>
<a class="github-link" href="https://github.com/kubeflow/website/blob/master/content/en/docs/external-add-ons/kserve/first_isvc_kserve.ipynb">View source on GitHub</a>
</div>
</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-580f002c431f1164489f2d51dc7b67f7">5 - Fairing</h1>
    <div class="lead">Documentation for Kubeflow Fairing.</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-438b7809283effefe16dd15e83ba1934">5.1 - Overview of Kubeflow Fairing</h1>
    <div class="lead">Build, train, and deploy your ML training jobs remotely</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<div class="alert alert-warning" role="alert">
  <h4 class="alert-heading">Beta</h4>
  This Kubeflow component has <b>beta</b> status. See the
  <a href="/docs/started/support/#application-status">Kubeflow versioning policies</a>.
  The Kubeflow team is interested in your   
  <a href="https://github.com/kubeflow/fairing/issues">feedback</a></h4> 
  about the usability of the feature.
</div>
<p>Kubeflow Fairing streamlines the process of building, training, and deploying
machine learning (ML) training jobs in a hybrid cloud environment. By using
Kubeflow Fairing and adding a few lines of code, you can run your ML training
job locally or in the cloud, directly from Python code or a Jupyter
notebook. After your training job is complete, you can use Kubeflow Fairing to
deploy your trained model as a prediction endpoint.</p>
<h2 id="getting-started">Getting started</h2>
<p>Use the following guides to get started with Kubeflow Fairing:</p>
<ol>
<li>To set up your development environment, follow the guide to <a href="/docs/external-add-ons/fairing/install-fairing/">installing
Kubeflow Fairing</a>.</li>
<li>To ensure that Kubeflow Fairing can access your Kubeflow cluster, follow
the guide to <a href="/docs/external-add-ons/fairing/configure-fairing/">configuring your development environment with access
to Kubeflow</a>.</li>
<li>To learn more about how to use Kubeflow Fairing in your environment,
<a href="/docs/external-add-ons/fairing/tutorials/other-tutorials/">follow the Kubeflow Fairing tutorials</a>.</li>
</ol>
<h2 id="what-is-kubeflow-fairing">What is Kubeflow Fairing?</h2>
<p>Kubeflow Fairing is a Python package that makes it easy to train and deploy ML
models on <a href="/docs/started/">Kubeflow</a>. Kubeflow Fairing can also been extended to
train or deploy on other platforms. Currently, Kubeflow Fairing has been
extended to train on <a href="https://cloud.google.com/ml-engine/docs/">Google AI Platform</a>.</p>
<p>Kubeflow Fairing packages your Jupyter notebook, Python function, or Python
file as a Docker image, then deploys and runs the training job on Kubeflow
or AI Platform. After your training job is complete, you can use Kubeflow
Fairing to deploy your trained model as a prediction endpoint on Kubeflow.</p>
<p>The following are the goals of the <a href="https://github.com/kubeflow/fairing">Kubeflow Fairing project</a>:</p>
<ul>
<li><strong>Easily package ML training jobs:</strong> Enable ML practitioners to easily package their ML model training code, and their code&rsquo;s dependencies, as a Docker image.</li>
<li><strong>Easily train ML models in a hybrid cloud environment:</strong> Provide a high-level API for training ML models to make it easy to run training jobs in the cloud, without needing to understand the underlying infrastructure.</li>
<li><strong>Streamline the process of deploying a trained model:</strong> Make it easy for ML practitioners to deploy trained ML models to a hybrid cloud environment.</li>
</ul>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>Learn how to <a href="/docs/components/notebooks/setup/">set up a Jupyter notebooks instance on your Kubeflow
cluster</a>.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a84f31f425711f3e0cddc13239820304">5.2 - Install Kubeflow Fairing</h1>
    <div class="lead">Setting up your Kubeflow Fairing development environment</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<p>You can use Kubeflow Fairing to build, train, and deploy machine learning (ML)
models in a hybrid cloud environment directly from Python code or a Jupyter
notebook. This guide describes how to install Kubeflow Fairing in your
development environment for <a href="#set-up-kubeflow-fairing-for-local-development">local development</a>, or <a href="#set-up-kubeflow-fairing-in-a-hosted-jupyter-notebook">development in a
hosted notebook</a>.</p>
<h2 id="using-kubeflow-fairing-with-kubeflow-notebooks">Using Kubeflow Fairing with Kubeflow notebooks</h2>
<p>Kubeflow notebook servers that are built from one of the standard Jupyter
Docker images include Kubeflow Fairing and come preconfigured for using
Kubeflow Fairing to run training jobs on your Kubeflow cluster.</p>
<p>If you use a Kubeflow notebook server that was built from a custom Jupyter
Docker image as your development environment, follow the instruction on
<a href="#set-up-kubeflow-fairing-in-a-hosted-jupyter-notebook">setting up Kubeflow Fairing in a hosted notebook environment</a>.</p>
<h2 id="set-up-kubeflow-fairing-for-local-development">Set up Kubeflow Fairing for local development</h2>
<p>Follow these instructions to set up Kubeflow Fairing for local development.
This guide has been tested on Linux and Mac OS X. Currently, this guide has
not been tested on Windows.</p>
<h3 id="set-up-python">Set up Python</h3>
<ol>
<li>
<p>You need <strong>Python 3.6</strong> or later to use Kubeflow Fairing. To check if
you have Python 3.6 or later installed, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python3 -V
</span></span></code></pre></div><p>The response should be something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Python 3.6.5
</span></span></code></pre></div><p>If you do not have Python 3.6 or later, you can <a href="https://www.python.org/downloads/">download
Python</a> from the Python Software
Foundation.</p>
</li>
<li>
<p>Use virtualenv to create a virtual environment to install Kubeflow
Fairing in. To check if you have virtualenv installed, run the
following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">which virtualenv
</span></span></code></pre></div><p>The response should be something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/usr/bin/virtualenv
</span></span></code></pre></div><p>If you do not have virtualenv, use pip3 to install virtualenv.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip3 install virtualenv
</span></span></code></pre></div><p>Create a new virtual environment, and activate it.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">virtualenv venv --python<span class="o">=</span>python3
</span></span><span class="line"><span class="cl"><span class="nb">source</span> venv/bin/activate
</span></span></code></pre></div></li>
</ol>
<h3 id="install-kubeflow-fairing">Install Kubeflow Fairing</h3>
<p>Run the following command to install Kubeflow Fairing in your virtual
environment.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install kubeflow-fairing
</span></span></code></pre></div><p>After the install is complete, the <code>fairing</code> python package is
available. Run the following command to verify that Kubeflow Fairing
is installed:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip show kubeflow-fairing
</span></span></code></pre></div><p>The response should be something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Name: kubeflow-fairing
</span></span><span class="line"><span class="cl">Version: 0.6.0
</span></span><span class="line"><span class="cl">Summary: Kubeflow Fairing Python SDK.
</span></span><span class="line"><span class="cl">Home-page: https://github.com/kubeflow/fairing
</span></span><span class="line"><span class="cl">Author: Kubeflow Authors
</span></span><span class="line"><span class="cl">Author-email: hejinchi@cn.ibm.com
</span></span><span class="line"><span class="cl">License: Apache License Version 2.0
</span></span><span class="line"><span class="cl">Location: &lt;path-to-kubeflow-fairing&gt;
</span></span><span class="line"><span class="cl">Requires: notebook, future, docker, tornado, cloudpickle, oauth2client, numpy, requests, setuptools, httplib2, google-auth, google-api-python-client, urllib3, boto3, azure, six, kubernetes, google-cloud-storage
</span></span></code></pre></div><h3 id="docker-setup">Docker setup</h3>
<p>Kubeflow Fairing uses Docker to package your code. Run the following command
to verify if Docker is installed and running:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker ps
</span></span></code></pre></div><ul>
<li>If you receive the <code>docker: command not found</code> message, <a href="https://docs.docker.com/install/">install
Docker</a>.</li>
<li>If you receive the <code>Error response from daemon: Bad response from Docker engine</code> message, <a href="https://docs.docker.com/config/daemon/#start-the-daemon-manually">restart your docker daemon</a>.</li>
<li>If you are using Linux and you use sudo to access Docker, follow these
steps to <a href="https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user">add your user to the docker group</a>. Note, the
docker group grants privileges equivalent to the root user. To learn more
about how this affects security in your system, see the guide to the
<a href="https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface">Docker daemon attack surface</a>.</li>
</ul>
<h3 id="configure-kubeflow-fairing">Configure Kubeflow Fairing</h3>
<p>To configure Kubeflow Fairing with access to an environment that you would like to
use for training and deployment, follow the instructions in the <a href="/docs/external-add-ons/fairing/configure-fairing/">guide to
configuring Kubeflow Fairing</a>.</p>
<h2 id="set-up-kubeflow-fairing-in-a-hosted-jupyter-notebook">Set up Kubeflow Fairing in a hosted Jupyter notebook</h2>
<p>Follow these instructions to set up Kubeflow Fairing in a hosted Jupyter
notebook.</p>
<p>If you are using a Kubeflow notebook server that was built from one of the
standard Jupyter Docker images, your notebooks environment has been
preconfigured for training and deploying ML models with Kubeflow Fairing and
no additional installation steps are required.</p>
<h3 id="prerequisites">Prerequisites</h3>
<p>Check the following prerequisites to verify that Kubeflow Fairing is compatible
with your hosted notebook environment.</p>
<ol>
<li>
<p>In the Jupyter notebooks user interface, click <strong>File</strong> &gt; <strong>New</strong> &gt;
<strong>Terminal</strong> in the menu to start a new terminal session in your notebook
environment.</p>
</li>
<li>
<p>You need <strong>Python 3.6</strong> or later to use Kubeflow Fairing. To check if you
have Python 3.6 or later installed, run the following command in your
terminal session:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python3 -V
</span></span></code></pre></div><p>The response should be something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Python 3.6.5
</span></span></code></pre></div></li>
<li>
<p>Kubeflow Fairing uses Docker to package your code. Run the following
command in your terminal session to verify if Docker is installed and
running in your notebook environment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker ps
</span></span></code></pre></div><ul>
<li>If you receive the <code>docker: command not found</code> message, <a href="https://docs.docker.com/install/">install
Docker</a>.</li>
<li>If you receive the <code>Error response from daemon: Bad response from Docker engine</code> message, <a href="https://docs.docker.com/config/daemon/#start-the-daemon-manually">restart your docker daemon</a>.</li>
<li>If you are using Linux and you use sudo to access Docker, follow these
steps to <a href="https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user">add your user to the docker group</a>. Note, the
docker group grants privileges equivalent to the root user. To learn
more about how this affects security in your system, see the guide to
the <a href="https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface">Docker daemon attack surface</a>.</li>
</ul>
</li>
</ol>
<h3 id="install-kubeflow-fairing-1">Install Kubeflow Fairing</h3>
<ol>
<li>
<p>In the Jupyter notebooks user interface, click <strong>File</strong> &gt; <strong>New</strong> &gt;
<strong>Terminal</strong> in the menu to start a new terminal session in your notebook
environment.</p>
</li>
<li>
<p>Run the following command to install Kubeflow Fairing.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip3 install kubeflow-fairing
</span></span></code></pre></div><p>After successful installation, the <code>fairing</code> python package should be
available. Run the following command to verify that Kubeflow Fairing
is installed:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip3 show kubeflow-fairing
</span></span></code></pre></div><p>The response should be something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Name: kubeflow-fairing
</span></span><span class="line"><span class="cl">Version: 0.6.0
</span></span><span class="line"><span class="cl">Summary: Kubeflow Fairing Python SDK.
</span></span><span class="line"><span class="cl">Home-page: https://github.com/kubeflow/fairing
</span></span><span class="line"><span class="cl">Author: Kubeflow Authors
</span></span><span class="line"><span class="cl">Author-email: hejinchi@cn.ibm.com
</span></span><span class="line"><span class="cl">License: Apache License Version 2.0
</span></span><span class="line"><span class="cl">Location: &lt;path-to-kubeflow-fairing&gt;
</span></span><span class="line"><span class="cl">Requires: notebook, future, docker, tornado, cloudpickle, oauth2client, numpy, requests, setuptools, httplib2, google-auth, google-api-python-client, urllib3, boto3, azure, six, kubernetes, google-cloud-storage
</span></span></code></pre></div></li>
</ol>
<h3 id="configure-kubeflow-fairing-1">Configure Kubeflow Fairing</h3>
<p>To configure Kubeflow Fairing with access to the environment you would like to
use for training and deployment, follow the instructions in the guide to
<a href="/docs/external-add-ons/fairing/configure-fairing/">configuring Kubeflow Fairing</a>.</p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li><a href="/docs/external-add-ons/fairing/configure-fairing/">Configure your Kubeflow Fairing development environment</a> with access
to run training jobs remotely.</li>
<li>Follow the <a href="/docs/external-add-ons/fairing/tutorials/other-tutorials/">samples and tutorials</a> to learn more about how to run
training jobs remotely with Kubeflow Fairing.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3337d0ae311b0fe9342c532be557e678">5.3 - Configure Kubeflow Fairing</h1>
    <div class="lead">Configuring your Kubeflow Fairing development environment with access to Kubeflow</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<p>In order to use Kubeflow Fairing to train or deploy a machine learning
model on Kubeflow, you must configure your development environment with access
to your container image registry and your Kubeflow cluster. This guide
describes how to configure Kubeflow Fairing to run training jobs on Kubeflow.</p>
<p>Additional configuration steps are required to access Kubeflow when it is hosted on a cloud
environment. Use the following guides to configure Kubeflow Fairing with access
to your hosted Kubeflow environment.</p>
<ul>
<li>To use Kubeflow Fairing to train and deploy on Kubeflow on Google Kubernetes
Engine, follow the guide to <a href="/docs/external-add-ons/fairing/gcp/configure-gcp/">configuring Kubeflow Fairing with access to
Google Cloud Platform</a>.</li>
</ul>
<h2 id="prerequisites">Prerequisites</h2>
<p>Before you configure Kubeflow Fairing, you must have a Kubeflow environment
and Kubeflow Fairing installed in your development environment.</p>
<ul>
<li>If you do not have a Kubeflow cluster, follow the <a href="/docs/started/getting-started/">getting started
with Kubeflow</a> guide to set one up.</li>
<li>If you have not installed Kubeflow Fairing, follow the <a href="/docs/external-add-ons/fairing/install-fairing/">installing
Kubeflow Fairing</a> guide.</li>
</ul>
<h2 id="using-kubeflow-fairing-with-kubeflow-notebooks">Using Kubeflow Fairing with Kubeflow notebooks</h2>
<p>The standard Kubeflow notebook images include Kubeflow Fairing and come
preconfigured to run training jobs on your Kubeflow cluster. No additional
configuration is required.</p>
<p>If you built your Kubeflow notebook server from a custom Jupyter Docker image,
follow the instruction in this guide to configure your notebooks environment
with access to your Kubeflow environment.</p>
<h2 id="configure-docker-with-access-to-your-container-image-registry">Configure Docker with access to your container image registry</h2>
<p>Authorize Docker to access your container image registry by following the
instructions in the <a href="https://docs.docker.com/engine/reference/commandline/login/"><code>docker login</code> reference guide</a>.</p>
<h2 id="configure-access-to-your-kubeflow-cluster">Configure access to your Kubeflow cluster</h2>
<p>Use the following instructions to configure <code>kubeconfig</code> with access to your
Kubeflow cluster.</p>
<ol>
<li>
<p>Kubeflow Fairing uses <code>kubeconfig</code> to access your Kubeflow cluster. This
guide uses <code>kubectl</code> to set up your <code>kubeconfig</code>. To check if you have
<code>kubectl</code> installed, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">which kubectl
</span></span></code></pre></div><p>The response should be something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/usr/bin/kubectl
</span></span></code></pre></div><p>If you do not have <code>kubectl</code> installed, follow the instructions in the
guide to <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">installing and setting up kubectl</a>.</p>
</li>
<li>
<p>Follow the <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">guide to configuring access to Kubernetes
clusters</a>, to update your <code>kubeconfig</code> with appropriate
credentials and endpoint information to access your Kubeflow cluster.</p>
</li>
</ol>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>Follow the <a href="/docs/external-add-ons/fairing/tutorials/other-tutorials/">samples and tutorials</a> to learn more about how to run
training jobs remotely with Kubeflow Fairing.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-eca1245e8180b42aff844c8f9dd77dae">5.4 - Fairing on Azure</h1>
    <div class="lead">Documentation for Kubeflow Fairing on Microsoft Azure Kubernetes Service</div>
	<p>This page documents how to run the <a href="https://github.com/kubeflow/fairing/blob/master/examples/prediction/xgboost-high-level-apis.ipynb">Fairing prediction example
notebook</a> on <a href="https://azure.microsoft.com/en-in/services/kubernetes-service/">Azure Kubernetes Service
(AKS)</a> in a notebook hosted on Kubeflow.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>Before you configure and use the Kubeflow Fairing Python SDK, you must have a
Kubeflow environment installed. The example notebook does in-cluster training.
The example notebook has been tested to work in a cluster with 2 nodes and
virtual machines with Standard_D4_v3 size. It is not recommended to use the
smallest size virtual machines.</p>
<ul>
<li>If you do not have a Kubeflow installation on Azure, follow the <a href="/docs/azure/deploy/install-kubeflow">installation
guide</a>.</li>
<li>You must have the <code>kubectl</code> command line interface installed and configured
to use the Kubernetes cluster where Kubeflow is installed. In the above
installation guide, the command <code>az aks get-credentials</code> configures <code>kubectl</code>
to access your Kubernetes cluster.</li>
</ul>
<h2 id="create-azure-container-registry-and-storage">Create Azure Container Registry and Storage</h2>
<p>The example notebook uses Azure Container Registry (ACR) to host docker
images for deployment and Azure Storage (Storage) is used as build context for
in-cluster building.</p>
<p>You can re-use existing ACR and Storage resources or create new ones. For more
information, see the documentation for <a href="https://docs.microsoft.com/en-us/azure/container-registry/">ACR</a> and
<a href="https://docs.microsoft.com/en-us/azure/storage/">Storage</a>.</p>
<p>After you have created your ACR and Storage resources, you must configure a
service principal with access to these resources.</p>
<p>You can create a new service principal with the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">az ad sp create-for-rbac --name &lt;name&gt;
</span></span></code></pre></div><p>The above command has output like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">{
</span></span><span class="line"><span class="cl">  &#34;appId&#34;: &#34;&lt;client-id&gt;&#34;,
</span></span><span class="line"><span class="cl">  &#34;displayName&#34;: &#34;&lt;name&gt;&#34;,
</span></span><span class="line"><span class="cl">  &#34;name&#34;: &#34;http://&lt;name&gt;&#34;,
</span></span><span class="line"><span class="cl">  &#34;password&#34;: &#34;&lt;client-secret&gt;&#34;,
</span></span><span class="line"><span class="cl">  &#34;tenant&#34;: &#34;&lt;tenant-id&gt;&#34;
</span></span><span class="line"><span class="cl">}
</span></span></code></pre></div><p>The role you grant must have at least read and write permissions to ACR and
Storage. We recommend using the <code>AcrPush</code> role and the <code>Storage Account Contributor</code> role scoped to a resource group.</p>
<p>You can perform the role assignment for ACR like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">az role assignment create --assignee &lt;id&gt; \
</span></span><span class="line"><span class="cl">  --scope /subscriptions/&lt;azure-subscription-id&gt;/resourceGroups/&lt;resource-group-name&gt;/providers/Microsoft.ContainerRegistry/registries/&lt;acr-name&gt; \
</span></span><span class="line"><span class="cl">  --role &#34;AcrPush&#34;
</span></span></code></pre></div><p>You can grant full storage rights at a resource group level like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">az role assignment create --assignee &lt;id&gt; \
</span></span><span class="line"><span class="cl">  --scope /subscriptions/&lt;azure-subscription-id&gt;/resourceGroups/&lt;resource-group-name&gt; \
</span></span><span class="line"><span class="cl">  --role &#34;Storage Account Contributor&#34;
</span></span></code></pre></div><p>To learn more about how to grant the service principal access, follow the
<a href="https://docs.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal">Azure role-based authentication documentation</a>.</p>
<h2 id="setting-up-credentials-as-kubernetes-secrets">Setting up credentials as Kubernetes secrets</h2>
<p>Before running the notebook, you must first configure credentials so that the
Python code running within the cluster can access the Azure resources required
to train and deploy a model.</p>
<p>Run the following commands to set up your credentials as a Kubernetes secret.</p>
<ol>
<li>
<p>Set environment variables to use in the following commands.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export AZ_CLIENT_ID=&lt;service-principal-client-id&gt;
</span></span><span class="line"><span class="cl">export AZ_CLIENT_SECRET=&lt;service-principal-client-secret&gt;
</span></span><span class="line"><span class="cl">export AZ_TENANT_ID=&lt;tenant-id&gt;
</span></span><span class="line"><span class="cl">export AZ_SUBSCRIPTION_ID=&lt;subscription-id&gt;
</span></span><span class="line"><span class="cl">export TARGET_NAMESPACE=&lt;target-namespace e.g. kubeflow-anonymous&gt;
</span></span><span class="line"><span class="cl">export ACR_NAME=&lt;acr-name&gt;
</span></span></code></pre></div><ul>
<li><strong>AZ_CLIENT_ID:</strong> The service principal client ID. You can get the
<code>client_id</code> property from ~/.azure/aksServicePrincipal.json.</li>
<li><strong>AZ_CLIENT_SECRET:</strong> The service principal secret.</li>
<li><strong>AZ_TENANT_ID:</strong> The Azure Tenant ID of your account. You can get the
Tenant ID from the <code>tenantId</code> field in the output of <code>az account show</code>.</li>
<li><strong>AZ_SUBSCRIPTION:</strong> The Azure Subscription ID of your account. You can
get the Subscription ID from the <code>id</code> field in the output of <code>az account show</code>.</li>
<li><strong>TARGET_NAMESPACE:</strong> Specify the namespace that your Notebook Server is
in. For example, this guide recommends using <code>kubeflow-anonymous</code>.</li>
<li><strong>ACR_NAME:</strong> The name of an ACR that the service principal can access.</li>
</ul>
</li>
<li>
<p>Use the following command to create a secret that Kubeflow can use to access
Azure APIs.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl create secret generic -n ${TARGET_NAMESPACE} azcreds \
</span></span><span class="line"><span class="cl">--from-literal=AZ_CLIENT_ID=${AZ_CLIENT_ID} \
</span></span><span class="line"><span class="cl">--from-literal=AZ_CLIENT_SECRET=${AZ_CLIENT_SECRET} \
</span></span><span class="line"><span class="cl">--from-literal=AZ_TENANT_ID=${AZ_TENANT_ID} \
</span></span><span class="line"><span class="cl">--from-literal=AZ_SUBSCRIPTION_ID=${AZ_SUBSCRIPTION_ID}
</span></span></code></pre></div></li>
<li>
<p>Use the following command to create a secret that Kubeflow can use to access
ACR.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl create secret docker-registry -n ${TARGET_NAMESPACE} acrcreds \
</span></span><span class="line"><span class="cl">--docker-server=${ACR_NAME}.azurecr.io \
</span></span><span class="line"><span class="cl">--docker-username=${AZ_CLIENT_ID} \
</span></span><span class="line"><span class="cl">--docker-password=${AZ_CLIENT_SECRET}
</span></span><span class="line"><span class="cl">kubectl patch serviceaccount default-editor -n ${TARGET_NAMESPACE} \
</span></span><span class="line"><span class="cl">-p &#34;{\&#34;imagePullSecrets\&#34;: [{\&#34;name\&#34;: \&#34;acrcreds\&#34;}]}&#34;
</span></span></code></pre></div></li>
</ol>
<h2 id="creating-a-notebook-server-in-kubeflow">Creating a Notebook Server in Kubeflow</h2>
<p>To create a notebook server, use your Web browser to access the Kubeflow
Central Dashboard and select the <strong>Notebook Servers</strong> panel from the menu.</p>
<p>First, select the target namespace in which you want to host the server. In the
default Kubeflow installation, there should be a namespace <code>kubeflow-anonymous</code>
available in the namespace drop-down menu.</p>
<p>After the target namespace is selected, click <strong>NEW SERVER</strong> and fill in the
mandatory fields. The fields with default values can all be left as they
are and do not have to be modified to run the example notebook.</p>
<p>After launching the server, wait for the <strong>CONNECT</strong> button to appear and click
<strong>CONNECT</strong> to launch your Notebook Server. It may take up to a minute for the
server to be ready for connections.</p>
<h2 id="cloning-the-example-notebook">Cloning the example notebook</h2>
<p>Clone the Kubeflow Fairing repository to download the files used in this example.</p>
<ol>
<li>
<p>Connect to your notebook server, then click the new terminal option
like in the screenshot below:
<img src="/docs/images/azure-notebook-new-terminal.png" alt="Creating new terminal after connecting to notebook server" class="mt-3 mb-3 p-3 border border-info rounded" /></p>
</li>
<li>
<p>Run the following command to clone the Kubeflow Fairing project:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">git clone https://github.com/kubeflow/fairing.git
</span></span></code></pre></div><p>This command clones the project including the example into your notebook server.</p>
</li>
</ol>
<p>You can now close the terminal window, and you should now see the <code>fairing</code> folder
in your notebooks server&rsquo;s Files tab. Navigate to the example notebooks at
<code>fairing/examples/prediction/xgboost-high-level-apis.ipynb</code>.</p>
<h2 id="executing-the-notebook">Executing the notebook</h2>
<p>Before running the cells, read the following to learn how to configure Fairing
to use your Azure resources.</p>
<p>In the <code>xgboost-high-level-apis.ipynb</code> notebook, find the cell tagged with
<code>parameters</code> and update this cell to use Azure. Update following values to
configure Fairing to use your Azure backend and Storage.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">export FAIRING_BACKEND = &#39;KubeflowAzureBackend&#39;
</span></span><span class="line"><span class="cl">export DOCKER_REGISTRY = &#39;&lt;acr-name&gt;.azurecr.io&#39;
</span></span><span class="line"><span class="cl">export AZURE_REGION = None # This is only relevant if you haven&#39;t created a
</span></span><span class="line"><span class="cl">                           # Storage yet and let Fairing create it for you.
</span></span><span class="line"><span class="cl">                           # In that case, you can specify the region as
</span></span><span class="line"><span class="cl">                           # string, for example, &#39;NorthEurope&#39;.
</span></span><span class="line"><span class="cl">export AZURE_RESOURCE_GROUP = &#39;&lt;storage-resource-group&gt;&#39;
</span></span><span class="line"><span class="cl">export AZURE_STORAGE_ACCOUNT = &#39;&lt;storage-name&gt;&#39;
</span></span></code></pre></div><p>After the above steps have been completed, you can run the example notebook.</p>
<p>You can also have a look at the <a href="https://dev.azure.com/kubeflow/kubeflow/_build">CI pipeline</a> that runs the
example notebook in AKS for steps involved to accomplish a successful run
programmatically.</p>

</div>



    
      
  
  
  
  

  
  

  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-9a587e37ad7a62dd0654ff1eed12a596">5.5 - Fairing on GCP</h1>
    <div class="lead">Documentation for using Kubeflow Fairing to train or deploy on Google Cloud Platform (GCP)</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-a1e551ade6d34b3ad1e2901b129d9c1a">5.5.1 - Configure Kubeflow Fairing with Access to GCP</h1>
    <div class="lead">Configuring your Kubeflow Fairing development environment to access Kubeflow on GKE</div>
	<p>This guide describes how to configure your development environment with access
to Google Cloud Platform (GCP), so you can use Kubeflow Fairing to train or
deploy a model on Kubeflow on Google Kubernetes Engine (GKE).</p>
<p>If you have not installed Kubeflow Fairing, follow the guide to <a href="/docs/external-add-ons/fairing/install-fairing/">installing
Kubeflow Fairing</a> before continuing.</p>
<h2 id="using-kubeflow-fairing-with-kubeflow-notebooks">Using Kubeflow Fairing with Kubeflow notebooks</h2>
<p>The standard Kubeflow notebook images include Kubeflow Fairing and come
preconfigured to run training jobs on your Kubeflow cluster. No additional
configuration is required.</p>
<p>If your Kubeflow notebook server was built from a custom Jupyter Docker image,
follow the instruction in this guide to configure your notebooks environment
with access to your Kubeflow environment.</p>
<h2 id="install-and-configure-the-google-cloud-sdk">Install and configure the Google Cloud SDK</h2>
<p>Follow these instructions to set up the Google Cloud SDK in your local
development environment.</p>
<ol>
<li>
<p>To check if you have the Google Cloud SDK installed, run the following
command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">which gcloud
</span></span></code></pre></div><p>The response should be something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/usr/bin/gcloud
</span></span></code></pre></div><p>If you do not have the Google Cloud SDK installed, follow the guide to
<a href="https://cloud.google.com/sdk/docs/">installing the Google Cloud SDK</a>.</p>
</li>
<li>
<p>Use <code>gcloud</code> to set a default project.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">PROJECT_ID</span><span class="o">=</span>&lt;your-project-id&gt;
</span></span><span class="line"><span class="cl">gcloud config <span class="nb">set</span> project <span class="nv">$PROJECT_ID</span>
</span></span></code></pre></div></li>
<li>
<p>Kubeflow Fairing needs a service account to make API calls to GCP. The
recommended way to provide Fairing with access to this
service account is to set the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment
variable. To check for the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment
variable, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ls <span class="s2">&#34;</span><span class="nv">$GOOGLE_APPLICATION_CREDENTIALS</span><span class="s2">&#34;</span>
</span></span></code></pre></div><p>The response should be something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/.../.../key.json
</span></span></code></pre></div><p>If you do not have a service account, then create one and grant it access
to the required roles.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">SA_NAME</span><span class="o">=</span>&lt;your-sa-name&gt;
</span></span><span class="line"><span class="cl">gcloud iam service-accounts create <span class="nv">$SA_NAME</span>
</span></span><span class="line"><span class="cl">gcloud projects add-iam-policy-binding <span class="nv">$PROJECT_ID</span> <span class="se">\ </span>
</span></span><span class="line"><span class="cl">    --member serviceAccount:<span class="nv">$SA_NAME</span>@<span class="nv">$PROJECT_ID</span>.iam.gserviceaccount.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --role <span class="s1">&#39;roles/editor&#39;</span>
</span></span></code></pre></div><p>Create a key for your service account.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud iam service-accounts keys create ~/key.json <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --iam-account <span class="nv">$SA_NAME</span>@<span class="nv">$PROJECT_ID</span>.iam.gserviceaccount.com
</span></span></code></pre></div><p>Create the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">GOOGLE_APPLICATION_CREDENTIALS</span><span class="o">=</span>~/key.json
</span></span></code></pre></div></li>
</ol>
<h2 id="configure-docker-with-access-to-container-registry">Configure Docker with access to Container Registry</h2>
<p>Authorize Docker to access your <a href="https://cloud.google.com/container-registry/">GCP Container Registry</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud auth configure-docker
</span></span></code></pre></div><h2 id="configure-access-to-your-kubeflow-cluster">Configure access to your Kubeflow cluster</h2>
<p>Use the following instructions to update your <code>kubeconfig</code> with credentials
and endpoint information for your Kubeflow cluster. If you do not have a
Kubeflow cluster, follow the guide to <a href="/docs/gke/deploy/">deploying Kubeflow on
GKE</a> to set one up.</p>
<ol>
<li>
<p>To find your cluster&rsquo;s name, run the following command to list the
clusters in your project:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud container clusters list
</span></span></code></pre></div></li>
<li>
<p>Update the following command with your cluster&rsquo;s name and GCP zone. Then,
run the command to update your <code>kubeconfig</code> to provide it with credentials
to access this cluster.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLUSTER_NAME</span><span class="o">=</span>kubeflow
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">ZONE</span><span class="o">=</span>us-central1-a
</span></span><span class="line"><span class="cl">gcloud container clusters get-credentials <span class="nv">$CLUSTER_NAME</span> --region <span class="nv">$ZONE</span>
</span></span></code></pre></div></li>
</ol>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>Follow the <a href="/docs/external-add-ons/fairing/gcp/tutorials/">GCP samples and tutorials</a> to learn more about how to run
training jobs remotely on GCP with Kubeflow Fairing.</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4b2739af619e49823a1aa74b7fcdbb6d">5.5.2 - GCP Samples and Tutorials</h1>
    <div class="lead">Try the samples and follow detailed tutorials for using Kubeflow Fairing to train and deploy on Google Cloud Platform (GCP)</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-5cc6802d62798ba1ebe9d6cca8600ea9">5.5.2.1 - Train and Deploy on GCP from a Local Notebook</h1>
    <div class="lead">Use Kubeflow Fairing to train and deploy a model on Google Cloud Platform (GCP) from a local notebook.</div>
	<p>This guide introduces you to using Kubeflow Fairing to train and deploy a
model to Kubeflow on Google Kubernetes Engine (GKE), and Google Cloud ML Engine.
As an example, this guide uses a local notebook to demonstrate how to:</p>
<ul>
<li>Train an XGBoost model in a local notebook,</li>
<li>Use Kubeflow Fairing to train an XGBoost model remotely on Kubeflow,</li>
<li>Use Kubeflow Fairing to train an XGBoost model remotely on Cloud ML Engine,</li>
<li>Use Kubeflow Fairing to deploy a trained model to Kubeflow, and</li>
<li>Call the deployed endpoint for predictions.</li>
</ul>
<p>This guide has been tested on Linux and Mac OS X. Currently, this guide has not been
tested on Windows.</p>
<h2 id="clone-the-kubeflow-fairing-repository">Clone the Kubeflow Fairing repository</h2>
<p>Clone the Kubeflow Fairing repository to download the files used in this example.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git clone https://github.com/kubeflow/fairing 
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> fairing
</span></span></code></pre></div><h2 id="set-up-python-jupyter-notebook-and-kubeflow-fairing">Set up Python, Jupyter Notebook, and Kubeflow Fairing</h2>
<ol>
<li>
<p>You need <strong>Python 3.6</strong> or later to use Kubeflow Fairing. To check if
you have Python 3.6 or later installed, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">python3 -V
</span></span></code></pre></div><p>The response should be something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Python 3.6.5
</span></span></code></pre></div><p>If you do not have Python 3.6 or later, you can <a href="https://www.python.org/downloads/">download
Python</a> from the Python Software
Foundation.</p>
</li>
<li>
<p>Use virtualenv to create a virtual environment to install Kubeflow
Fairing in. To check if you have virtualenv installed, run the
following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">which virtualenv
</span></span></code></pre></div><p>The response should be something like this.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/usr/bin/virtualenv
</span></span></code></pre></div><p>If you do not have virtualenv, use pip3 to install virtualenv.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip3 install --upgrade virtualenv
</span></span></code></pre></div><p>Create a new virtual environment, and activate it.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">virtualenv venv --python<span class="o">=</span>python3
</span></span><span class="line"><span class="cl"><span class="nb">source</span> venv/bin/activate
</span></span></code></pre></div></li>
<li>
<p>Install Jupyter Notebook.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip3 install --upgrade jupyter
</span></span></code></pre></div></li>
<li>
<p>Install Kubeflow Fairing from the cloned repository.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip3 install --upgrade .
</span></span></code></pre></div></li>
<li>
<p>Install the Python dependencies for the XGBoost demo notebook.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip3 install -r examples/prediction/requirements.txt
</span></span></code></pre></div></li>
</ol>
<h2 id="install-and-configure-the-google-cloud-sdk">Install and configure the Google Cloud SDK</h2>
<p>In order to use Kubeflow Fairing to train or deploy to Kubeflow on GKE,
or Cloud Machine Learning Engine, you must configure
your development environment with access to GCP.</p>
<ol>
<li>
<p>If you do not have the Cloud SDK installed, <a href="https://cloud.google.com/sdk/docs/">install the
Cloud SDK</a>.</p>
</li>
<li>
<p>Use <code>gcloud</code> to set a default project.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">PROJECT_ID</span><span class="o">=</span>&lt;your-project-id&gt;
</span></span><span class="line"><span class="cl">gcloud config <span class="nb">set</span> project <span class="si">${</span><span class="nv">PROJECT_ID</span><span class="si">}</span>
</span></span></code></pre></div></li>
<li>
<p>Kubeflow Fairing needs a service account to make API calls to GCP. The
recommended way to provide Fairing with access to this
service account is to set the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment
variable. To check for the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment
variable, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ls <span class="s2">&#34;</span><span class="si">${</span><span class="nv">GOOGLE_APPLICATION_CREDENTIALS</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span></code></pre></div><p>The response should be something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/.../.../key.json
</span></span></code></pre></div><p>If you do not have a service account, then create one and grant it
access to the required roles.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">SA_NAME</span><span class="o">=</span>&lt;your-sa-name&gt;
</span></span><span class="line"><span class="cl">gcloud iam service-accounts create <span class="si">${</span><span class="nv">SA_NAME</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">gcloud projects add-iam-policy-binding <span class="si">${</span><span class="nv">PROJECT_ID</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --member serviceAccount:<span class="si">${</span><span class="nv">SA_NAME</span><span class="si">}</span>@<span class="si">${</span><span class="nv">PROJECT_ID</span><span class="si">}</span>.iam.gserviceaccount.com <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --role <span class="s1">&#39;roles/editor&#39;</span>
</span></span></code></pre></div><p>Create a key for your service account.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud iam service-accounts keys create ~/key.json <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --iam-account <span class="si">${</span><span class="nv">SA_NAME</span><span class="si">}</span>@<span class="si">${</span><span class="nv">PROJECT_ID</span><span class="si">}</span>.iam.gserviceaccount.com
</span></span></code></pre></div><p>Create the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">GOOGLE_APPLICATION_CREDENTIALS</span><span class="o">=</span>~/key.json
</span></span></code></pre></div></li>
</ol>
<h2 id="set-up-docker">Set up Docker</h2>
<p>You need to have Docker installed to use Kubeflow Fairing. Fairing packages
your code as a Docker image and executes it in the remote cluster. To check
if your local Docker daemon is running, run the following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker ps
</span></span></code></pre></div><ul>
<li>If you get a message like <code>docker: command not found</code>, then <a href="https://docs.docker.com/install/">install
Docker</a>.</li>
<li>If you get an error like <code>Error response from daemon: Bad response from Docker engine</code>, then <a href="https://docs.docker.com/config/daemon/#start-the-daemon-using-operating-system-utilities">restart your docker daemon</a>.</li>
<li>If you are using Linux and you use sudo to access Docker, follow these
steps to <a href="https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user">add your user to the <code>docker</code> group</a>. Note, the
<code>docker</code> group grants privileges equivalent to the root user. To learn more
about how this affects security in your system, see the guide to the
<a href="https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface">Docker daemon attack surface</a>.</li>
</ul>
<p>Authorize Docker to access your <a href="https://cloud.google.com/container-registry/">GCP Container Registry</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud auth configure-docker
</span></span></code></pre></div><h2 id="set-up-kubeflow">Set up Kubeflow</h2>
<p>Use the following instructions to set up and configure your Kubeflow and
development environments for training and prediction from Kubeflow Fairing.</p>
<ol>
<li>
<p>If you do not have a Kubeflow environment, follow the guide to <a href="https://www.kubeflow.org/docs/gke/deploy/">deploying
Kubeflow on GKE</a> to set up your Kubeflow environment.
The guide provides two options for setting up your environment:</p>
<ul>
<li>The <a href="https://deploy.kubeflow.cloud">Kubeflow deployment user interface</a> is an easy
way for you to set up a GKE cluster with Kubeflow
installed, or</li>
<li>You can deploy Kubeflow using the <a href="https://www.kubeflow.org/docs/gke/deploy/deploy-cli/">command line</a>.</li>
</ul>
</li>
<li>
<p>Update your <code>kubeconfig</code> with appropriate credentials and endpoint
information for your Kubeflow cluster. To find your
cluster&rsquo;s name, run the following command to list the clusters in your
project:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">gcloud container clusters list
</span></span></code></pre></div><p>Update the following command with your cluster&rsquo;s name and GCP zone, then
run the command to update your <code>kubeconfig</code> to provide it with credentials
to access this Kubeflow cluster.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CLUSTER_NAME</span><span class="o">=</span>kubeflow
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">ZONE</span><span class="o">=</span>us-central1-a
</span></span><span class="line"><span class="cl">gcloud container clusters get-credentials <span class="si">${</span><span class="nv">CLUSTER_NAME</span><span class="si">}</span> --region <span class="si">${</span><span class="nv">ZONE</span><span class="si">}</span>
</span></span></code></pre></div></li>
</ol>
<h2 id="use-kubeflow-fairing-to-train-a-model-locally-and-on-gcp">Use Kubeflow Fairing to train a model locally and on GCP</h2>
<ol>
<li>
<p>Launch the XGBoost quickstart in a local Jupyter notebook.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">jupyter notebook examples/prediction/xgboost-high-level-apis.ipynb
</span></span></code></pre></div></li>
<li>
<p>Follow the instructions in the notebook to train a model locally, on
Kubeflow, and on Cloud ML Engine. Then deploy the trained model
to Kubeflow for predictions and send requests to the prediction endpoint.</p>
</li>
</ol>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-656baba8a2689835585aa3ac071d3dd9">5.5.2.2 - Train and Deploy on GCP from a Kubeflow Notebook</h1>
    <div class="lead">Use Kubeflow Fairing to train and deploy a model on Google Cloud Platform (GCP) from a notebook that is hosted on Kubeflow</div>
	<p>This guide introduces you to using <a href="https://github.com/kubeflow/fairing">Kubeflow Fairing</a> to train and
deploy a model to Kubeflow on Google Kubernetes Engine (GKE) and
Google AI Platform Training.</p>
<p>Your Kubeflow deployment includes services for spawning and managing Jupyter
notebooks. Kubeflow Fairing is preinstalled in the Kubeflow notebooks, along
with a number of machine learning (ML) libraries.</p>
<h2 id="set-up-kubeflow-and-access-the-kubeflow-notebook-environment">Set up Kubeflow and access the Kubeflow notebook environment</h2>
<p>Follow the <a href="/docs/components/notebooks/setup/">Kubeflow notebook setup guide</a>
to install Kubeflow, access your Kubeflow hosted notebook environment, and
create a new notebook server.</p>
<p>When selecting a Docker image and other settings for the baseline deployment
of your notebook server, you can leave all the settings at the default value.</p>
<h2 id="run-the-example-notebook">Run the example notebook</h2>
<p>As an example, this guide uses a notebook that is hosted on Kubeflow
to demonstrate how to:</p>
<ul>
<li>Train an XGBoost model in a notebook,</li>
<li>Use Kubeflow Fairing to train an XGBoost model remotely on Kubeflow,</li>
<li>Use Kubeflow Fairing to train an XGBoost model remotely on
AI Platform Training,</li>
<li>Use Kubeflow Fairing to deploy a trained model to Kubeflow, and</li>
<li>Call the deployed endpoint for predictions.</li>
</ul>
<p>Follow these instructions to run the XGBoost quickstart notebook:</p>
<ol>
<li>
<p>Download the files used in this example and install the packages that the
XGBoost quickstart notebook depends on.</p>
<ol>
<li>
<p>On the <strong>Jupyter dashboard</strong> for your notebook server, click <strong>New</strong> and
select <strong>Terminal</strong> to start a new terminal session in your notebook
environment. Use the terminal session to set up your notebook
environment to run this example.</p>
</li>
<li>
<p>Clone the Kubeflow Fairing repository to download the files used in
this example.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git clone https://github.com/kubeflow/fairing 
</span></span></code></pre></div></li>
<li>
<p>Install the Python dependencies for the XGBoost quickstart notebook.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip3 install -r fairing/examples/prediction/requirements.txt
</span></span></code></pre></div></li>
</ol>
</li>
<li>
<p>Use the notebook user interface to open the XGBoost quickstart notebook
at <code>fairing/examples/prediction/xgboost-high-level-apis.ipynb</code>.</p>
</li>
<li>
<p>Follow the instructions in the notebook to:</p>
<ol>
<li>Train an XGBoost model in a notebook,</li>
<li>Use Kubeflow Fairing to train an XGBoost model remotely on Kubeflow,</li>
<li>Use Kubeflow Fairing to train an XGBoost model remotely on AI Platform Training,</li>
<li>Use Kubeflow Fairing to deploy a trained model to Kubeflow, and</li>
<li>Call the deployed endpoint for predictions.</li>
</ol>
</li>
</ol>

</div>



    
	
  

    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-aa5f7153490b8c6be91ce5c4e17ef94f">5.6 - Tutorials</h1>
    <div class="lead">Try the samples and follow detailed tutorials for training and deploying with Kubeflow Fairing</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-fdf27b5ffdb640b8636dbdc6acc3436e">5.6.1 - Other Samples and Tutorials</h1>
    <div class="lead">Try the samples and follow detailed tutorials for training and deploying with Kubeflow Fairing</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<p>Read the following tutorials to learn more about using Kubeflow Fairing to train
and deploy on various environments such as on the Google Cloud Platform (GCP).</p>
<ul>
<li>Learn how to <a href="/docs/external-add-ons/fairing/gcp/tutorials/gcp-local-notebook/">train and deploy a model on GCP from a local
notebook</a>.</li>
<li>Learn how to <a href="/docs/external-add-ons/fairing/gcp/tutorials/gcp-kubeflow-notebook/">train and deploy a model on GCP from a notebook hosted on
Kubeflow</a>.</li>
<li>Learn how to <a href="/docs/external-add-ons/fairing/azure/">train and deploy a model on Azure from a notebook hosted on
Kubeflow</a>.</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-cca7dd273c2da2edff5d8a8155b942f3">5.7 - Reference</h1>
    <div class="lead">Reference docs for Kubeflow Fairing</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-c952cee2f080abc66b03de0b66a321a1">5.7.1 -  Kubeflow Fairing SDK Reference</h1>
    <div class="lead">Reference documentation for the Kubeflow Fairing SDK</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<p>See the <a href="https://kubeflow-fairing.readthedocs.io/en/latest/">generated reference docs for the Kubeflow Fairing
SDK</a> (hosted on
<em>Read the Docs</em>).</p>

</div>



    
	
  

    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-c2886c51a8da8d6861953f8f0cbdec4e">6 - Feature Store</h1>
    <div class="lead">Feature storage, management, validation, and serving</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-1151208ab46a907bcb9a8aa4c91e827f">6.1 - Introduction to Feast</h1>
    <div class="lead">Overview of Feast for feature storage, management, and serving</div>
	<div class="alert alert-warning" role="alert">
  <h4 class="alert-heading">Alpha</h4>
  This Kubeflow component has <b>alpha</b> status with limited support. See the
  <a href="/docs/started/support/#application-status">Kubeflow versioning policies</a>.
  The Kubeflow team is interested in your   
  <a href="https://github.com/feast-dev/feast/issues">feedback</a></h4> 
  about the usability of the feature.
</div>
<p>Use <a href="http://feast.dev/">Feast</a> for defining, managing, discovering, validating, and serving features to your models during training and inference.</p>
<p>This page introduces feature store concepts as well as Feast as a component of Kubeflow.</p>
<h2 id="introduction-to-feature-stores">Introduction to feature stores</h2>
<p>Feature stores are systems that help to address some of the key challenges that ML teams face when productionizing features</p>
<ul>
<li>
<p><strong>Feature sharing and reuse</strong>: Engineering features is one of the most time consuming activities in building an end-to-end ML system, yet many teams continue to develop features in silos. This leads to a high amount of re-development and duplication of work across teams and projects.</p>
</li>
<li>
<p><strong>Serving features at scale</strong>: Models need data that can come from a variety of sources, including event streams, data lakes, warehouses, or notebooks. ML teams need to be able to store and serve all these data sources to their models in a performant and reliable way. The challenge is scalably producing massive datasets of features for model training, and providing access to real-time feature data at low latency and high throughput in serving.</p>
</li>
<li>
<p><strong>Consistency between training and serving</strong>: The separation between data scientists and engineering teams often lead to the re-development of feature transformations when moving from training to online serving. Inconsistencies that arise due to discrepancies between training and serving implementations frequently leads to a drop in model performance in production.</p>
</li>
<li>
<p><strong>Point-in-time correctness</strong>:  General purpose data systems are not built with ML use cases in mind and by extension don&rsquo;t provide point-in-time correct lookups of feature data. Without a point-in-time correct view of data, models are trained on datasets that are not representative of what is found in production, leading to a drop in accuracy.</p>
</li>
<li>
<p><strong>Data quality and validation</strong>: Features are business critical inputs to ML systems. Teams need to be confident in the quality of data that is served in production and need to be able to react when there is any drift in the underlying data.</p>
</li>
</ul>
<h2 id="feast-as-a-feature-store">Feast as a feature store</h2>
<p><a href="http://feast.dev/">Feast</a> is an <a href="https://github.com/feast-dev/feast">open-source</a> feature store that helps teams operate ML systems at scale by allowing them to define, manage, validate, and serve features to models in production.</p>
<p>Feast provides the following functionality:</p>
<ul>
<li>
<p><strong>Load streaming and batch data</strong>: Feast is built to be able to ingest data from a variety of bounded or unbounded sources. Feast allows users to ingest data from streams, object stores, databases, or notebooks. Data that is ingested into Feast is persisted in both online store and historical stores, which in turn is used for the creation of training datasets and serving features to online systems.</p>
</li>
<li>
<p><strong>Standardized definitions</strong>: Feast becomes the single source of truth for all feature definitions and data within an organization. Teams are able to capture documentation, metadata, and metrics about features. This allows teams to communicate clearly about features, test feature data, and determine if a feature is both safe and relevant to their use cases.</p>
</li>
<li>
<p><strong>Historical serving</strong>: Features that are persisted in Feast can be retrieved through its feature serving APIs to produce training datasets. Feast is able to produce massive training datasets that are agnostics of the data source that was used to ingest the data originally. Feast is also able to ensure point-in-time correctness when joining these data sources, which in turn ensures the quality and consistency of features reaching models.</p>
</li>
<li>
<p><strong>Online serving</strong>: Feast exposes low latency serving APIs for all data that has been ingested into the system. This allows all production ML systems to use Feast as the primary data source when looking up real-time features.</p>
</li>
<li>
<p><strong>Consistency between training and serving</strong>: Feast provides a consistent view of feature data through the use of a unified ingestion layer, unified serving API and canonical feature references. By building ML systems on feature references, teams abstract away the underlying data infrastructure and make it possible to safely move models between training and serving without a drop in data consistency.</p>
</li>
<li>
<p><strong>Feature sharing and reuse</strong>: Feast provides a discovery and metadata API that allows teams to track, share, and reuse features across projects. Feast also decouples the process of creating features from the process of consumption, meaning teams that start new projects can begin by simply consuming features that already exist in the store, instead of starting from scratch.</p>
</li>
<li>
<p><strong>Statistics and validation</strong>: Feast allows for the generation of statistics based on features within the systems. Feast has compatibility with TFDV, meaning statistics that are generated by Feast can be validated using TFDV. Feast also allows teams to capture TFDV schemas as part of feature definitions, allowing domain experts to define data properties that can be used for validating these features in other production settings like training, ingestion, or serving.</p>
</li>
</ul>
<h2 id="next-steps">Next steps</h2>
<p>Please follow the <a href="/docs/external-add-ons/feature-store/getting-started/">Getting Started with Feast</a> guide to set up Feast and run walk through our tutorials.</p>
<h2 id="resources">Resources</h2>
<ul>
<li><a href="http://docs.feast.dev/">Feast: Documentation</a></li>
<li><a href="https://github.com/feast-dev/feast">Feast: Source Code</a></li>
<li><a href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-feast-an-open-source-feature-store-for-machine-learning">Google Cloud - Introducing Feast: An open source feature store for machine learning</a></li>
<li><a href="https://blog.gojekengineering.com/feast-bridging-ml-models-and-data-efd06b7d1644">Medium - Feast: Bridging ML Models and Data</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-107bc41b7c52562cbc5f3663aabf54fa">6.2 - Getting started with Feast</h1>
    <div class="lead">How to set up Feast and walk through examples</div>
	<p>This guide provides the necessary resources to install <a href="http://feast.dev/">Feast</a> alongside Kubeflow, describes the usage of Feast with Kubeflow components, and provides examples that users can follow to test their setup.</p>
<p>For an overview of Feast, please read <a href="/docs/external-add-ons/feature-store/overview/">Introduction to Feast</a>.</p>
<h2 id="installing-feast-with-kubeflow">Installing Feast with Kubeflow</h2>
<p><strong>Overview</strong></p>
<ul>
<li>This guide assumes that you have a running Kubeflow cluster already. If you don&rsquo;t have Kubeflow installed, then head on over to the <a href="/docs/started/getting-started/">Kubeflow installation guide</a>.</li>
<li>This guide also assumes that you have a running online feature store that Feast supports (Redis, Datastore, DynamoDB).</li>
<li>The latest version of Feast does not need to be installed into Kubernetes. It is possible to run Feast completely from CI or as a client library (during training or inference)</li>
<li>Feast requires a bucket (S3, GCS, Minio, etc) to maintain a feature registry, requires an online feature store for serving feature values, and it requires a scheduler to keep the online store up to date.</li>
</ul>
<p><strong>Installation</strong></p>
<p>To use Feast with Kubeflow, please follow the following steps</p>
<ul>
<li><a href="https://docs.feast.dev/how-to-guides/feast-gcp-aws/install-feast">Install Feast</a> into your development environment, as well as any environment where you want to register feature views or read features from the feature store.</li>
<li><a href="https://docs.feast.dev/how-to-guides/feast-gcp-aws/create-a-feature-repository">Create a feature repository</a> to store your feature views and entities. Make sure to configure your feature_store.yaml to point to your online store. Pleas see the online store <a href="https://docs.feast.dev/reference/online-stores">configuration reference</a> here for more details.</li>
<li><a href="https://docs.feast.dev/how-to-guides/feast-gcp-aws/deploy-a-feature-store">Deploy your feature store</a>. This step configures your online store and sets up your feature registry.</li>
<li><a href="https://docs.feast.dev/how-to-guides/feast-gcp-aws/build-a-training-dataset">Build a training dataset</a>. This step is typically executed from a Kubeflow Pipeline from which you&rsquo;d train a model.</li>
<li><a href="https://docs.feast.dev/how-to-guides/feast-gcp-aws/load-data-into-the-online-store">Load features into the online store</a>. This step can also be executed from a Kubernetes cron job.</li>
<li><a href="https://docs.feast.dev/how-to-guides/feast-gcp-aws/read-features-from-the-online-store">Read features from the online store</a>. This step is typically executed from your model serving service, right before calling your model for a prediction.</li>
</ul>
<p><strong>Advanced</strong></p>
<ul>
<li>Please see <a href="https://docs.feast.dev/how-to-guides/running-feast-in-production">this guide</a> which provides best practices for running Feast in a production context.</li>
<li>Please see <a href="https://docs.google.com/document/u/1/d/1AOsr_baczuARjCpmZgVd8mCqTF4AZ49OEyU4Cn-uTT0/edit">this guide</a> for upgrading from Feast 0.9 (Spark-based) to the latest Feast (0.12+).</li>
</ul>
<h2 id="accessing-feast-from-kubeflow">Accessing Feast from Kubeflow</h2>
<p>Once Feast is installed within the same Kubernetes cluster as Kubeflow, users can access its APIs directly without any additional steps.</p>
<p>Feast APIs can roughly be grouped into the following sections:</p>
<ul>
<li>
<p><strong>Feature definition and management</strong>: Feast provides both a <a href="https://docs.feast.dev/getting-started/quickstart">Python SDK</a> and <a href="https://docs.feast.dev/reference/feast-cli-commands">CLI</a> for interacting with Feast Core. Feast Core allows users to define and register features and entities and their associated metadata and schemas. The Python SDK is typically used from within a Jupyter notebook by end users to administer Feast, but ML teams may opt to version control feature specifications in order to follow a GitOps based approach.</p>
</li>
<li>
<p><strong>Model training</strong>: The Feast Python SDK can be used to trigger the <a href="https://docs.feast.dev/how-to-guides/feast-gcp-aws/build-a-training-dataset">creation of training datasets</a>. The most natural place to use this SDK is to create a training dataset as part of a <a href="/docs/components/pipelines/introduction">Kubeflow Pipeline</a> prior to model training.</p>
</li>
<li>
<p><strong>Model serving</strong>: The Feast Python SDK can also be used for <a href="https://docs.feast.dev/how-to-guides/feast-gcp-aws/read-features-from-the-online-store">online feature retrieval</a>. This client is used to retrieve feature values for inference with <a href="/docs/components/pipelines/introduction">Model Serving</a> systems like KFServing, TFX, or Seldon.</p>
</li>
</ul>
<h2 id="examples">Examples</h2>
<p>Please see our <a href="https://docs.feast.dev/tutorials/tutorials-overview">tutorials</a> section for a full list of examples</p>
<ul>
<li><a href="https://docs.feast.dev/tutorials/driver-ranking-with-feast">Driver ranking with Feast</a></li>
<li><a href="https://docs.feast.dev/tutorials/fraud-detection">Fraud detection on GCP</a></li>
<li><a href="https://docs.feast.dev/tutorials/real-time-credit-scoring-on-aws">Real-time credit scoring on AWS</a></li>
</ul>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>
<p>For more details on Feast concepts please see the <a href="https://docs.feast.dev/">Feast documentation</a></p>
</li>
<li>
<p>Please see our <a href="https://github.com/feast-dev/feast/blob/master/CHANGELOG.md">changelog</a> and <a href="https://docs.feast.dev/roadmap">roadmap</a> for new or upcoming functionality.</p>
</li>
<li>
<p>Please use <a href="https://github.com/feast-dev/feast/issues">GitHub issues</a> for any feedback, issues, or feature requests.</p>
</li>
<li>
<p>If you would like to get involved with Feast, come and visit us in <a href="https://slack.feast.dev">#Feast</a> or join our <a href="https://docs.feast.dev/community">community calls</a>, <a href="https://docs.feast.dev/community">mailing list</a>, or have a look at our <a href="https://docs.feast.dev/project/contributing">contribution process</a></p>
</li>
</ul>

</div>



    
	
  

    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-4a81d5e91c644e4da361430d155bd3e3">7 - Tools for Serving</h1>
    <div class="lead">Serving of ML models in Kubeflow</div>
	
</div>



    
      
  
  
  
  

  
  

  
    
    
	
    

<div class="td-content" style="">
    
	<h1 id="pg-b846ae272222f6de23d4042d43dfeea9">7.1 - Overview</h1>
    <div class="lead">Model serving overview</div>
	<p>Kubeflow supports two model serving systems that allow multi-framework model
serving: <em>KFServing</em> and <em>Seldon Core</em>. Alternatively, you can use a
standalone model serving system. This page gives an overview of the options, so
that you can choose the framework that best supports your model serving
requirements.</p>
<h2 id="multi-framework-serving-with-kfserving-or-seldon-core">Multi-framework serving with KFServing or Seldon Core</h2>
<p>KFServing and Seldon Core are both open source systems that allow
multi-framework model serving. The following table compares
KFServing and Seldon Core. A check mark (<strong>✓</strong>) indicates that the system
(KFServing or Seldon Core) supports the feature specified in that row.</p>
<div class="table-responsive">
  <table class="table table-bordered">
    <thead class="thead-light">
      <tr>
        <th>Feature</th>
        <th>Sub-feature</th>
        <th>KFServing</th>
        <th>Seldon Core</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Framework</td>
        <td>TensorFlow</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/tensorflow">sample</a></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/servers/tensorflow.html">docs</a></td>
      </tr>
      <tr>
        <td></td>
        <td>XGBoost</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/xgboost">sample</a></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/servers/xgboost.html">docs</a></td>
      </tr>
      <tr>
        <td></td>
        <td>scikit-learn</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/sklearn/v2">sample</a></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/servers/sklearn.html">docs</a></td>
      </tr>
      <tr>
        <td></td>
        <td>NVIDIA Triton Inference Server</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/triton">sample</a></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/nvidia_mnist.html">docs</a></td>
      </tr>
      <tr>
        <td></td>
        <td>ONNX</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1alpha2/onnx">sample</a></td>
        <td></td>
      </tr>
      <tr>
        <td></td>
        <td>PyTorch</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/torchserve">sample</a></td>
        <td><b>&check;</b></td>
      </tr>
      <tr>
        <td>Graph</td>
        <td>Transformers</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/blob/master/docs/samples/v1beta1/transformer/torchserve_image_transformer">sample</a></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/transformer_spam_model.html">docs</a></td>
      </tr>
      <tr>
        <td></td>
        <td>Combiners</td>
        <td>Roadmap</td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/openvino_ensemble.html">sample</a></td>
      </tr>
      <tr>
        <td></td>
        <td>Routers including <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">MAB</a></td>
        <td>Roadmap</td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/analytics/routers.html">docs</a></td>
      </tr>
      <tr>
        <td>Analytics</td>
        <td>Explanations</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/explanation/alibi">sample</a></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/analytics/explainers.html">docs</a></td>
      </tr>
      <tr>
        <td>Scaling</td>
        <td>Knative</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/autoscaling">sample</a></td>
        <td></td>
      </tr>
      <tr>
        <td></td>
        <td>GPU AutoScaling</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/autoscaling">sample</a></td>
        <td></td>
      </tr>
      <tr>
        <td></td>
        <td>HPA</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/blob/master/test/benchmark/README.md">HPA vs KPA</a></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/graph/scaling.html#autoscaling-seldon-deployments">docs</a></td>
      </tr>
      <tr>
        <td>Custom</td>
        <td>Container</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1alpha2/custom">sample</a></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/language_wrappers.html">docs</a></td>
      </tr>
      <tr>
        <td></td>
        <td>Language Wrappers</td>
        <td></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/python/index.html">Python</a>, <a href="https://docs.seldon.io/projects/seldon-core/en/latest/java/README.html">Java</a>, <a href="https://docs.seldon.io/projects/seldon-core/en/latest/R/README.html">R</a></td>
      </tr>
      <tr>
        <td></td>
        <td>Multi-Container</td>
        <td></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/graph/inference-graph.html">docs</a></td>
      </tr>
      <tr>
        <td>Rollout</td>
        <td>Canary</td>
        <td><b>&check;</b> <a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/rollout">sample</a></td>
        <td><b>&check;</b> <a href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/istio_canary.html">docs</a></td>
      </tr>
      <tr>
        <td></td>
        <td>Shadow</td>
        <td></td>
        <td><b>&check;</b></td>
      </tr>
      <tr>
        <td>Istio</td>
        <td></td>
        <td><b>&check;</b></td>
        <td><b>&check;</b></td>
      </tr>
    </tbody>
  </table>
</div>
<p>Notes:</p>
<ul>
<li>KFServing and Seldon Core share some technical features, including
explainability (using <a href="https://github.com/SeldonIO/alibi">Seldon Alibi
Explain</a>) and payload logging, as well
as other areas.</li>
<li>A commercial product,
<a href="https://www.seldon.io/tech/products/deploy/">Seldon Deploy</a>, supports both
KFServing and Seldon in production.</li>
<li>KFServing is part of the Kubeflow project ecosystem. Seldon Core is an
external project supported within Kubeflow.</li>
</ul>
<p>Further information:</p>
<ul>
<li>KFServing:
<ul>
<li><a href="/docs/components/kfserving/">Kubeflow documentation</a></li>
<li><a href="https://github.com/kubeflow/kfserving">GitHub repository</a></li>
<li><a href="/docs/about/community/">Kubeflow Community</a></li>
</ul>
</li>
<li>Seldon Core
<ul>
<li><a href="/docs/external-add-ons/serving/seldon/">Kubeflow documentation</a></li>
<li><a href="https://docs.seldon.io/projects/seldon-core/en/latest/">Seldon Core documentation</a></li>
<li><a href="https://github.com/SeldonIO/seldon-core">GitHub repository</a></li>
<li><a href="https://docs.seldon.io/projects/seldon-core/en/latest/developer/community.html">Community</a></li>
</ul>
</li>
</ul>
<h2 id="tensorflow-serving">TensorFlow Serving</h2>
<p>For TensorFlow models you can use TensorFlow Serving for
<a href="/docs/external-add-ons/serving/tfserving_new">real-time prediction</a>.
However, if you plan to use multiple frameworks, you should consider KFServing
or Seldon Core as described above.</p>
<h2 id="nvidia-triton-inference-server">NVIDIA Triton Inference Server</h2>
<p>NVIDIA Triton Inference Server is a REST and GRPC service for deep-learning
inferencing of TensorRT, TensorFlow, Pytorch, ONNX and Caffe2 models. The server is
optimized to deploy machine learning algorithms on both GPUs and
CPUs at scale. Triton Inference Server was previously known as TensorRT Inference Server.</p>
<p>You can use NVIDIA Triton Inference Server as a
<a href="/docs/external-add-ons/serving/tritoninferenceserver">standalone system</a>,
but you should consider KFServing as described above. KFServing includes support
for NVIDIA Triton Inference Server.</p>
<h2 id="bentoml">BentoML</h2>
<p><a href="https://bentoml.org">BentoML</a> is an open-source platform for high-performance ML model
serving. It makes building production API endpoint for your ML model easy and supports
all major machine learning training frameworks, including Tensorflow, Keras, PyTorch,
XGBoost, scikit-learn and etc.</p>
<p>BentoML comes with a high-performance API model server with adaptive micro-batching
support, which achieves the advantage of batch processing in online serving. It also
provides model management and model deployment functionality, giving ML teams an
end-to-end model serving workflow, with DevOps best practices baked in.</p>
<ul>
<li><a href="/docs/external-add-ons/serving/bentoml">BentoML guide for Kubeflow</a></li>
<li><a href="https://github.com/bentoml/BentoML">BentoML GitHub repository</a></li>
<li><a href="https://docs.bentoml.org">BentoML documentation</a></li>
<li><a href="https://docs.bentoml.org/en/latest/quickstart.html">Quick start guide</a></li>
<li><a href="https://join.slack.com/t/bentoml/shared_invite/enQtNjcyMTY3MjE4NTgzLTU3ZDc1MWM5MzQxMWQxMzJiNTc1MTJmMzYzMTYwMjQ0OGEwNDFmZDkzYWQxNzgxYWNhNjAxZjk4MzI4OGY1Yjg">Community</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-3108d9ddd997e24e5b8436c63b4914d4">7.2 - Seldon Core Serving</h1>
    <div class="lead">Model serving using Seldon</div>
	<div class="alert alert-primary" role="alert">
This Kubeflow component has <b>stable</b> status. See the
<a href="/docs/started/support/#application-status">Kubeflow versioning policies</a>.
</div>
<p>Seldon Core comes installed with Kubeflow. The <a href="https://docs.seldon.io/projects/seldon-core/en/latest/">Seldon Core documentation site</a> provides full documentation for running Seldon Core inference.</p>
<p>Seldon presently requires a Kubernetes cluster version &gt;= 1.12 and &lt;= 1.17.</p>
<p>If you have a saved model in a PersistentVolume (PV), Google Cloud Storage bucket or Amazon S3 Storage you can use one of the <a href="https://docs.seldon.io/projects/seldon-core/en/latest/servers/overview.html">prepackaged model servers provided by Seldon Core</a>.</p>
<p>Seldon Core also provides <a href="https://docs.seldon.io/projects/seldon-core/en/latest/wrappers/language_wrappers.html">language specific model wrappers</a> to wrap your inference code for it to run in Seldon Core.</p>
<h2 id="kubeflow-specifics">Kubeflow specifics</h2>
<ul>
<li>A namespace label set as <code>serving.kubeflow.org/inferenceservice=enabled</code></li>
</ul>
<p>The following example applies the label <code>seldon</code> to the namespace for serving:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl create namespace seldon
</span></span><span class="line"><span class="cl">kubectl label namespace seldon serving.kubeflow.org/inferenceservice=enabled
</span></span></code></pre></div><h3 id="istio-gateway">Istio Gateway</h3>
<p>By default Seldon will use the <code>kubeflow-gateway</code> in the kubeflow namespace. If you wish to change to a separate Gateway you would need to update the Kubeflow Seldon kustomize by changing the environment variable ISTIO_GATEWAY in the seldon-manager Deployment.</p>
<h4 id="kubeflow-100-101-102">Kubeflow 1.0.0, 1.0.1, 1.0.2</h4>
<p>For the above versions you would need to create an Istio Gateway in the namespace you want to run inference called kubeflow-gateway. For example, for a namespace <code>seldon</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cat &lt;&lt;EOF | kubectl create -n seldon -f -
</span></span><span class="line"><span class="cl">apiVersion: networking.istio.io/v1alpha3
</span></span><span class="line"><span class="cl">kind: Gateway
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: kubeflow-gateway
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  selector:
</span></span><span class="line"><span class="cl">    istio: ingressgateway
</span></span><span class="line"><span class="cl">  servers:
</span></span><span class="line"><span class="cl">  - hosts:
</span></span><span class="line"><span class="cl">    - &#39;*&#39;
</span></span><span class="line"><span class="cl">    port:
</span></span><span class="line"><span class="cl">      name: http
</span></span><span class="line"><span class="cl">      number: 80
</span></span><span class="line"><span class="cl">      protocol: HTTP
</span></span><span class="line"><span class="cl">EOF
</span></span></code></pre></div><h2 id="simple-example">Simple example</h2>
<p>Create a new namespace:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl create ns seldon
</span></span></code></pre></div><p>Label that namespace so you can run inference tasks in it:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl label namespace seldon serving.kubeflow.org/inferenceservice=enabled
</span></span></code></pre></div><p>For Kubeflow version 1.0.0, 1.0.1 and 1.0.2 create an Istio Gateway as shown above.</p>
<p>Create an example <code>SeldonDeployment</code> with a dummy model:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cat &lt;&lt;EOF | kubectl create -n seldon -f -
</span></span><span class="line"><span class="cl">apiVersion: machinelearning.seldon.io/v1
</span></span><span class="line"><span class="cl">kind: SeldonDeployment
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: seldon-model
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  name: test-deployment
</span></span><span class="line"><span class="cl">  predictors:
</span></span><span class="line"><span class="cl">  - componentSpecs:
</span></span><span class="line"><span class="cl">    - spec:
</span></span><span class="line"><span class="cl">        containers:
</span></span><span class="line"><span class="cl">        - image: seldonio/mock_classifier_rest:1.3
</span></span><span class="line"><span class="cl">          name: classifier
</span></span><span class="line"><span class="cl">    graph:
</span></span><span class="line"><span class="cl">      children: []
</span></span><span class="line"><span class="cl">      endpoint:
</span></span><span class="line"><span class="cl">        type: REST
</span></span><span class="line"><span class="cl">      name: classifier
</span></span><span class="line"><span class="cl">      type: MODEL
</span></span><span class="line"><span class="cl">    name: example
</span></span><span class="line"><span class="cl">    replicas: 1
</span></span><span class="line"><span class="cl">EOF
</span></span></code></pre></div><p>Wait for state to become available:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get sdep seldon-model -n seldon -o jsonpath=&#39;{.status.state}\n&#39;
</span></span></code></pre></div><p>Port forward to the Istio gateway:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl port-forward $(kubectl get pods -l istio=ingressgateway -n istio-system -o jsonpath=&#39;{.items[0].metadata.name}&#39;) -n istio-system 8004:80
</span></span></code></pre></div><p>Send a prediction request:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">curl -s -d &#39;{&#34;data&#34;: {&#34;ndarray&#34;:[[1.0, 2.0, 5.0]]}}&#39;    -X POST http://localhost:8004/seldon/seldon/seldon-model/api/v1.0/predictions    -H &#34;Content-Type: application/json&#34;
</span></span></code></pre></div><p>You should see a response:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">{
</span></span><span class="line"><span class="cl">  &#34;meta&#34;: {
</span></span><span class="line"><span class="cl">    &#34;puid&#34;: &#34;i2e1i8nq3lnttadd5i14gtu11j&#34;,
</span></span><span class="line"><span class="cl">    &#34;tags&#34;: {
</span></span><span class="line"><span class="cl">    },
</span></span><span class="line"><span class="cl">    &#34;routing&#34;: {
</span></span><span class="line"><span class="cl">    },
</span></span><span class="line"><span class="cl">    &#34;requestPath&#34;: {
</span></span><span class="line"><span class="cl">      &#34;classifier&#34;: &#34;seldonio/mock_classifier_rest:1.3&#34;
</span></span><span class="line"><span class="cl">    },
</span></span><span class="line"><span class="cl">    &#34;metrics&#34;: []
</span></span><span class="line"><span class="cl">  },
</span></span><span class="line"><span class="cl">  &#34;data&#34;: {
</span></span><span class="line"><span class="cl">    &#34;names&#34;: [&#34;proba&#34;],
</span></span><span class="line"><span class="cl">    &#34;ndarray&#34;: [[0.43782349911420193]]
</span></span><span class="line"><span class="cl">  }
</span></span><span class="line"><span class="cl">}
</span></span></code></pre></div><h2 id="further-documentation">Further documentation</h2>
<ul>
<li><a href="https://docs.seldon.io/projects/seldon-core/en/latest/">Seldon Core documentation</a></li>
<li><a href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/notebooks.html">Example notebooks</a></li>
<li><a href="https://github.com/SeldonIO/seldon-core">GitHub repository</a></li>
<li><a href="https://docs.seldon.io/projects/seldon-core/en/latest/developer/community.html">Community</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-bd9c19194ad8f66e71fc6b468874ed49">7.3 - BentoML</h1>
    <div class="lead">Model serving with BentoML</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<p>This guide demonstrates how to serve a scikit-learn based iris classifier model with
BentoML on a Kubernetes cluster. The same deployment steps are also applicable for models
trained with other machine learning frameworks, see more BentoML examples
<a href="https://docs.bentoml.org/en/latest/examples.html">here</a>.</p>
<p><a href="https://bentoml.org">BentoML</a> is an open-source platform for high-performance ML model
serving. It makes building production API endpoint for your ML model easy and supports
all major machine learning training frameworks, including Tensorflow, Keras, PyTorch,
XGBoost, scikit-learn and etc.</p>
<p>BentoML comes with a high-performance API model server with adaptive micro-batching
support, which achieves the advantage of batch processing in online serving. It also
provides model management and model deployment functionality, giving ML teams an
end-to-end model serving workflow, with DevOps best practices baked in.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>Before starting this tutorial, make sure you have the following:</p>
<ul>
<li>a Kubernetes cluster and <code>kubectl</code> installed on your local machine.
<ul>
<li><code>kubectl</code> install instruction: <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a></li>
</ul>
</li>
<li>Docker and Docker Hub installed and configured in your local machine.
<ul>
<li>Docker install instruction: <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a></li>
</ul>
</li>
<li>Python 3.6 or above and required PyPi packages: <code>bentoml</code>, <code>scikit-learn</code>
<ul>
<li><code>pip install bentoml scikit-learn</code></li>
</ul>
</li>
</ul>
<h2 id="build-an-iris-classifier-model-server-with-bentoml">Build an iris classifier model server with BentoML</h2>
<p>The following code defines a BentoML prediction service that requires a <code>scikit-learn</code> model, and
asks BentoML to figure out the required PyPI packages automatically. It also defines an
API, which is the entry point for accessing this prediction service. And the API is
expecting a <code>pandas.DataFrame</code> object as its input data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># iris_classifier.py</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">bentoml</span> <span class="kn">import</span> <span class="n">env</span><span class="p">,</span> <span class="n">artifacts</span><span class="p">,</span> <span class="n">api</span><span class="p">,</span> <span class="n">BentoService</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">bentoml.handlers</span> <span class="kn">import</span> <span class="n">DataframeHandler</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">bentoml.artifact</span> <span class="kn">import</span> <span class="n">SklearnModelArtifact</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@env</span><span class="p">(</span><span class="n">auto_pip_dependencies</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nd">@artifacts</span><span class="p">([</span><span class="n">SklearnModelArtifact</span><span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">IrisClassifier</span><span class="p">(</span><span class="n">BentoService</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@api</span><span class="p">(</span><span class="n">DataframeHandler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</span></span></code></pre></div><p>The following code trains a classifier model and serves it with the IrisClassifier
defined above:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># main.py</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">iris_classifier</span> <span class="kn">import</span> <span class="n">IrisClassifier</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&#34;__main__&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Load training data</span>
</span></span><span class="line"><span class="cl">    <span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Model Training</span>
</span></span><span class="line"><span class="cl">    <span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Create a iris classifier service instance</span>
</span></span><span class="line"><span class="cl">    <span class="n">iris_classifier_service</span> <span class="o">=</span> <span class="n">IrisClassifier</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Pack the newly trained model artifact</span>
</span></span><span class="line"><span class="cl">    <span class="n">iris_classifier_service</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Save the prediction service to disk for model serving</span>
</span></span><span class="line"><span class="cl">    <span class="n">saved_path</span> <span class="o">=</span> <span class="n">iris_classifier_service</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</span></span></code></pre></div><p>The sample code above can be found in the BentoML repository, run them directly with the
following command:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">git clone git@github.com:bentoml/BentoML.git
</span></span><span class="line"><span class="cl">python ./bentoml/guides/quick-start/main.py
</span></span></code></pre></div><p>After saving the BentoService instance, you can now start a REST API server with the
model trained and test the API server locally:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="c1"># Start BentoML API server:</span>
</span></span><span class="line"><span class="cl">bentoml serve IrisClassifier:latest
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Send test request</span>
</span></span><span class="line"><span class="cl">curl -i <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --header <span class="s2">&#34;Content-Type: application/json&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --request POST <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --data <span class="s1">&#39;[[5.1, 3.5, 1.4, 0.2]]&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  localhost:5000/predict
</span></span></code></pre></div><p>BentoML provides a convenient way of containerizing the model API server with Docker. To
create a docker container image for the sample model above:</p>
<ol>
<li>Find the file directory of the SavedBundle with <code>bentoml get</code> command, which is
directory structured as a docker build context.</li>
<li>Running docker build with this directory produces a docker image containing the model
API server</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nv">saved_path</span><span class="o">=</span><span class="k">$(</span>bentoml get IrisClassifier:latest -q <span class="p">|</span> jq -r <span class="s2">&#34;.uri.uri&#34;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Replace `{docker_username} with your Docker Hub username</span>
</span></span><span class="line"><span class="cl">docker build -t <span class="o">{</span>docker_username<span class="o">}</span>/iris-classifier <span class="nv">$saved_path</span>
</span></span><span class="line"><span class="cl">docker push <span class="o">{</span>docker_username<span class="o">}</span>/iris-classifier
</span></span></code></pre></div><h2 id="deploy-model-server-to-kubernetes">Deploy model server to Kubernetes</h2>
<p>The following is an example YAML file for specifying the resources required to run and
expose a BentoML model server in a Kubernetes cluster. Replace <code>{docker_username}</code>
with your Docker Hub username and save it to <code>iris-classifier.yaml</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">predict</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">5000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">5000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">LoadBalancer</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span>{<span class="l">docker_username}/iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">5000</span><span class="w">
</span></span></span></code></pre></div><p>Use <code>kubectl</code> CLI to deploy the model API server to the Kubernetes cluster</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl apply -f iris-classifier.yaml
</span></span></code></pre></div><h2 id="send-prediction-request">Send prediction request</h2>
<p>Use <code>kubectl describe</code> command to get the <code>NODE_PORT</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl describe svc iris-classifier --namespace kubeflow
</span></span></code></pre></div><p>And then send the request:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">curl -i <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --header <span class="s2">&#34;Content-Type: application/json&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --request POST <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --data <span class="s1">&#39;[[5.1, 3.5, 1.4, 0.2]]&#39;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  http://EXTERNAL_IP:NODE_PORT/predict
</span></span></code></pre></div><h2 id="monitor-metrics-with-prometheus">Monitor metrics with Prometheus</h2>
<h3 id="prerequisites-1">Prerequisites</h3>
<p>Before starting this section, make sure you have the following:</p>
<ul>
<li>Prometheus installed in the cluster
<ul>
<li><a href="https://prometheus.io/docs/introduction/overview/">Prometheus documentation</a></li>
<li><a href="https://github.com/helm/charts/tree/master/stable/prometheus">Installation instruction with Helm chart</a></li>
</ul>
</li>
</ul>
<p>BentoML API server provides Prometheus support out of the box. It comes with a &ldquo;/metrics&rdquo;
endpoint which includes the essential metrics for model serving and the ability to
create and customize new metrics base on needs.</p>
<p>To enable Prometheus monitoring on the deployed model API server, update the YAML file
with Prometheus related annotations. Change the deployment spec as the following, and replace
<code>{docker_username}</code> with your Docker Hub username:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">prometheus.io/scrape</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">prometheus.io/port</span><span class="p">:</span><span class="w"> </span><span class="m">5000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">image</span><span class="p">:</span><span class="w"> </span>{<span class="l">docker_username}/iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">iris-classifier</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">5000</span><span class="w">
</span></span></span></code></pre></div><p>Apply the change with <code>kubectl</code> CLI.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl apply -f iris-classifier.yaml
</span></span></code></pre></div><h2 id="remove-deployment">Remove deployment</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">kubectl delete -f iris-classifier.yaml
</span></span></code></pre></div><h2 id="additional-resources">Additional resources</h2>
<ul>
<li><a href="https://github.com/bentoml/BentoML">GitHub repository</a></li>
<li><a href="https://docs.bentoml.org">BentoML documentation</a></li>
<li><a href="https://docs.bentoml.org/en/latest/quickstart.html">Quick start guide</a></li>
<li><a href="https://join.slack.com/t/bentoml/shared_invite/enQtNjcyMTY3MjE4NTgzLTU3ZDc1MWM5MzQxMWQxMzJiNTc1MTJmMzYzMTYwMjQ0OGEwNDFmZDkzYWQxNzgxYWNhNjAxZjk4MzI4OGY1Yjg">Community</a></li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-145f96a34e4a8b370a6013fbd8a1b5e1">7.4 - MLRun Serving Pipelines</h1>
    <div class="lead">Real-time Serving Pipelines and Model Monitoring with MLRun and Nuclio</div>
	<p><a href="https://docs.mlrun.org/en/latest/serving/build-graph-model-serving.html">MLRun serving</a> graphs allow you to build, test, deploy, and monitor real-time data processing and advanced model serving pipelines with minimal effort.
MLRun Serving is built on top of the real-time serverless framework <a href="https://github.com/nuclio/nuclio">Nuclio</a>, and is API compatible with KFServing v2. MLRun’s serving functions can be deployed automatically using CLI, SDK, or Kubeflow Pipelines (KFP) operations.</p>
<p>With MLRun Serving you compose a graph of steps (composed of pre-defined graph blocks or native python classes/functions).
A graph can have data processing steps, model ensembles, model servers, post-processing, etc. (<a href="https://docs.mlrun.org/en/latest/serving/graph-example.html">see example</a>).
MLRun Serving supports complex and distributed graphs (<a href="https://docs.mlrun.org/en/latest/serving/distributed-graph.html">see example</a>)
which may involve streaming, data/document/image processing, NLP, model monitoring, etc.</p>
<p>MLRun is natively integrated with Kubeflow and Kubeflow Pipelines, MLRun function objects can be deployed, tested and executed through Kubeflow (see example below).</p>
<h3 id="accelerate-performance-and-time-to-production">Accelerate performance and time to production</h3>
<p>MLRun&rsquo;s underline serverless engine (<a href="https://nuclio.io/">Nuclio</a>) uses a high-performance parallel processing engine that maximizes the utilization of CPUs and GPUs.</p>
<p>MLRun Serving provides native model monitoring, including auto drift detection and custom metric, models can be tracked via the Grafana plug-in or in MLRun UI (<a href="https://docs.mlrun.org/en/latest/model_monitoring/index.html">see details</a>).</p>
<p>The serving pipelines can be tested locally or in a notebook, and deployed into multiple managed serverless functions in a single command (<code>deploy()</code>). Such functions are fully managed, with logging, monitoring, auto-scaling, security, etc., which eliminate the deployment overhead, improve performance and scalability, and accelerate time to production.</p>
<p>MLRun serving is natively integrated with MLRun Online Feature Store, which can be used to generate and/or enrich real-time feature vectors as well as store back production features for later analysis and re-training.</p>
<p>MLRun allows developers to focus on code and deploy faster by supporting:</p>
<ul>
<li>13 protocols and invocation methods (HTTP, Cron, Kafka, Kinesis, etc&hellip;),</li>
<li>Dynamic auto-scaling for http and streaming,</li>
<li>Optimal resource management for CPUs and GPUs,</li>
<li>Full life cycle&ndash;including auto-generation of micro-services, APIs, load-balancing, logging, monitoring, and configuration management.</li>
</ul>
<h3 id="examples">Examples</h3>
<p>Loading library serving function, adding models, testing the pipeline, deploy to the cluster, and test the live endpoint:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">mlrun</span>  
</span></span><span class="line"><span class="cl"><span class="c1"># load the sklearn model serving function and add models to it  </span>
</span></span><span class="line"><span class="cl"><span class="n">fn</span> <span class="o">=</span> <span class="n">mlrun</span><span class="o">.</span><span class="n">import_function</span><span class="p">(</span><span class="s1">&#39;hub://v2_model_server&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">fn</span><span class="o">.</span><span class="n">add_model</span><span class="p">(</span><span class="s2">&#34;model1&#34;</span><span class="p">,</span> <span class="n">model_path</span><span class="o">=</span><span class="p">{</span><span class="n">model1</span><span class="o">-</span><span class="n">url</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">fn</span><span class="o">.</span><span class="n">add_model</span><span class="p">(</span><span class="s2">&#34;model2&#34;</span><span class="p">,</span> <span class="n">model_path</span><span class="o">=</span><span class="p">{</span><span class="n">model2</span><span class="o">-</span><span class="n">url</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># test the serving pipeline using the graph simulator</span>
</span></span><span class="line"><span class="cl"><span class="n">server</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">to_mock_server</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="s2">&#34;/v2/models/model1/infer&#34;</span><span class="p">,</span> <span class="p">{</span><span class="s2">&#34;inputs&#34;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># deploy the function to the cluster</span>
</span></span><span class="line"><span class="cl"><span class="n">fn</span><span class="o">.</span><span class="n">deploy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># test the live model endpoint</span>
</span></span><span class="line"><span class="cl"><span class="n">fn</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s1">&#39;/v2/models/model1/infer&#39;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;inputs&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">]})</span>
</span></span></code></pre></div><p><strong>Building your own serving class:</strong></p>
<p>MLRun Model Serving classes look and behave like KFServing classes, but are faster, support advanced graphs and capabilities, and eliminate all the deployment overhead.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">cloudpickle</span> <span class="kn">import</span> <span class="n">load</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">mlrun</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ClassifierModel</span><span class="p">(</span><span class="n">mlrun</span><span class="o">.</span><span class="n">serving</span><span class="o">.</span><span class="n">V2ModelServer</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;load and initialize the model and/or other elements&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">model_file</span><span class="p">,</span> <span class="n">extra_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_model</span><span class="p">(</span><span class="s1">&#39;.pkl&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">model_file</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">body</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Generate model predictions from sample&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">feats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">body</span><span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">result</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">feats</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</span></span></code></pre></div><p><strong>Deploy and Test Model Serving using Kubeflow Pipelines:</strong></p>
<p>The following Kubeflow pipeline uses MLRun Serverless functions from the MLRun marketplace and
execute a simple training, serving deployment, and serving testing Kubefow pipeline.
(see the <a href="https://github.com/mlrun/demos/blob/0.6.x/scikit-learn-pipeline/sklearn-project.ipynb">full example</a>)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@dsl</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&#34;Demo pipeline&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">kfpipeline</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">    <span class="c1"># train with hyper-paremeters </span>
</span></span><span class="line"><span class="cl">    <span class="n">train</span> <span class="o">=</span> <span class="n">mlrun</span><span class="o">.</span><span class="n">import_function</span><span class="p">(</span><span class="s1">&#39;hub://sklearn_classifier&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">as_step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">name</span><span class="o">=</span><span class="s2">&#34;train&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;sample&#34;</span>          <span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;label_column&#34;</span>    <span class="p">:</span> <span class="n">LABELS</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;test_size&#34;</span>       <span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;model_pkg_class&#39;</span><span class="p">:</span> <span class="s2">&#34;sklearn.ensemble.RandomForestClassifier&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;dataset&#34;</span>         <span class="p">:</span> <span class="n">DATASET</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="s1">&#39;test_set&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># deploy our model as a serverless function, we can pass a list of models to serve </span>
</span></span><span class="line"><span class="cl">    <span class="n">deploy</span> <span class="o">=</span> <span class="n">mlrun</span><span class="o">.</span><span class="n">import_function</span><span class="p">(</span><span class="s1">&#39;hub://v2_model_server&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">deploy_step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">models</span><span class="o">=</span><span class="p">[{</span><span class="s2">&#34;key&#34;</span><span class="p">:</span> <span class="s2">&#34;mymodel:v1&#34;</span><span class="p">,</span> <span class="s2">&#34;model_path&#34;</span><span class="p">:</span> <span class="n">train</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">]}])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># test out new model server (via REST API calls)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tester</span> <span class="o">=</span> <span class="n">mlrun</span><span class="o">.</span><span class="n">import_function</span><span class="p">(</span><span class="s1">&#39;hub://v2_model_tester&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">as_step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;model-tester&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;addr&#39;</span><span class="p">:</span> <span class="n">deploy</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;endpoint&#39;</span><span class="p">],</span> <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="s2">&#34;mymodel:v1&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;table&#39;</span><span class="p">:</span> <span class="n">train</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;test_set&#39;</span><span class="p">]})</span>
</span></span></code></pre></div><p>See the documentation links below for more advanced examples</p>
<h3 id="additional-resources">Additional Resources</h3>
<p><strong>Further Documentation for Serving</strong></p>
<ul>
<li><a href="https://docs.mlrun.org/en/latest/serving/serving-graph.html">MLRun Serving Graphs</a>
<ul>
<li><a href="https://docs.mlrun.org/en/latest/serving/serving-graph.html#overview">Overview</a></li>
<li><a href="https://docs.mlrun.org/en/latest/serving/serving-graph.html#examples">Examples</a></li>
<li><a href="https://docs.mlrun.org/en/latest/serving/serving-graph.html#the-graph-state-machine">The Graph State Machine</a></li>
</ul>
</li>
<li><a href="https://docs.mlrun.org/en/latest/serving/model-api.html">Model Serving API and Protocol</a>
<ul>
<li><a href="https://docs.mlrun.org/en/latest/serving/model-api.html#creating-custom-model-serving-class">Creating Custom Model Serving Class</a></li>
<li><a href="https://docs.mlrun.org/en/latest/serving/model-api.html#model-server-api">Model Server API</a></li>
<li><a href="https://docs.mlrun.org/en/latest/serving/model-api.html#model-monitoring">Model Monitoring</a></li>
</ul>
</li>
<li><a href="https://docs.mlrun.org/en/latest/serving/graph-example.html">Advance Graph Notebook Example</a>
<ul>
<li><a href="https://docs.mlrun.org/en/latest/serving/graph-example.html#define-functions-and-classes-used-in-our-graph">Define Functions and Classes (used in our graph)</a></li>
<li><a href="https://docs.mlrun.org/en/latest/serving/graph-example.html#create-a-new-serving-function-and-graph">Create a New Serving Function and Graph</a></li>
<li><a href="https://docs.mlrun.org/en/latest/serving/graph-example.html#test-our-function-locally">Test our functions locally</a></li>
<li><a href="https://docs.mlrun.org/en/latest/serving/graph-example.html#deploy-the-graph-as-a-real-time-serverless-function">Deploy a Graph as a Real-time Serverless function</a></li>
</ul>
</li>
<li><a href="https://docs.mlrun.org/en/latest/serving/distributed-graph.html">Distributed (Multi-function) Pipeline Example</a>
<ul>
<li><a href="https://docs.mlrun.org/en/latest/serving/distributed-graph.html#create-the-pipeline">Create the Pipeline</a></li>
<li><a href="https://docs.mlrun.org/en/latest/serving/distributed-graph.html#test-the-pipeline-locally">Test the Pipeline locally</a></li>
<li><a href="https://docs.mlrun.org/en/latest/serving/distributed-graph.html#deploy-to-the-cluster">Deploy to the Cluster</a></li>
</ul>
</li>
</ul>
<p><strong>Further Documentation for Monitoring</strong></p>
<ul>
<li><a href="https://docs.mlrun.org/en/latest/model_monitoring/model-monitoring-deployment.html">Model Monitoring Overview</a>
<ul>
<li><a href="https://docs.mlrun.org/en/latest/model_monitoring/model-monitoring-deployment.html#introduction">Introduction</a></li>
<li><a href="https://docs.mlrun.org/en/latest/model_monitoring/model-monitoring-deployment.html#architecture">Architecture</a>
<ul>
<li><a href="https://docs.mlrun.org/en/latest/model_monitoring/model-monitoring-deployment.html#drift-analysis">Drift Analysis</a></li>
<li><a href="https://docs.mlrun.org/en/latest/model_monitoring/model-monitoring-deployment.html#common-terminology">Common Terminology</a></li>
</ul>
</li>
<li><a href="https://docs.mlrun.org/en/latest/model_monitoring/model-monitoring-deployment.html#model-monitoring-using-the-iguazio-platform-interface">Model Monitoring Using the Iguazio Platform Interface</a>
<ul>
<li><a href="https://docs.mlrun.org/en/latest/model_monitoring/model-monitoring-deployment.html#model-endpoint-summary-list">Model Endpoint Summary List</a></li>
<li><a href="https://docs.mlrun.org/en/latest/model_monitoring/model-monitoring-deployment.html#model-endpoint-overview">Model Endpoint Overview</a></li>
<li><a href="https://docs.mlrun.org/en/latest/model_monitoring/model-monitoring-deployment.html#model-drift-analysis">Model Drift Analysis</a></li>
<li><a href="https://docs.mlrun.org/en/latest/model_monitoring/model-monitoring-deployment.html#model-features-analysis">Model Features Analysis</a></li>
</ul>
</li>
</ul>
</li>
</ul>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-d97b7cb2a1b9c6a681aceec8809193ff">7.5 - NVIDIA Triton Inference Server</h1>
    <div class="lead">Model serving with Triton Inference Server</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<p>Kubeflow currently doesn&rsquo;t have a specific guide for NVIDIA Triton Inference
Server. Note that Triton was previously known as the TensorRT Inference Server.
See the <a href="https://github.com/NVIDIA/triton-inference-server/tree/master/deploy/single_server">NVIDIA
documentation</a>
for instructions on running NVIDIA inference server on Kubernetes.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-a3d0c6465054fdbbd7e6aec57771eb34">7.6 - TensorFlow Serving</h1>
    <div class="lead">Serving TensorFlow models</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<div class="alert alert-primary" role="alert">
This Kubeflow component has <b>stable</b> status. See the
<a href="/docs/started/support/#application-status">Kubeflow versioning policies</a>.
</div>
<h2 id="serving-a-model">Serving a model</h2>
<p>To deploy a model we create following resources as illustrated below</p>
<ul>
<li>A deployment to deploy the model using TFServing</li>
<li>A K8s service to create an endpoint a service</li>
<li>An Istio virtual service to route traffic to the model and expose it through the Istio gateway</li>
<li>An Istio DestinationRule is for doing traffic splitting.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mnist-service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grpc-tf-serving</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http-tf-serving</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8500</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="m">8500</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">ClusterIP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mnist-v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">sidecar.istio.io/inject</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">args</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">port=9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">rest_api_port=8500</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">model_name=mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">model_base_path=YOUR_MODEL</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">/usr/bin/tensorflow_model_server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">tensorflow/serving:1.11.1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">30</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">30</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">tcpSocket</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">8500</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;4&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">4Gi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;1&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/var/config/</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config-volume</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">configMap</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mnist-v1-config</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config-volume</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">networking.istio.io/v1alpha3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">DestinationRule</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mnist-service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">mnist-service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">subsets</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">networking.istio.io/v1alpha3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">VirtualService</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mnist-service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">gateways</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">kubeflow-gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="s1">&#39;*&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">http</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">match</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">method</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">exact</span><span class="p">:</span><span class="w"> </span><span class="l">POST</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">uri</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">prefix</span><span class="p">:</span><span class="w"> </span><span class="l">/tfserving/models/mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">rewrite</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">uri</span><span class="p">:</span><span class="w"> </span><span class="l">/v1/models/mnist:predict</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">route</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">destination</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">mnist-service</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">port</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="m">8500</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">subset</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">100</span><span class="w">
</span></span></span></code></pre></div><p>Referring to the above example, you can customize your deployment by changing the following configurations in the YAML file:</p>
<ul>
<li>
<p>In the deployment resource, the <code>model_base_path</code> argument points to the model.
Change the value to your own model.</p>
</li>
<li>
<p>The example contains three configurations for Google Cloud Storage (GCS) access:
volumes (secret <code>user-gcp-sa</code>), volumeMounts, and
env (GOOGLE_APPLICATION_CREDENTIALS).
If your model is not at GCS (e.g. using S3 from AWS), See the section below on
how to setup access.</p>
</li>
<li>
<p>GPU. If you want to use GPU, add <code>nvidia.com/gpu: 1</code>
in container resources, and use a GPU image, for example:
<code>tensorflow/serving:1.11.1-gpu</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;4&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">4Gi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></span></span></code></pre></div></li>
<li>
<p>The resource <code>VirtualService</code> and <code>DestinationRule</code> are for routing.
With the example above, the model is accessible at <code>HOSTNAME/tfserving/models/mnist</code>
(HOSTNAME is your Kubeflow deployment hostname). To change the path, edit the
<code>http.match.uri</code> of VirtualService.</p>
</li>
</ul>
<h3 id="pointing-to-the-model">Pointing to the model</h3>
<p>Depending where model file is located, set correct parameters</p>
<p><em>Google cloud</em></p>
<p>Change the deployment spec as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">sidecar.istio.io/inject</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">args</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">port=9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">rest_api_port=8500</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">model_name=mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">model_base_path=gs://kubeflow-examples-data/mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">/usr/bin/tensorflow_model_server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">GOOGLE_APPLICATION_CREDENTIALS</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">/secret/gcp-credentials/user-gcp-sa.json</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">tensorflow/serving:1.11.1-gpu</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">30</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">30</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">tcpSocket</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">8500</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;4&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">4Gi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;1&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/var/config/</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config-volume</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/secret/gcp-credentials</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">gcp-credentials</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">configMap</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">mnist-v1-config</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config-volume</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">gcp-credentials</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">user-gcp-sa</span><span class="w">
</span></span></span></code></pre></div><p>The changes are:</p>
<ul>
<li>environment variable  <code>GOOGLE_APPLICATION_CREDENTIALS</code></li>
<li>volume <code>gcp-credentials</code></li>
<li>volumeMount <code>gcp-credentials</code></li>
</ul>
<p>We need a service account that can access the model.
If you are using Kubeflow&rsquo;s click-to-deploy app, there should be already a secret, <code>user-gcp-sa</code>, in the cluster.</p>
<p>The model at gs://kubeflow-examples-data/mnist is publicly accessible. However, if your environment doesn&rsquo;t
have google cloud credential setup, TF serving will not be able to read the model.
See this <a href="https://github.com/kubeflow/kubeflow/issues/621">issue</a> for example.
To setup the google cloud credential, you should either have the environment variable
<code>GOOGLE_APPLICATION_CREDENTIALS</code> pointing to the credential file, or run <code>gcloud auth login</code>.
See <a href="https://cloud.google.com/docs/authentication/">doc</a> for more detail.</p>
<p><em>S3</em></p>
<p>To use S3, first you need to create secret that will contain access credentials. Use base64 to encode your credentials and check details in the Kubernetes guide to <a href="https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually">creating a secret manually</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">  name: secretname
</span></span><span class="line"><span class="cl">data:
</span></span><span class="line"><span class="cl">  AWS_ACCESS_KEY_ID: bmljZSB0cnk6KQ==
</span></span><span class="line"><span class="cl">  AWS_SECRET_ACCESS_KEY: YnV0IHlvdSBkaWRuJ3QgZ2V0IG15IHNlY3JldCE=
</span></span><span class="line"><span class="cl">kind: Secret
</span></span></code></pre></div><p>Then use the following manifest as an example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">s3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">s3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kubeflow</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">mnist</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">sidecar.istio.io/inject</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">s3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">args</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">port=9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">rest_api_port=8500</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">model_name=s3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">model_base_path=s3://abc</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- --<span class="l">monitoring_config_file=/var/config/monitoring_config.txt</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">/usr/bin/tensorflow_model_server</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">AWS_ACCESS_KEY_ID</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">secretKeyRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">AWS_ACCESS_KEY_ID</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secretname</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">AWS_SECRET_ACCESS_KEY</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">secretKeyRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">AWS_SECRET_ACCESS_KEY</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secretname</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">AWS_REGION</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">us-west-1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">S3_USE_HTTPS</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">S3_VERIFY_SSL</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">S3_ENDPOINT</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">s3.us-west-1.amazonaws.com</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">tensorflow/serving:1.11.1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">IfNotPresent</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">livenessProbe</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="m">30</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="m">30</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">tcpSocket</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">s3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">9000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">8500</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;4&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">4Gi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;1&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">1Gi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l">/var/config/</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config-volume</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">configMap</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">s3-config</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">config-volume</span><span class="w">
</span></span></span></code></pre></div><h3 id="sending-prediction-request-directly">Sending prediction request directly</h3>
<p>If the service type is LoadBalancer, it will have its own accessible external ip.
Get the external ip by:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">kubectl get svc mnist-service
</span></span></code></pre></div><p>And then send the request</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">curl -X POST -d @input.json http://EXTERNAL_IP:8500/v1/models/mnist:predict
</span></span></code></pre></div><h3 id="sending-prediction-request-through-ingress-and-iap">Sending prediction request through ingress and IAP</h3>
<p>If the service type is ClusterIP, you can access through ingress.
It&rsquo;s protected and only one with right credentials can access the endpoint.
Below shows how to programmatically authenticate a service account to access IAP.</p>
<ol>
<li>Save the client ID that you used to
<a href="/docs/gke/deploy/">deploy Kubeflow</a> as <code>IAP_CLIENT_ID</code>.</li>
<li>Create a service account
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud iam service-accounts create --project=$PROJECT $SERVICE_ACCOUNT
</span></span></code></pre></div></li>
<li>Grant the service account access to IAP enabled resources:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud projects add-iam-policy-binding $PROJECT \
</span></span><span class="line"><span class="cl"> --role roles/iap.httpsResourceAccessor \
</span></span><span class="line"><span class="cl"> --member serviceAccount:$SERVICE_ACCOUNT
</span></span></code></pre></div></li>
<li>Download the service account key:
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">gcloud iam service-accounts keys create ${KEY_FILE} \
</span></span><span class="line"><span class="cl">   --iam-account ${SERVICE_ACCOUNT}@${PROJECT}.iam.gserviceaccount.com
</span></span></code></pre></div></li>
<li>Export the environment variable <code>GOOGLE_APPLICATION_CREDENTIALS</code> to point to the key file of the service account.</li>
</ol>
<p>Finally, you can send the request with an input file with this python
<a href="https://github.com/kubeflow/kubeflow/blob/master/docs/gke/iap_request.py">script</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">python iap_request.py https://YOUR_HOST/tfserving/models/mnist IAP_CLIENT_ID --input=YOUR_INPUT_FILE
</span></span></code></pre></div><p>To send a GET request:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">python iap_request.py https://YOUR_HOST/models/MODEL_NAME/ IAP_CLIENT_ID
</span></span></code></pre></div><h2 id="telemetry-and-rolling-out-model-using-istio">Telemetry and Rolling out model using Istio</h2>
<p>Please look at the <a href="/docs/external-add-ons/istio/">Istio guide</a>.</p>
<h2 id="logs-and-metrics-with-stackdriver">Logs and metrics with Stackdriver</h2>
<p>See the guide to <a href="/docs/gke/monitoring/">logging and monitoring</a>
for instructions on getting logs and metrics using Stackdriver.</p>

</div>



    
	
  
    
    
	
    

<div class="td-content" style="page-break-before: always">
    
	<h1 id="pg-dec844de279b9242c3a930b5ff4eda64">7.7 - TensorFlow Batch Prediction</h1>
    <div class="lead">See Kubeflow <a href="https://v0-6.kubeflow.org/docs/external-add-ons/serving/tfbatchpredict/">v0.6 docs</a> for batch prediction with TensorFlow models</div>
	

<div class="alert alert-warning" role="alert">
<h4 class="alert-heading">Out of date</h4>

    This guide contains outdated information pertaining to Kubeflow 1.0. This guide
needs to be updated for Kubeflow 1.1.

</div>

<div class="alert alert-warning" role="alert">
  <h4 class="alert-heading">Alpha</h4>
  This Kubeflow component has <b>alpha</b> status with limited support. See the
  <a href="/docs/started/support/#application-status">Kubeflow versioning policies</a>.
  The Kubeflow team is interested in your   
  <a href="https://github.com/kubeflow/batch-predict/issues">feedback</a></h4> 
  about the usability of the feature.
</div>
<p><a href="https://github.com/kubeflow/batch-predict">TensorFlow batch prediction</a> is not
supported in Kubeflow versions greater than v0.6. See the <a href="https://v0-6.kubeflow.org/docs/external-add-ons/serving/tfbatchpredict/">Kubeflow v0.6
documentation</a>
for earlier support for batch prediction with TensorFlow models.</p>

</div>



    
	
  

    
	
  



          </main>
        </div>
      </div>
      
<footer class="bg-dark pt-3 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Kubeflow mailing list" aria-label="Kubeflow mailing list">
    <a class="text-white" target="_blank" rel="noopener" href="https://www.kubeflow.org/docs/about/community/#kubeflow-mailing-list" aria-label="Kubeflow mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" rel="noopener" href="https://twitter.com/kubeflow" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Stack Overflow" aria-label="Stack Overflow">
    <a class="text-white" target="_blank" rel="noopener" href="https://stackoverflow.com/questions/tagged/kubeflow" aria-label="Stack Overflow">
      <i class="fab fa-stack-overflow"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener" href="https://github.com/GoogleCloudPlatform/kubeflow-distribution" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Slack" aria-label="Slack">
    <a class="text-white" target="_blank" rel="noopener" href="https://www.kubeflow.org/docs/about/community/#kubeflow-slack" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-5 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2022 Google | Documentation Distributed under CC BY 4.0</small>
        <p><small class="ml-1"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></small></p>
	
		<p class="mt-2"><a href="/kubeflow-docs/docs/about/">About</a></p>
	
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="/kubeflow-docs/js/main.min.301c0110334bec1b4445867d27dc7dd69b2df859154ccf252005508bc860cc5a.js" integrity="sha256-MBwBEDNL7BtERYZ9J9x91pst&#43;FkVTM8lIAVQi8hgzFo=" crossorigin="anonymous"></script>
<script src='/kubeflow-docs/js/tabpane-persist.js'></script>

  </body>
</html>
